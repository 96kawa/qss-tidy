# Probability

## Prerequisites

```{r}
library("tidyverse")
```

## Probability

```{r}
birthday <- function(k) {
  logdenom <- k * log(365) + lfactorial(365 - k)
  lognumer <- lfactorial(365)
  pr <- 1 -   exp(lognumer - logdenom)
  pr
}

bday <- tibble(k = 1:50,
               pr = birthday(k))
ggplot(bday, aes(x = k , y = pr)) +
  geom_hline(yintercept = 0.5, colour = "gray") +
  geom_point() +
  scale_y_continuous("Probability that at least two\n people have the same birthday", limits = c(0, 1)) +
  labs(x = "Number of people")
```

**Note:** The logarithm is used for numerical stability. Basically,  "floating-point" numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues.
See "Falling Into the Floating Point Trap" in [The R Inforno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf) for a summary of floating point numbers.
See these John Fox posts [1](http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/) [2](http://www.johndcook.com/blog/2008/09/28/theoretical-explanation-for-numerical-results/) for an example of numerical stability gone wrong.
Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/.


Instead of using a for-loop, we could do the simulations using a functional
as described in R for Data Science.
Define the function `sim_bdays` to randomly sample k birthdays, and returns
`TRUE` if there are any duplicates.
```{r}
sim_bdays <- function(k) {
  days <- sample(1:365, k, replace = TRUE)
  length(unique(days)) < k
}
```
Set the parameters for 1,000 simulations, and 23 individuals.
We use `map_lgl` since `sim_bdays` returns a logical value (`TRUE`, `FALSE`):
```{r}
sims <- 1000
k <- 23
map_lgl(seq_len(sims), ~ sim_bdays(k)) %>%
  mean()
```

```{r}
FLVoters <- read_csv(qss_data_url("probability", "FLVoters.csv"))
dim(FLVoters)
FLVoters <- FLVoters %>%
  na.omit()
```

```{r}
margin.race <-
  FLVoters %>%
  count(race) %>%
  mutate(prop = n / sum(n))
margin.race
```

```{r}
margin.gender <- FLVoters %>%
  count(gender) %>%
  mutate(prop = n / sum(n))
margin.gender
```

```{r}
FLVoters %>% 
  filter(gender == "f") %>%
  count(race) %>%
  mutate(prop = n / sum(n))
```

```{r}
joint.p <-
  FLVoters %>%
  count(gender, race) %>%
  # needed because it is still grouped by gender
  ungroup() %>%
  mutate(prop = n / sum(n))
joint.p
```
or 
```{r}
joint.p %>%
  ungroup() %>%
  select(-n) %>%
  spread(gender, prop)
  
```

```{r}
joint.p %>%
  group_by(race) %>%
  summarise(prop = sum(prop))
```

```{r}
joint.p %>%
  group_by(gender) %>%
  summarise(prop = sum(prop))
```


```{r}
FLVoters <-
  FLVoters %>%
  mutate(age_group = 
           case_when(
             .$age <= 20 ~ 1,
             .$age > 20 & .$age <= 40 ~ 2,
             .$age > 40 & .$age <= 60 ~ 3,
             .$age > 60 ~ 4
           ))
```

```{r}
joint3 <-
  FLVoters %>%
  count(race, age_group, gender) %>%
  ungroup() %>%
  mutate(prop = n / sum(n))
joint3
```

Marginal probabilities by age groups
```{r}
margin_age <- 
  FLVoters %>%
  count(age_group) %>%
  mutate(prop = n / sum(n))
margin_age
```

Calculate the probabilities that each group is in a given age group.
```{r}
left_join(joint3, 
          select(margin_age, age_group, margin_age = prop),
          by = "age_group") %>%
  mutate(prob_age_group = prop / margin_age) %>%
  filter(race == "black", gender == "f", age_group == 4)
```

Two-way joint probability table for age group and gender
```{r}
joint2 <- FLVoters %>%
  count(age_group, gender) %>%
  ungroup() %>%
  mutate(prob_age_gender = n / sum(n)) 
joint2
```

Conditional probablities $P(race | gender, age)$,
```{r}
left_join(joint3, select(joint2, -n), by = c("age_group", "gender")) %>%
  mutate(prob_race = prop / prob_age_gender) %>% 
  arrange(age_group, gender) %>%
  select(age_group, gender, race, prob_race)
```
Each row is the $P(race | age_group, gender)$.


### Independence



### Predicting Race Using Surname and Residence Location

Start with the Census names files:
```{r}
cnames <- read_csv(qss_data_url("probability", "names.csv"))
glimpse(cnames)
```

For each surname, contains variables with the probability that it belongs to an individual of a given race (`pctwhite`, `pctblack`, ...).
We want to find the most-likely race for a given surname, by finding the race with the maximum proportion.
Instead of dealing with multiple variables, it is easier to use `max` on a single variable, so we will rearrange the data to in order to use a grouped summarise, and then merge the new variable back to the original datasets.

```{r}
library("forcats")
most_likely <-
  cnames %>%
  select(-count) %>%
  gather(race_pred, pct, -surname) %>%
  # remove pct prefix
  mutate(race_pred = stringr::str_replace(race_pred, "^pct", "")) %>%
  # group by surname
  group_by(surname) %>%
  filter(pct == max(pct)) %>%
  # if there are ties, keep only one obs
  slice(1) %>%
  # don't need pct anymore
  select(-pct) %>%
  # Need to make equivalent to levels in FLVoters
  mutate(race_pred = fct_recode(race_pred, asian = "api", other = "others"))
```
Now merge that back to the original cnames
```{r}
cnames <- left_join(cnames, most_likely, by = "surname")
```


Intead of using `match`, use `inner_join` to merge the surnames to FLVoters 
```{r}
FLVoters <- inner_join(FLVoters, cnames, by = "surname")
dim(FLVoters)
```
`FLVoters` also includes a "native" category that the surname dataset does not.
```{r}
FLVoters <- FLVoters %>%
  mutate(race2 = fct_recode(race, other = "native"))
```

Check that the levels of `race` and `race_pred` are the same:
```{r}
FLVoters %>%
  count(race2)
```
```{r}
FLVoters %>%
  count(race_pred)
```


Now we can calculate *True positive rate* for *all* races
```{r}
FLVoters %>%
  group_by(race2) %>%
  summarise(tp = mean(race2 == race_pred)) %>%
  arrange(desc(tp))
```
and the *False dicsovery rate* for *all* races,
```{r}
FLVoters %>%
  group_by(race_pred) %>%
  summarise(fp = mean(race2 != race_pred)) %>%
  arrange(desc(fp))
```

Now add get residence data
```{r}
FLCensus <- read_csv(qss_data_url("probability", "FLCensusVTD.csv"))
```

$P(race)$ in Florida:
```{r}
race.prop <- 
  FLCensus %>%
  select(total.pop, white, black, api, hispanic, others) %>%
  gather(race, pct, -total.pop) %>%
  group_by(race) %>%
  summarise(mean = weighted.mean(pct, weights = total.pop)) %>%
  arrange(desc(mean))
race.prop
```

