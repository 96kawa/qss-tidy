# Probability

## Prerequisites

```{r message=FALSE}
library("tidyverse")
library("forcats")
library("stringr")
```
We will also, use the function `qss_data_url`:
```{r echo=FALSE}
insert_fun("qss_data_url")
```
```{r qss_data_url-source,eval=FALSE}
```


## Probability

**Original code**
```{r eval=FALSE}
k <- 23 # number of people
sims <- 1000 # number of simulations event <- 0 # counter
for (i in 1:sims) {
days <- sample(1:365, k, replace = TRUE)
days.unique <- unique(days) # unique birthdays
## if there are duplicates, the number of unique birthdays 
## will be less than the number of birthdays, which is `k' 
if (length(days.unique) < k) {
        event <- event + 1
    }
}
## fraction of trials where at least two bdays are the same
answer <- event / sims
answer
```

**tidyverse**
```{r}
birthday <- function(k) {
  logdenom <- k * log(365) + lfactorial(365 - k)
  lognumer <- lfactorial(365)
  pr <- 1 -   exp(lognumer - logdenom)
  pr
}

bday <- tibble(k = 1:50,
               pr = birthday(k))
ggplot(bday, aes(x = k , y = pr)) +
  geom_hline(yintercept = 0.5, colour = "white", size = 2) +
  geom_line() +
  geom_point() +
  scale_y_continuous("Probability that at least two\n people have the same birthday", limits = c(0, 1)) +
  labs(x = "Number of people")
```

**Note:** The logarithm is used for numerical stability. Basically,  "floating-point" numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues.
See "Falling Into the Floating Point Trap" in [The R Inforno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf) for a summary of floating point numbers.
See these John Fox posts [1](http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/) [2](http://www.johndcook.com/blog/2008/09/28/theoretical-explanation-for-numerical-results/) for an example of numerical stability gone wrong.
Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/.


### Sampling without replacement


Instead of using a for loop, we could do the simulations using a functional
as described in `r R4DS`.

Define the function `sim_bdays` which randomly samples `k` birthdays, and returns
`TRUE` if there are any duplicate birthdays, and `FALSE` if there are none.
```{r}
sim_bdays <- function(k) {
  days <- sample(1:365, k, replace = TRUE)
  length(unique(days)) < k
}
```

We can test the code for `k = 10` birthdays.
```{r}
sim_bday(10)
```
Since the function is randomly sampling birthdays 

One helpful feature of a functional style of writing code vs. a `for` loop is that
the function encapsulates the code and allows you to test that it works for different
inputs before repeating it for many inputs.
It is more difficult to debug functions that produce random outputs, but some sanity checks
are

- function always returns a logical vector of length one (`TRUE` or `FALSE`)
- it is always `FALSE` when `k = 1` since there can never be a duplicates with one person
- it is always `TRUE` when `k >= 365` by the [pidgeonhole principle](https://en.wikipedia.org/wiki/Pigeonhole_principle).

```{r}
sim_bday(1)
sim_bday(365)
```

Set the parameters for 1,000 simulations, and 23 individuals.
We use `map_lgl` since `sim_bdays` returns a logical value (`TRUE`, `FALSE`):
```{r}
sims <- 1000
k <- 23
map_lgl(seq_len(sims), ~ sim_bdays(k)) %>%
  mean()
```


### Combinations

**Original code**
```{r}
choose(84, 6)
```

## Conditional Probability

### Conditional, Marginal, and Joint Probabilities


```{r message=FALSE}
FLVoters <- read_csv(qss_data_url("probability", "FLVoters.csv"))
dim(FLVoters)
FLVoters <- FLVoters %>%
  na.omit()
```

Instead of using `r rdoc("base", "prop.base")`, we calculate the probabilities
with a data frame.
Marginal probabilities of race,
```{r}
margin_race <-
  FLVoters %>%
  count(race) %>%
  mutate(prop = n / sum(n))
margin_race
```

Marginal probabilities of gender:
```{r}
margin_gender <- FLVoters %>%
  count(gender) %>%
  mutate(prop = n / sum(n))
margin_gender
```

```{r}
FLVoters %>%
  filter(gender == "f") %>%
  count(race) %>%
  mutate(prop = n / sum(n))
```

```{r}
joint_p <-
  FLVoters %>%
  count(gender, race) %>%
  # needed because it is still grouped by gender
  ungroup() %>%
  mutate(prop = n / sum(n))
joint_p
```

We can convert the data frame to have gender as columns:
```{r}
joint_p %>%
  ungroup() %>%
  select(-n) %>%
  spread(gender, prop)
```


```{r}
joint_p %>%
  group_by(race) %>%
  summarise(prop = sum(prop))
```

Sum over gender:
```{r}
joint_p %>%
  group_by(gender) %>%
  summarise(prop = sum(prop))
```

```{r}
FLVoters <-
  FLVoters %>%
  mutate(age_group = cut(age, c(0, 20, 40, 60, Inf), right = TRUE,
                         labels = c("<= 20", "20-40", "40-60", "> 60")))
```

```{r}
joint3 <-
  FLVoters %>%
  count(race, age_group, gender) %>%
  ungroup() %>%
  mutate(prop = n / sum(n))
joint3
```

Marginal probabilities by age groups
```{r}
margin_age <-
  FLVoters %>%
  count(age_group) %>%
  mutate(prop = n / sum(n))
margin_age
```


Calculate the probabilities that each group is in a given age group, and show
$P(\text{black} \land \text{female} | \text{"> 60""})
```{r}
left_join(joint3,
          select(margin_age, age_group, margin_age = prop),
          by = "age_group") %>%
  mutate(prob_age_group = prop / margin_age) %>%
  filter(race == "black", gender == "f", age_group == "> 60") %>%
  select(race, age_group, gender, prob_age_group)
```

Two-way joint probability table for age group and gender
```{r}
joint2 <- FLVoters %>%
  count(age_group, gender) %>%
  ungroup() %>%
  mutate(prob_age_gender = n / sum(n))
joint2
```

The joint probability $P(\text{age} > 60 \land \text{female})$,
```{r}
filter(joint2, age_group == "> 60", gender == "f")
```

The conditional probabilities $P(race | gender, age)$,
```{r}
condprob_race <-
  left_join(joint3, select(joint2, -n), by = c("age_group", "gender")) %>%
  mutate(prob_race = prop / prob_age_gender) %>%
  arrange(age_group, gender) %>%
  select(age_group, gender, race, prob_race)
```
Each row is the $P(race | age_group, gender)$, so $P(\text{black} | \text{female} \land \text{age} > 60)$,
```{r}
filter(condprob_race, gender == "f", age_group == "> 60", race == "black")
```


### Independence

Create a table with the products of margins of race and age.
Using the function `r rdoc("tidyr", "crossing")` to create a tibble with
all combinations of race and gender and the independent prob.
```{r}
race_gender_indep <-
  crossing(select(margin_race, race, prob_race = prop),
           select(margin_gender, gender, prob_gender = prop)) %>%
  mutate(prob_indep = prob_race * prob_gender) %>%
  left_join(select(joint_p, gender, race, prob = prop),
            by = c("gender", "race")) %>%
  select(race, gender, everything())
race_gender_indep
```

```{r}
ggplot(race_gender_indep,
       aes(x = prob_indep, y = prob, colour = race)) +
  geom_abline(intercept = 0, slope = 1, colour = "white", size = 2) +
  geom_point() +
  facet_grid(. ~ gender) +
  coord_fixed() +
  theme(legend.position = "bottom") +
  labs(x = expression(P("race") * P("gender")),
       y = expression(P("race and gender")))

```


While the original code only calculates joint-independence value for values of
age > 60, and female, this calculates the joint probabilities for all combinations
of the three variables, and facets by age and gender.

```{r fig.height = 10, fig.width = 7, fig.asp = NA}
joint_indep <-
  # all combinations of race, age, gender
  crossing(select(margin_race, race, prob_race = prop),
           select(margin_age, age_group, prob_age = prop),
           select(margin_gender, gender, prob_gender = prop)) %>%
  mutate(indep_prob = prob_race * prob_age * prob_gender) %>%
  left_join(select(joint3, race, age_group, gender, prob = prop),
            by = c("gender", "age_group", "race")) %>%
  replace_na(list(prob = 0))
  
ggplot(joint_indep, aes(x = prob, y = indep_prob, colour = race)) +
  geom_abline(intercept = 0, slope = 1, colour = "white", size = 2) +
  geom_point() +
  facet_grid(age_group ~ gender) +
  coord_fixed() +
  labs(x = "P(race and age and gender)",
       y = "P(race) * P(age) * P(gender)",
       title = "Joint Independence")
    
```


While code in *QSS* only calculates the conditional independence given female,
the following code calculates conditional independence for all values of `gender`:
```{r fig.height = 10, fig.width = 7, fig.asp = NA}
cond_gender <- 
  left_join(select(joint3, race, age_group, gender, joint_prob = prop),
            select(margin_gender, gender, prob_gender = prop),
            by = c("gender")) %>%
  mutate(cond_prob = joint_prob / prob_gender)
```

Calculate the conditional distribution $\Pr(\mathtt{race} | \mathtt{gender})$:
```
prob_race_gender <- 
  left_join(select(joint_p, race, gender, prob_race_gender = prop),
            select(margin_gender, gender, prob_gender = prop),
            by = "gender") %>%
  mutate(prob_race = prob_race_gender / prob_gender)
```

Calculate the conditional distribution $\Pr(\mathtt{age} | \mathtt{gender})$:
```
# P(age | gender)
prob_age_gender <- 
  left_join(select(joint2, age_group, gender, prob_age_gender),
            select(margin_gender, gender, prob_gender = prop),
            by = "gender") %>%
  mutate(prob_age = prob_age_gender / prob_gender)

# indep prob of race and age    
indep_cond_gender <-
  full_join(select(prob_race_gender, race, gender, prob_race),
            select(prob_age_gender, age_group, gender, prob_age),
            by = "gender") %>%
  mutate(indep_prob = prob_race * prob_age)

inner_join(select(indep_cond_gender, race, age_group, gender, indep_prob),
           select(cond_gender, race, age_group, gender, cond_prob),
           by = c("gender", "age_group", "race")) %>%
  ggplot(aes(x = cond_prob, y = indep_prob, colour = race)) +
  geom_abline(intercept = 0, slope = 1, colour = "white", size = 2) +
  geom_point() +
  facet_grid(age_group ~ gender) +
  coord_fixed() +
  labs(x = "P(race and age | gender)",
       y = "P(race | gender) * P(age | gender)",
       title = "Marginal independence")

```


**Monty-hall problem**

The `for` loop approach in *QSS * is prefectly valid code, but here we provide a more
functional approach to solving the problem. 
We will define a function to choose a door, repeat the function multiple times while storing
the results in a data frame, and then summarize that data frame.

This code doesn't rely on any non-tidyverse code, but the following is another way to write it.

First, create a function for a single iteration.
This will save the results as data frame.
```{r}
choose_door <- function(.id) {
  # what's behind each door door:
  # Why is it okay that this is fixed?
  doors <- c("goat", "goat", "car")
  
  # User randomly samples a door
  first <- sample(1:3, 1)
  
  # randomly choose the door that Monty Hall reveals
  remain <- doors[-first]
  monty <- sample((1:2)[remain == "goat"], size = 1)
  # 
  ret <- tribble(~strategy, ~result,
                 "no switch", doors[first],
                  "switch", remain[-monty])
  ret[[".id"]] <- .id
  ret
}
```
Now use `r rdoc("purrr", "map_df")` to run `choose_door` multiple times,
and then summarize the results:
```{r}
sims <- 1000
results <- map_df(seq_len(sims), choose_door)
results %>%
  group_by(strategy) %>%
  summarise(pct_win = mean(result == "car"))
```

Can we make this even more general? What about a function that takes a data frame as its input with the choices? ...



### Predicting Race Using Surname and Residence Location

Start with the Census names files:
```{r message=FALSE}
cnames <- read_csv(qss_data_url("probability", "names.csv"))
glimpse(cnames)
```

For each surname, contains variables with the probability that it belongs to an individual of a given race (`pctwhite`, `pctblack`, ...).
We want to find the most-likely race for a given surname, by finding the race with the maximum proportion.
Instead of dealing with multiple variables, it is easier to use `max` on a single variable, so we will rearrange the data to in order to use a grouped summarize, and then merge the new variable back to the original data sets.

<!-- This needs to be sped up. Possibly a  -->

Note, this code takes a while.
```{r}
most_likely <-
  cnames %>%
  select(-count) %>%
  gather(race_pred, pct, -surname) %>%
  # remove pct prefix
  mutate(race_pred = str_replace(race_pred, "^pct", "")) %>%
  # group by surname
  group_by(surname) %>%
  filter(pct == max(pct)) %>%
  # if there are ties, keep only one obs
  slice(1) %>%
  # don't need pct anymore
  select(-pct) %>%
  mutate(race_pred = factor(race_pred),
         # need to convert to factor first, else errors
         race_pred = fct_recode(race_pred, 
                                "asian" = "api", "other" = "others"))

```

Now merge that back to the original `cnames` data frame.
```{r}
cnames <- left_join(cnames, most_likely, by = "surname")
```


Instead of using `match`, use `inner_join` to merge the surnames to `FLVoters`:
```{r}
FLVoters <- inner_join(FLVoters, cnames, by = "surname")
dim(FLVoters)
```
`FLVoters` also includes a "native" category that the surname dataset does not.
```{r}
FLVoters <- FLVoters %>%
  mutate(race2 = fct_recode(race, other = "native"))
```

Check that the levels of `race` and `race_pred` are the same:
```{r}
FLVoters %>%
  count(race2)
```
```{r}
FLVoters %>%
  count(race_pred)
```


Now we can calculate *True positive rate* for *all* races
```{r}
FLVoters %>%
  group_by(race2) %>%
  summarise(tp = mean(race2 == race_pred)) %>%
  arrange(desc(tp))
```
and the *False discovery rate* for *all* races,
```{r}
FLVoters %>%
  group_by(race_pred) %>%
  summarise(fp = mean(race2 != race_pred)) %>%
  arrange(desc(fp))
```

Now add residence data
```{r message=FALSE}
FLCensus <- read_csv(qss_data_url("probability", "FLCensusVTD.csv"))
```

$P(race)$ in Florida:
```{r}
race.prop <-
  FLCensus %>%
  select(total.pop, white, black, api, hispanic, others) %>%
  gather(race, pct, -total.pop) %>%
  group_by(race) %>%
  summarise(mean = weighted.mean(pct, weights = total.pop)) %>%
  arrange(desc(mean))
race.prop
```

### Predicting Election Outcomes with Uncertainty

```{r message=FALSE}
pres08 <- read_csv(qss_data_url("probability", "pres08.csv")) %>%
  mutate(p = Obama / (Obama + McCain))
```

Write a function to simulate the elections. `df` is the data frame (`pres08`)
with the state, EV, and `p` columns. `n_draws` is the size of the binomial distribution to draw from. `.id` is the simulation number.
```{r}
sim_election <- function(.id, df, n_draws = 1000) {
  # For each state randomly sample
  mutate(df,
         draws = rbinom(n(), n_draws, p)) %>%
  filter(draws > (n_draws / 2)) %>%
  summarise(EV = sum(EV),
            .id = .id)
}
```

Now simulate the election 10,000 times:
```{r}
sims <- 10000
sim_results <- map_df(seq_len(sims), ~ sim_election(.x, pres08, n_draws = 1000))
```
In the 2008 election, Obama received 364 electoral votes
```{r}
ELECTION_EV <- 364
```
And plot them,
```{r}
ggplot(sim_results, aes(x = EV, y = ..density..)) +
  geom_histogram(binwidth = 10, fill = "gray30") +
  geom_vline(xintercept = ELECTION_EV, colour = "black", size = 2) +
  labs(x = "Electoral Votes", y = "density")
```
Simulation mean, variance, and standard deviations:
```{r}
sim_results %>%
  select(EV) %>%
  summarise_all(funs(mean, var, sd))
```

Theoretical probabilities
```{r}
# we cannot use n, because mutate will look for n() first.
n_draws <- 1000
pres08 %>%
  mutate(pb = pbinom(n_draws / 2, size = n_draws, prob = p,
                     lower.tail  = FALSE)) %>%
  summarise(mean = sum(pb * EV),
            V = sum(pb * (1 - pb) * EV ^ 2),
            sd = sqrt(V))
```

## Large Sample Theorems

### Law of Large Numbers

Put the simulation number, `x` and `mean` in a tibble:
```{r}
sims <- 1000
p <- 0.2
size = 10
lln_binom <- tibble(
  n = seq_len(sims),
  x = rbinom(sims, p = p, size = size),
  mean = cumsum(x) / n,
  distrib = str_c("Binomial(", size, ", ", p, ")"))
```

```{r}
lln_unif <-
 tibble(n = seq_len(sims),
        x = runif(sims),
        mean = cumsum(x) / n,
        distrib = str_c("Uniform(0, 1)"))
```

```{r}
true_means <- 
  tribble(~distrib, ~mean,
          "Uniform(0, 1)", 0.5,
          str_c("Binomial(", size, ", ", p, ")"), size * p)

ggplot() +
  geom_hline(aes(yintercept = mean), data = true_means, 
             colour = "white", size = 2) +
  geom_line(aes(x = n, y = mean), 
            data = bind_rows(lln_binom, lln_unif)) +
  facet_grid(distrib ~ ., scales = "free_y") +
  labs(x = "Sample Size", y = "Sample Mean")
```

### Central Limit Theorem


The population mean of the binomial distribution is $\mu = p n$ and the variance is $\mu = p (1 - p) n$.
```{r}
sims <- 1000
n_samp <- 1000

# Mean of binomial distribution
binom_mean <- function(size, p) {
  size * p
}

# Variance of binomial distribution
binom_var <- function(size, p) {
  size * p * (1 - p) 
}

sim_binom_clt <- function(n_samp, size, p) {
  x <- rbinom(n_samp, prob = p, size = size)
  z <- (mean(x) - binom_mean(size, p)) /
    sqrt(binom_var(size, p) / n_samp)
  tibble(distrib = str_c("Binomial(", p, ", ", size, ")"),
         z = z)
}

# Mean of uniform distribution
unif_mean <- function(min, max) {
  0.5 * (min + max)
}

# Variance of uniform distribution
unif_var <- function(min, max) {
  (1 / 12) * (max - min) ^ 2
}

sim_unif_clt <- function(n_samp, min = 0, max = 1) {
  x <- runif(n_samp, min = min, max = max)
  z <- (mean(x) - unif_mean(min, max)) /
    sqrt(unif_var(min, max) / n_samp)
  tibble(distrib = str_c("Uniform(", min, ", ", max, ")"),
         z = z)
}
```

Since we will calculate this for `n_samp = 1000` and `n_samp`, we might as well write a function for it. 
```{r}
clt_plot <- function(n_samp) {
  bind_rows(map_df(seq_len(sims), ~ sim_binom_clt(n_samp, size, p)),
            map_df(seq_len(sims), ~ sim_unif_clt(n_samp))) %>%
    ggplot(aes(x = z)) +
    geom_density() +
    geom_rug() +
    stat_function(fun = dnorm, colour = "red") +
    facet_grid(distrib ~ .) +
    ggtitle(str_c("Sample size = ", n_samp))
}
clt_plot(1000)
clt_plot(100)
```

