# Prediction

## Prerequisites

```{r message = FALSE}
library("tidyverse")
```
We will use the package `r pkg_link("lubridate")` to work with dates.
```{r}
library("lubridate")
```
The packages `r pkg_link("modelr")` and `r pkg_link("broom")` are used to wrangle the results of linear regressions,
```{r}
library("broom")
library("modelr")
```


## Loops in R

For more on looping and iteration in R see the *R for Data Science* chapter [Iteration](http://r4ds.had.co.nz/data-visualisation.html).

RStudio provides many features to help debugging, which will be useful in
for loops and function: see  [this](https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio) article for an example.

## General Conditional Statements in R

**TODO** Is this in R4DS, find good cites.

If you are using conditional statements to assign values for data frame, 
see the **dplyr** functions `r rdoc("dplyr", "if_else")`, `r rdoc("dplyr", "recode")`, and `r rdoc("dplyr", "case_when")`

## Poll Predictions

Load the election polls and election results for 2008,
```{r polls08,message=FALSE}
polls08 <- read_csv(qss_data_url("UNCERTAINTY", "polls08.csv"))
glimpse(polls08)
```
```{r pres08,message=FALSE}
pres08 <- read_csv(qss_data_url("UNCERTAINTY", "pres08.csv"))
glimpse(pres08)
```

Compute Obama's margin in polls and final election
```{r polls08_margin}
polls08 <-
  polls08 %>% mutate(margin = Obama - McCain)
pres08 <- 
  pres08 %>% mutate(margin = Obama - McCain)
```

To work with dates, the R package `r pkg_link("lubridate")` makes wrangling them
much easier.
See the `r R4DS` chapter [Dates and Times](http://r4ds.had.co.nz/dates-and-times.html).

The function `r rdoc("lubridate", "ymd")` will convert character strings like `year-month-day` and more
into dates, as long as the order is (year, month, day). See `r rdoc("lubridate", "dmy")`, `r rdoc("lubridate", "ymd")`, and others for other ways to convert strings to dates.
```{r}
x <- ymd("2008-11-04")
y <- ymd("2008/9/1")
x - y
```

However, note that in `polls08`, the date `middate` is *already* a `date` object,
```{r}
class(polls08$middate)
```
The function `r rdoc("readr", "read_csv")` by default will check character vectors to see if they have patterns that appear to be dates, and if so, will
parse those columns as dates.

We'll create a variable for election day
```{r}
ELECTION_DAY <- ymd("2008-11-04")
```
and add a new column to `poll08` with the days to the election
```{r}
polls08 <-
  mutate(polls08, ELECTION_DAY - middate)
```

Although the code in the chapter uses a `for` loop, there is no reason to do so.
We can accomplish the same task by merging the election results data to the polling data by `state`.

```{r}
polls_w_results <- 
  left_join(polls08,
            select(pres08, state, elec_margin = margin),
            by = "state") %>%
  mutate(error = elec_margin - margin)
glimpse(polls_w_results)
```


To get the last poll in each state, arrange and filter on middate
```{r}
last_polls <- 
  polls_w_results %>%
  arrange(state, desc(middate)) %>%
  group_by(state) %>%
  slice(1)
last_polls
```


**Challenge:** Instead of using the last poll, use the average of polls in the last week? Last month? How do the margins on the polls change over the election period?

To simplify things for later, let's define a function `rmse` which calculates the root mean squared error, as defined in the book.
See the `r R4DS` chapter [Functions](http://r4ds.had.co.nz/functions.html) for more on writing functions.

```{r}
rmse <- function(actual, pred) {
  sqrt(mean((actual - pred) ^ 2))
}
```
Now we can use `rmse()` to calculate the RMSE for all the final polls:
```{r}
rmse(last_polls$margin, last_polls$elec_margin)
```
Or since we already have a variable `error`,
```{r}
sqrt(mean(last_polls$error ^ 2))
```
The mean prediction error is
```{r}
mean(last_polls$error)
```


This is slightly different than what is in the book due to the difference in the poll used as the final poll; many states have many polls on the last day.

I'll choose binwidths of 1%, since that is fairly interpretable:
```{r}
ggplot(last_polls, aes(x = error)) +
  geom_histogram(binwidth = 1, boundary = 0)
```

The text uses bindwidths of 5%:
```{r}
ggplot(last_polls, aes(x = error)) +
  geom_histogram(binwidth = 5, boundary = 0)
```

**Challenge:** What other ways could you visualize the results? How would you show all states? What about plotting the absolute or squared errors instead of the errors?

**Challenge:** What happens to prediction error if you average polls?
Consider averaging back over time? 
What happens if you take the averages of the state poll average and average of **all** polls - does that improve prediction? 

To create a scatterplot using the state abbreviations instead of points use
`r ggdoc("geom_text")` instead of `r ggdoc("geom_point")`.
```{r}
ggplot(last_polls, aes(x = margin, y = elec_margin, label = state)) +
  geom_text() +
  geom_abline(col = "red") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  coord_fixed() +
  labs(x = "Poll Results", y = "Actual Election Results")
```

We can create a confusion matrix as follow.

Create a new columns `classification` which shows whether how the poll's classification was related to the actual election outcome ("true positive", "false positive", "false negative", "false positive").
This can be accomplished easily with the `r pkg_link("dplyr")` funtion `r rdoc("dplyr", "case_when")`.
*Note:* You need to use `.` to refer to the data frame when using `case_when` with a `mutate` function.
```{r}
last_polls <-
  last_polls %>%
  ungroup() %>%
  mutate(classification = 
           case_when(
             (.$margin > 0 & .$elec_margin > 0) ~ "true positive",
             (.$margin > 0 & .$elec_margin < 0) ~ "false positive",
             (.$margin < 0 & .$elec_margin < 0) ~ "true negative",
             (.$margin < 0 & .$elec_margin > 0) ~ "false negative"
           ))
```
No simply count the 
```{r}
last_polls %>%
  group_by(classification) %>%
  count()
```

Which states were incorrectly predicted?
```{r}
last_polls %>%
  filter(classification %in% c("false positive", "false negative")) %>%
  select(state, margin, elec_margin, classification) %>%
  arrange(desc(elec_margin))
```

What was the difference in the poll prediction of electoral votes and actual electoral votes. 
We hadn't included the variable `EV` when we first merged, but that's no problem, we'll just merge again in order to grab that variable:
```{r}
last_polls %>%
  left_join(select(pres08, state, EV), by = "state") %>%
  summarise(EV_pred = sum((margin > 0) * EV),
            EV_actual = sum((elec_margin > 0) * EV))
```

To look at predictions of the 

```{r message=FALSE}
# load the data
pollsUS08 <- read_csv(qss_data_url("prediction", "pollsUS08.csv")) %>%
  mutate(DaysToElection = ELECTION_DAY - middate)
```

We'll produce the seven-day averages slightly differently than the method used in the text. 
For all dates in the data, we'll calculate the moving average.
```{r}
all_dates <- seq(min(polls08$middate), ELECTION_DAY,
                 by = "days")

# Number of poll days to use
POLL_DAYS <- 7

pop_vote_avg <- vector(length(all_dates), mode = "list")
for (i in seq_along(all_dates)) {
  date <- all_dates[i]
  # summarise the seven day
  week_data <-
     pollsUS08 %>%
     filter(as.integer(middate - date) <= 0,
            as.integer(middate - date) > -POLL_DAYS) %>%
     summarise(Obama = mean(Obama, na.rm = TRUE),
               McCain = mean(McCain, na.rm = TRUE))
  # add date for the observation
  week_data$date <- date
  pop_vote_avg[[i]] <- week_data
}

pop_vote_avg <- bind_rows(pop_vote_avg)
```

It is easier to plot this if the data are tidy, with `Obama` and `McCain` as categories of a column `candidate`.
```{r}
pop_vote_avg_tidy <-
  pop_vote_avg %>% 
  gather(candidate, share, -date, na.rm = TRUE)
pop_vote_avg_tidy
```

```{r}
ggplot(pop_vote_avg_tidy, aes(x = date, y = share,
                              colour = forcats::fct_reorder2(candidate,
                                                             date, share))) +
  geom_point() +
  geom_line()
```


**Challenge** read `r R4DS` chapter [Iteration](http://r4ds.had.co.nz/iteration.html#the-map-functions) and use the function `r rdoc("purrr", "map_df")` instead of a for loop.

The 7-day average is similar to the simple method used by [Real Clear Politics](http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html). 
The RCP average is simply the average of all polls in their data for the last seven days.
Sites like 538 and the Huffington Post on the other hand, also use what amounts to averaging polls, but using more sophisticated statistical methods to assign different weights to different polls.

**Challenge** Why do we need to use different polls for the popular vote data? Why not simply average all the state polls?
What would you have to do? 
Would the overall popular vote be useful in predicting state-level polling, or vice-versa? How would you use them?


## Linear Regression

### Facial Appearance and Election Outcomes

Load the `face` dataset:
```{r face,message=FALSE}
face <- read_csv(qss_data_url("prediction", "face.csv"))
```
Add Democrat and Republican vote shares, and the difference in shares:
```{r face_shares}
face <-
  face %>%
  mutate(d.share = d.votes / (d.votes + r.votes),
         r.share = r.votes / (d.votes + r.votes),
         diff.share = d.share - r.share)
```

Plot facial competence vs. vote share:
```{r}
ggplot(face, aes(x = d.comp, y = diff.share, colour = w.party)) +
  geom_point() +
  labs(x = "Competence scores for Democrats",
       y = "Democratic margin in vote share")
```

### Correlation

### Least Squares

Run the linear regression
```{r}
fit <- lm(diff.share ~ d.comp, data = face)
summary(fit)
```

There are many functions to get data out of the `lm` model.

In addition to these, the **broom** package provides three functions: `glance`, `tidy`, and `augment` that always return data frames.

The function `r rdoc("broom", "glance.lm", text="glance")` returns a one-row data-frame summary of the model,
```{r}
glance(fit)
```
The function `r rdoc("broom", "tidy.lm", text="tidy")` returns a data frame in which each row is a coefficient,
```{r}
tidy(fit)
```
The function `r rdoc("broom", "augment.lm", text="augment")` returns the original data with fitted values, residuals, and other observation level stats from the model appended to it.
```{r}
augment(fit) %>% head()
```


We can plot the results of the bivariate linear regression as follows:
```{r}
ggplot() +
  geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) +
  geom_abline(slope = coef(fit)["d.comp"],
              intercept = coef(fit)["(Intercept)"])
```

A more general way to plot the predictions of the model against the data 
is to use the methods described in [Ch 23.3.3](http://r4ds.had.co.nz/model-basics.html#visualising-models) of R4DS.
Create an evenly spaced grid of values of `d.comp`, and add predictions 
of the model to it.
```{r}
grid <- face %>%
  data_grid(d.comp) %>%
  add_predictions(fit)
head(grid)
```
Now we can plot the regression line and the original data just like any other plot.
```{r}
ggplot() +
  geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) +
  geom_line(data = grid, mapping = aes(x = d.comp, y = pred))
```
This method is more complicated than the `geom_abline` method for a bivariate regerssion, but will work for more complicated models, while the `geom_abline` method won't.


Note that `r ggdoc("geom_smooth")` can be used to add a regression line to a data-set.
```{r}
ggplot(data = face, mapping = aes(x = d.comp, y = diff.share)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```
The argument `method = "lm"` specifies that the function `lm` is to be used to generate fitted values. 
It is equivalent to running the regression `lm(y ~ x)` and plotting the regression line, where `y` and `x` are the aesthetics specified by the mappings.
The argument `se = FALSE` tells the function not to plot the confidence interval of the regresion (discussed later).

### Regresion towards the mean

### Merging Data Sets in R

See the `r R4DS` chapter [Relational data](http://r4ds.had.co.nz/relational-data.html).

Merge - or intter join the data frames by state:
```{r pres12,message=FALSE}
pres12 <- read_csv(qss_data_url("prediction", "pres12.csv"))
```

```{r}
full_join(pres08, pres12, by = "state")
```

*Note: this could be a question. How would you change the .x, .y suffixes to something more informative*
To avoid the duplicate names, or change them, you can rename before merging, or
use the `suffix` argument:
```{r}
pres <- full_join(pres08, pres12, by = "state", suffix = c("_08", "_12"))
pres
```


The **dplyr** equivalent functions for `r rdoc("base", "cbind")` is `r rdoc("dplyr", "bind_cols")`.

```{r}
pres <- pres %>%
  mutate(Obama2008.z = scale(Obama_08),
         Obama2012.z = scale(Obama_12))
```

Scatterplot of states with vote shares in 2008 and 2012
```{r}
ggplot(pres, aes(x = Obama2008.z, y = Obama2012.z, label = state)) +
  geom_text() +
  geom_abline() +
  coord_fixed() +
  scale_x_continuous("Obama's standardized vote share in 2008",
                     limits = c(-4, 4)) +  
  scale_y_continuous("Obama's standardized vote share in 2012",
                     limits = c(-4, 4))
```

To calcualte the bottom and top quartiles

```{r}
pres %>%
  filter(Obama2008.z < quantile(Obama2008.z, 0.25)) %>%
  summarise(improve = mean(Obama2012.z > Obama2008.z))

pres %>%
  filter(Obama2008.z < quantile(Obama2008.z, 0.75)) %>%
  summarise(improve = mean(Obama2012.z > Obama2008.z))
```

**Challenge:** Why is it important to standardize the vote shares?

### Model Fit

```{r}
florida <- read_csv(qss_data_url("prediction", "florida.csv"))
fit2 <- lm(Buchanan00 ~ Perot96, data = florida)
summary(fit2)
```

In addition to,
```{r}
summary(fit2)$r.squared
```
we can get the R2 squared value from the data frame `r rdoc("broom", "glance.lm", "glance")` returns:
```{r}
glance(fit2)
```

We can get predictions and residuals on the original data frame using the `r pkg_link("modelr")` functions `r rdoc("modelr", "add_residuals")` and `r rdoc("modelr", "add_predictions")`
```{r}
florida <-
  florida %>%
  add_predictions(fit2) %>%
  add_residuals(fit2)
glimpse(florida)
```
There are now two new columns in `florida`, `pred` with the fitted values (predictions), and `resid` with the residuals.

Use `fit2_augment` to create a residual plot:
```{r}
fit2_resid_plot <-
  ggplot(florida, aes(x = pred, y = resid)) +
  geom_ref_line(h = 0) +
  geom_point() +  
  labs(x = "Fitted values", y = "residuals")
fit2_resid_plot
```
Note, we use the function `r rdoc("modelr", "geom_refline")` to add a reference line at 0.

Let's add some labels to points, who is that outlier?
```{r}
fit2_resid_plot +
  geom_label(aes(label = county))
```

The outlier county is "Palm Beach"
```{r}
arrange(florida) %>%
  arrange(desc(abs(resid))) %>%
  select(county, resid) %>%
  head()
```

![Ted Mosby dressed as a Hanging Chad in "How I Met Your Mother"](./images/hanging-chad.png)
