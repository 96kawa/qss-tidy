# Uncertainty

## Prerequisites

```{r message=FALSE}
library("tidyverse")
library("forcats")
library("lubridate")
library("stringr")
library("modelr")
library("broom")
```

## Estimation

**Original:**
```{r}
n <- 100 # sample size
mu0 <- 0 # mean of Y_i(0)
sd0 <- 1 # standard deviation of Y_i(0)
mu1 <- 1 # mean of Y_i(1)
sd1 <- 1 # standard deviation of Y_i(1)

## generate a sample
Y0 <- rnorm(n, mean = mu0, sd = sd0)
Y1 <- rnorm(n, mean = mu1, sd = sd1)
tau <- Y1 - Y0 # individual treatment effect
## true value of the sample average treatment effect
SATE <- mean(tau)
SATE

## repeatedly conduct randomized controlled trials
sims <- 5000 # repeat 5,000 times, we could do more
diff.means <- rep(NA, sims)  # container

for (i in 1:sims) {
    ## randomize the treatment by sampling of a vector of 0's and 1's
    treat <- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE)
    ## difference-in-means
    diff.means[i] <- mean(Y1[treat == 1]) - mean(Y0[treat == 0])
}

## estimation error for SATE
est.error <- diff.means - SATE
summary(est.error)
```

**Tidyverse**
```{r}
n <- 100
mu0 <- 0
sd0 <- 1
mu1 <- 1
sd1 <- 1
smpl <- tibble(id = seq_len(n),
               Y0 = rnorm(n, mean = mu0, sd = sd0),
               Y1 = rnorm(n, mean = mu1, sd = sd1),
               tau = Y1 - Y0)
SATE <- mean(smpl[["tau"]])
SATE
```


```{r}
sims <- 5000

sim_treat <- function(smpl) {
  SATE <- mean(smpl[["tau"]])
  # indexes of obs receiving treatment
  idx <- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE)
  # treat variable are those receiving treatment, else 0
  smpl[["treat"]] <- as.integer(seq_len(nrow(smpl)) %in% idx)
  smpl %>%
    mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %>%
    group_by(treat) %>%
    summarise(Y_obs = mean(Y_obs)) %>%
    spread(treat, Y_obs) %>%
    mutate(diff = `1` - `0`,
           est_error = diff - SATE)
}

diff_means <- map_df(seq_len(sims), ~ sim_treat(smpl))
summary(diff_means[["est_error"]])
```

**Original:**
```{r eval=FALSE}
## PATE simulation
PATE <- mu1 - mu0
diff.means <- rep(NA, sims)  
for (i in 1:sims) {
    ## generate a sample for each simulation: this used to be outside of loop
    Y0 <- rnorm(n, mean = mu0, sd = sd0)
    Y1 <- rnorm(n, mean = mu1, sd = sd1)
    treat <- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE)
    diff.means[i] <- mean(Y1[treat == 1]) - mean(Y0[treat == 0])
}

## estimation error for PATE
est.error <- diff.means - PATE

## unbiased 
summary(est.error) 
```

**tidyverse:**
```{r}
PATE <- mu1 - mu0

sim_pate <- function(mu0, mu1, sd0, sd1) {
  PATE <- mu1 - mu0
  smpl <- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0),
                 Y1 = rnorm(n, mean = mu1, sd = sd1),
                 tau = Y1 - Y0)
  # indexes of obs receiving treatment
  idx <- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE)
  # treat variable are those receiving treatment, else 0
  smpl[["treat"]] <- as.integer(seq_len(nrow(smpl)) %in% idx)
  smpl %>%
    mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %>%
    group_by(treat) %>%
    summarise(Y_obs = mean(Y_obs)) %>%
    spread(treat, Y_obs) %>%
    mutate(diff = `1` - `0`,
           est_error = diff - PATE)
}

diff_means <-
  map_df(seq_len(sims), ~ sim_pate(mu0, mu1, sd0, sd1)) 

summary(diff_means[["est_error"]])

```

### Standard Error

**Original:**
```{r eval=FALSE}
hist(diff.means, freq = FALSE, xlab = "difference-in-means estimator",
     main = "sampling distribution")
abline(v = SATE, col = "red") # true value of SATE 
text(0.6, 2.4, "true SATE", col = "red")
```

**tidyverse:**
```{r}
ggplot(diff_means, aes(x = diff)) +
  geom_histogram() +
  geom_vline(xintercept = PATE, colour = "white", size = 2) +
  ggtitle("sampling distribution")
```

**Original:**
```{r eval=FALSE}
## PATE simulation with standard error
sims <- 5000 
diff.means <- se <- rep(NA, sims)  # container for standard error added
for (i in 1:sims) {
    ## generate a sample
    Y0 <- rnorm(n, mean = mu0, sd = sd0)
    Y1 <- rnorm(n, mean = mu1, sd = sd1)
    ## randomize the treatment by sampling of a vector of 0's and 1's
    treat <- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE)
    diff.means[i] <- mean(Y1[treat == 1]) - mean(Y0[treat == 0])
    ## standard error
    se[i] <- sqrt(var(Y1[treat == 1]) / (n / 2) + var(Y0[treat == 0]) / (n / 2)) 
}

## standard deviation of difference-in-means
sd(diff.means)
## mean of standard errors
mean(se) 
```

**tidyverse:**
```{r}
sim_pate_se <- function(mu0, mu1, sd0, sd1) {
  PATE <- mu1 - mu0
  smpl <- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0),
                 Y1 = rnorm(n, mean = mu1, sd = sd1),
                 tau = Y1 - Y0)
  # indexes of obs receiving treatment
  idx <- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE)
  # treat variable are those receiving treatment, else 0
  smpl[["treat"]] <- as.integer(seq_len(nrow(smpl)) %in% idx)
  smpl %>%
    mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %>%
    group_by(treat) %>%
    summarise(mean = mean(Y_obs), var = var(Y_obs),
              nobs = n()) %>%
    summarise(diff_mean = diff(mean), 
              se = sqrt(sum(var / nobs)),
              est_error = diff_mean - PATE)
}
diff_means <-
  map_df(seq_len(sims), ~ sim_pate_se(mu0, mu1, sd0, sd1)) 
```
```{r}
summary(diff_means[["se"]])
```

### Confidence Intervals

**original:**
```{r eval=FALSE}
## empty container matrices for 2 sets of confidence intervals
ci95 <- ci90 <- matrix(NA, ncol = 2, nrow = sims)
## 95 percent confidence intervals
ci95[, 1] <- diff.means - qnorm(0.975) * se # lower limit
ci95[, 2] <- diff.means + qnorm(0.975) * se # upper limit

## 90 percent confidence intervals
ci90[, 1] <- diff.means - qnorm(0.95) * se # lower limit
ci90[, 2] <- diff.means + qnorm(0.95) * se # upper limit

## coverage rate for 95% confidence interval
mean(ci95[, 1] <= 1 & ci95[, 2] >= 1)
## coverage rate for 90% confidence interval
mean(ci90[, 1] <= 1 & ci90[, 2] >= 1)
```

**tidyverse:**
```{r}

```



**original:**
```{r eval=FALSE}
p <- 0.6 # true parameter value
n <- c(50, 100, 1000) # 3 sample sizes to be examined
alpha <- 0.05 
sims <- 5000 # number of simulations
results <- rep(NA, length(n)) # a container for results

## loop for different sample sizes
for (i in 1:length(n)) { 
    ci.results <- rep(NA, sims) # a container for whether CI includes truth
    ## loop for repeated hypothetical survey sampling
    for (j in 1:sims) {
        data <- rbinom(n[i], size = 1, prob = p) # simple random sampling
        x.bar <- mean(data) # sample proportion as an estimate
        s.e. <- sqrt(x.bar * (1 - x.bar) / n[i]) # standard errors
        ci.lower <- x.bar - qnorm(1 - alpha/2) * s.e.
        ci.upper <- x.bar + qnorm(1 - alpha/2) * s.e.
        ci.results[j] <- (p >= ci.lower) & (p <= ci.upper) 
    }
    ## proportion of CIs that contain the true value
    results[i] <- mean(ci.results)
}
results
```

**tidyverse:**
```{r}

sim_binom_ci <- function(size, prob =  p) {
  
}

```


### Margin of Error and Sample Size Calculation in Polls

**Original:** 
```{r eval=FALSE}
par(cex = 1.5, lwd = 2) 
MoE <- c(0.01, 0.03, 0.05)  # the desired margin of error
p <- seq(from = 0.01, to = 0.99, by = 0.01)
n <- 1.96^2 * p * (1 - p) / MoE[1]^2
plot(p, n, ylim = c(-1000, 11000), xlab = "Population proportion", 
     ylab = "Sample size", type = "l")
lines(p, 1.96^2 * p * (1 - p) / MoE[2]^2, lty = "dashed")
lines(p, 1.96^2 * p * (1 - p) / MoE[3]^2, lty = "dotted")
text(0.4, 10000, "margin of error = 0.01")
text(0.4, 1800, "margin of error = 0.03") 
text(0.6, -200, "margin of error = 0.05")
```

**tidyverse** Write a function to calculate the sample size needed for a given proportion.
```{r}
moe_pop_prop <- function(MoE) {
  tibble(p = seq(from = 0.01, to = 0.99, by = 0.01),
         n = 1.96 ^ 2 * p * (1 - p) / MoE ^ 2,
         MoE = MoE)
}
moe_pop_prop(0.01)
```
Then use `r rdoc("purrr", "map_df")` to call this function for different margins of error, and return the entire thing as a data frame with columns: `n`, `p`, and `MoE`.
```{r}
MoE <- c(0.01, 0.03, 0.05)
props <- map_df(MoE, moe_pop_prop)
```
Since its a data frame, its easy to plot with `r rdoc("ggplot2", "ggplot")`:
```{r}
ggplot(props, aes(x = p, y = n, colour = factor(MoE))) +
  geom_line() +
  labs(colour = "margin of error",
       x = "population proportion",
       y = "sample size") +
  theme(legend.position = "bottom")
```



**Original:**
```{r eval=FALSE}
## election and polling results, by state
pres08 <- read.csv("pres08.csv")
polls08 <- read.csv("polls08.csv")

## convert to a Date object
polls08$middate <- as.Date(polls08$middate)
## number of days to the election day
polls08$DaysToElection <- as.Date("2008-11-04") - polls08$middate

## create a matrix place holder
poll.pred <- matrix(NA, nrow = 51, ncol = 3)

## state names which the loop will iterate through
st.names <- unique(pres08$state)
## add labels for easy interpretation later on
row.names(poll.pred) <- as.character(st.names)

## loop across 50 states plus DC
for (i in 1:51){
    ## subset the ith state
    state.data <- subset(polls08, subset = (state == st.names[i]))
    ## subset the latest polls within the state
    latest <- state.data$DaysToElection == min(state.data$DaysToElection) 
    ## compute the mean of latest polls and store it
    poll.pred[i, 1] <- mean(state.data$Obama[latest]) / 100
}

## upper and lower confidence limits 
n <- 1000 # sample size
alpha <- 0.05
s.e. <- sqrt(poll.pred[, 1] * (1 - poll.pred[, 1]) / n) # standard error
poll.pred[, 2] <- poll.pred[, 1] - qnorm(1 - alpha/2) * s.e.
poll.pred[, 3] <- poll.pred[, 1] + qnorm(1 - alpha/2) * s.e.
```

**tidyverse:** `r rdoc("readr", "read_csv")` already recognizes the date columns, so we don't need to convert them.
```{r message=FALSE}
ELECTION_DATE <- ymd(20081104)
pres08 <- read_csv(qss_data_url("uncertainty", "pres08.csv"))

polls08 <- read_csv(qss_data_url("uncertainty", "polls08.csv")) %>%
  mutate(DaysToElection = as.integer(ELECTION_DATE - middate))
```
For each state calculate the mean of the latest polls,
```{r}
poll_pred <-
  polls08 %>%
  group_by(state) %>%
  # latest polls in the state
  filter(DaysToElection == min(DaysToElection)) %>%
  # take mean of latest polls and convert from 0-100 to 0-1
  summarise(Obama = mean(Obama) / 100)

# Add confidence itervals
# sample size
sample_size <- 1000
# confidence level
alpha <- 0.05
poll_pred <- 
  poll_pred %>%
  mutate(se = sqrt(Obama * (1 - Obama) / sample_size),
         ci_lwr = Obama + qnorm(alpha / 2) * se,
         ci_upr = Obama + qnorm(1 - alpha / 2) * se)

# Add actual outcome
poll_pred <-
  left_join(poll_pred, 
            select(pres08, state, actual = Obama), 
            by = "state") %>%
  mutate(actual = actual / 100,
         covers = (ci_lwr <= actual) & (actual <= ci_upr))
poll_pred
```


**original:**
```{r eval=FALSE}
par(cex = 1.5)
alpha <- 0.05
plot(pres08$Obama / 100, poll.pred[, 1], xlim = c(0, 1), ylim = c(0, 1),
     xlab = "Obama's vote share", ylab = "Poll prediction")
abline(0, 1)

## adding 95% confidence intervals for each state
for (i in 1:51) {
    lines(rep(pres08$Obama[i] / 100, 2), c(poll.pred[i, 2], poll.pred[i, 3]))
}
## proportion of confidence intervals that contain the election day outcome
mean((poll.pred[, 2] <= pres08$Obama / 100) & 
         (poll.pred[, 3] >= pres08$Obama / 100))
```

**tidyverse:** In the plot, color the point ranges by whether they include the election day outcome.
```{r}
ggplot(poll_pred, aes(x = actual, y = Obama,
                      ymin = ci_lwr, ymax = ci_upr,
                      colour = covers)) +
  geom_abline(intercept = 0, slope = 1, colour = "white", size = 2) +
  geom_pointrange() +
  scale_y_continuous("Poll prediction", limits = c(0, 1)) +
  scale_x_continuous("Obama's vote share", limits = c(0, 1)) +
  scale_colour_discrete("CI includes result?") +
  coord_fixed() +
  theme(legend.position = "bottom")
```
Proportion of polls with confidence intervals that include the election outcome?
```{r}
poll_pred %>% 
  summarise(mean(covers))
```

**original:**
```{r eval=FALSE}
## bias
bias <- mean(poll.pred[, 1] - pres08$Obama / 100)
bias

## bias corrected estimate
poll.bias <- poll.pred[, 1] - bias
## bias-corrected standard error 
s.e.bias <- sqrt(poll.bias * (1 - poll.bias) / n)
## bias-corrected 95% confidence interval
ci.bias.lower <- poll.bias - qnorm(1 - alpha / 2) * s.e.bias
ci.bias.upper <- poll.bias + qnorm(1 - alpha / 2) * s.e.bias

## proportion of bias-corrected CIs that contain the election day outcome
mean((ci.bias.lower <= pres08$Obama / 100) & 
         (ci.bias.upper >= pres08$Obama / 100))
```

**tidyverse:**
```{r}
poll_pred <-
  poll_pred %>%
  # calc bias
  mutate(bias = Obama - actual) %>%
  # bias corrected prediction, se, and CI
  mutate(Obama_bc = Obama - mean(bias),
         se_bc = sqrt(Obama_bc * (1 - Obama_bc) / sample_size),
         ci_lwr_bc = Obama_bc + qnorm(alpha / 2) * se_bc,
         ci_upr_bc = Obama_bc + qnorm(1 - alpha / 2) * se_bc,
         covers_bc = (ci_lwr_bc <= actual) & (actual <= ci_upr_bc))

poll_pred %>%
  summarise(mean(covers_bc))
  
```



### Analyais of Randomized Controlled Trials

**original:**
```{r eval = FALSE}
par(cex = 1.5)
## read in data
STAR <- read.csv("STAR.csv", head = TRUE)
hist(STAR$g4reading[STAR$classtype == 1], freq = FALSE, xlim = c(500, 900),
     ylim = c(0, 0.01), main = "Small class",
     xlab = "Fourth-grade reading test score")
abline(v = mean(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE), 
       col = "red")

hist(STAR$g4reading[STAR$classtype == 2], freq = FALSE, xlim = c(500, 900),
     ylim = c(0, 0.01), main = "Regular class",
     xlab = "Fourth-grade reading test score")
abline(v = mean(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE), 
       col = "red")
```

**tidyverse:**
```{r message=FALSE}
STAR <- read_csv(qss_data_url("uncertainty", "STAR.csv")) %>%
  mutate(classtype = factor(classtype,
                            labels = c("small class", "regular class",
                                       "regular class with aid")))

classtype_means <- 
  STAR %>%
  group_by(classtype) %>%
  summarise(g4reading = mean(g4reading, na.rm = TRUE))

classtypes_used <- c("small class", "regular class")

ggplot(filter(STAR,
              classtype %in% classtypes_used,
              !is.na(g4reading)),
       aes(x = g4reading, y = ..density..)) +
  geom_histogram(binwidth = 20) +
  geom_vline(data = filter(classtype_means, classtype %in% classtypes_used),
             mapping = aes(xintercept = g4reading), 
             colour = "white", size = 2) +
  facet_grid(classtype ~ .) + 
  labs(x = "Fourth grade reading score", y = "Density")
```


**Original:**
```{r eval = FALSE}
## estimate and standard error for small class
n.small <- sum(STAR$classtype == 1 & !is.na(STAR$g4reading))
est.small <- mean(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE)
se.small <- sd(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE) / 
    sqrt(n.small)
est.small
se.small

## estimate and standard error for regular class
n.regular <- sum(STAR$classtype == 2 & !is.na(STAR$classtype) & 
                     !is.na(STAR$g4reading))
est.regular <- mean(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE)
se.regular <- sd(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE) / 
    sqrt(n.regular)
est.regular
se.regular

alpha <- 0.05
## 95% confidence intervals for small class
ci.small <- c(est.small - qnorm(1 - alpha / 2) * se.small, 
              est.small + qnorm(1 - alpha / 2) * se.small)
ci.small

## 95% confidence intervals for regular class
ci.regular <- c(est.regular - qnorm(1 - alpha / 2) * se.regular,
                est.regular + qnorm(1 - alpha / 2) * se.regular)
ci.regular
```

**tidyverse:**
```{r}
alpha <- 0.05

star_estimates <- 
  STAR %>%
  filter(!is.na(g4reading)) %>%
  group_by(classtype) %>%
  summarise(n = n(),
            est = mean(g4reading),
            se = sd(g4reading) / sqrt(n)) %>%
  mutate(lwr = est + qnorm(alpha / 2) * se,
         upr = est + qnorm(1 - alpha / 2) * se)

star_estimates
```

**Original:**
```{r eval = FALSE}
## difference-in-means estimator
ate.est <- est.small - est.regular
ate.est

## standard error and 95% confidence interval
ate.se <- sqrt(se.small^2 + se.regular^2)
ate.se

ate.ci <- c(ate.est - qnorm(1 - alpha / 2) * ate.se,
            ate.est + qnorm(1 - alpha / 2) * ate.se)
ate.ci
```

**tidyverse:** Difference-in-means estimator
```{r}
star_estimates %>%
  filter(classtype %in% c("small class", "regular class")) %>%
  # ensure that it is ordered small then regular
  arrange(desc(classtype)) %>%
  summarise(
    se = sqrt(sum(se ^ 2)),
    est = diff(est)
  ) %>%
  mutate(ci_lwr = est + qnorm(alpha / 2) * se,
         ci_up = est + qnorm(1 - alpha / 2) * se)
```

Use we could use `r rdoc("tidyr", "spread")` and `r rdoc("tidyr", "gather")`:
```{r}
star_ate <-
  star_estimates %>%
  filter(classtype %in% c("small class", "regular class")) %>%
  mutate(classtype = fct_recode(factor(classtype), 
                                "small" = "small class",
                                "regular" = "regular class")) %>%
  select(classtype, est, se) %>%
  gather(stat, value, -classtype) %>%
  unite(variable, stat, classtype)  %>%
  spread(variable, value) %>%
  mutate(ate_est = est_small - est_regular,
         ate_se = sqrt(se_small ^ 2 + se_regular ^ 2),
         ci_lwr = ate_est + qnorm(alpha / 2) * ate_se,
         ci_upr = ate_est + qnorm(1 - alpha / 2) * ate_se) 
star_ate 
```


### Analysis Based on Student's t-Distribution

**Original:**
```{r eval=FALSE}
## 95% CI for small class  
c(est.small - qt(0.975, df = n.small - 1) * se.small, 
  est.small + qt(0.975, df = n.small - 1) * se.small)

## 95% CI based on the central limit theorem
ci.small

## 95% CI for regular class
c(est.regular - qt(0.975, df = n.regular - 1) * se.regular, 
  est.regular + qt(0.975, df = n.regular - 1) * se.regular)

## 95% CI based on the central limit theorem
ci.regular
```

No code needs to be adjusted.

**Original:**
```{r eval=FALSE}
t.ci <- t.test(STAR$g4reading[STAR$classtype == 1], 
               STAR$g4reading[STAR$classtype == 2])
t.ci
```

**tidyverse:** Use `r rdoc("dplyr", "filter")` to subset.
```{r}
t.test(filter(STAR, classtype == "small class")$g4reading,
       filter(STAR, classtype == "regular class")$g4reading)
```
The function `r rdoc("stat", "t.test")` can also take a formula as its first parameter.
```{r}
t.test(g4reading ~ classtype,
       data = filter(STAR, classtype %in% c("small class", "regular class")))
```


## Hypothesis Testing

### Tea-Testing Experiment

**Original:**
```{r eval=FALSE}
## truth: enumerate the number of assignment combinations
true <- c(choose(4, 0) * choose(4, 4), 
          choose(4, 1) * choose(4, 3), 
          choose(4, 2) * choose(4, 2), 
          choose(4, 3) * choose(4, 1), 
          choose(4, 4) * choose(4, 0))
true

## compute probability: divide it by the total number of events
true <- true / sum(true) 
## number of correctly classified cups as labels
names(true) <- c(0, 2, 4, 6, 8)
true
```

**tidyverse:** Generate the combinations more pro grammatically,
and save the true distribution in a data frame rather than a vector.
```{r}
# Number of cups of tea
cups <- 4
# Number guessed correctly
k <- c(0, seq_len(cups))
true <-  tibble(correct = k * 2,
                n = choose(cups, k) * choose(cups, cups - k)) %>%
  mutate(prob = n / sum(n))
true
```


**Original:** 
```{r eval=FALSE}
## simulations
sims <- 1000
## lady's guess: M stands for `Milk first,' T stands for `Tea first'
guess <- c("M", "T", "T", "M", "M", "T", "T", "M") 
correct <- rep(NA, sims) # place holder for number of correct guesses
for (i in 1:sims) {
    ## randomize which cups get Milk/Tea first
    cups <- sample(c(rep("T", 4), rep("M", 4)), replace = FALSE)
    correct[i] <- sum(guess == cups) # number of correct guesses
}
## estimated probability for each number of correct guesses
prop.table(table(correct)) 
## comparison with analytical answers; the differences are small
prop.table(table(correct)) - true
```

**tidyverse:** As with other simulations, rewrite it to use a function and map function rather than a for loop.
```{r}
sims <- 1000

guess <- tibble(guess = c("M", "T", "T", "M", "M", "T", "T", "M"))

randomize_tea <- function(df) {
  # randomize the order of teas
  assignment <- sample_frac(df, 1) %>%
    rename(actual = guess)
  bind_cols(df, assignment) %>%
    summarise(correct = sum(guess == actual))
}

approx <-
  map_df(seq_len(sims), ~ randomize_tea(guess)) %>%
  count(correct) %>%
  mutate(prob = n / sum(n))

left_join(select(approx, correct, prob_sim = prob),
          select(true, correct, prob_exact = prob),
          by = "correct") %>%
  mutate(diff = prob_sim - prob_exact)
```


### The General Framework

The test functions like `r rdoc("stat", "fisher.test")` do not work particularly well with data frames, and expect vectors or matrices as input, so tidyverse functions are less directly applicable

**Original:**
```{r eval=FALSE}

## all correct
x <- matrix(c(4, 0, 0, 4), byrow = TRUE, ncol = 2, nrow = 2)
## six correct
y <- matrix(c(3, 1, 1, 3), byrow = TRUE, ncol = 2, nrow = 2)

## `M' milk first, `T' tea first
rownames(x) <- colnames(x) <- rownames(y) <- colnames(y) <- c("M", "T")
x
y

## one-sided test for 8 correct guesses
fisher.test(x, alternative = "greater") 

## two-sided test for 6 correct guesses
fisher.test(y) 
```

```{r}
# all guesses correct
x <- tribble(~Guess, ~Truth, ~Number,
             "Milk", "Milk", 4L,
             "Milk", "Tea", 0L,
             "Tea", "Milk", 0L,
             "Tea", "Tea", 4L)
x

# 6 correct guesses
y <- x %>%
  mutate(Number = c(3L, 1L, 1L, 3L))
y

# Turn into a 2x2 table for fisher.test
select(spread(x, Truth, Number), -Guess)

# Use spread to make it a 2 x 2 table
fisher.test(select(spread(x, Truth, Number), -Guess),
            alternative = "greater")
fisher.test(select(spread(y, Truth, Number), -Guess))
```

### One-Sample Tests

**Original:**
```{r}
n <- 1018
x.bar <- 550 / n
se <- sqrt(0.5 * 0.5 / n) # standard deviation of sampling distribution

## upper red area in the figure
upper <- pnorm(x.bar, mean = 0.5, sd = se, lower.tail = FALSE)  

## lower red area in the figure; identical to the upper area
lower <- pnorm(0.5 - (x.bar - 0.5), mean = 0.5, sd = se)  

## two-side p-value
upper + lower

2 * upper

## one-sided p-value
upper

z.score <- (x.bar - 0.5) / se
z.score

pnorm(z.score, lower.tail = FALSE) # one-sided p-value
2 * pnorm(z.score, lower.tail = FALSE) # two-sided p-value

## 99% confidence interval contains 0.5
c(x.bar - qnorm(0.995) * se, x.bar + qnorm(0.995) * se)

## 95% confidence interval does not contain 0.5
c(x.bar - qnorm(0.975) * se, x.bar + qnorm(0.975) * se)

## no continuity correction to get the same p-value as above
prop.test(550, n = n, p = 0.5, correct = FALSE)

## with continuity correction
prop.test(550, n = n, p = 0.5)
prop.test(550, n = n, p = 0.5, conf.level = 0.99)
```

None of that is directly working with data frames or plotting, so nothing to change.

**Original:**
```{r}
## two-sided one-sample t-test
t.test(STAR$g4reading, mu = 710)
```

The only operation on the data frame is extracting a column, so no reason to rewrite this to use `r pkg_link("dplyr")`.

### Two-sample tests

**Original:**
```{r eval=FALSE}
## one-sided p-value
pnorm(-abs(ate.est), mean = 0, sd = ate.se)
## two-sided p-value
2 * pnorm(-abs(ate.est), mean = 0, sd = ate.se)
```

**tidyverse:** The ATE estimates are stored in a data frame, `star_ate`. Note that the `r pkg_link("dplyr")` function `r rdoc("dplyr", "transmute")` is like `mutate`, but only returns the variables specified in the function.
```{r}
star_ate %>%
  transmute(p_value_1sided = pnorm(-abs(ate_est),
                                   mean = 0, sd = ate_se),
            p_value_2sided = 2 * pnorm(-abs(ate_est), mean = 0, 
                                   sd = ate_se))

```

**Original:**
```{r eval=FALSE}
## testing the null of zero average treatment effect
t.test(STAR$g4reading[STAR$classtype == 1], 
       STAR$g4reading[STAR$classtype == 2])
```

**tidyverse:**
```{r}
t.test(g4reading ~ classtype,
  data = filter(STAR, classtype %in% c("small class", "regular class")))
```
or 
```{r}
t.test(filter(STAR, classtype == "small class")$g4reading,
       filter(STAR, classtype == "regular class")$g4reading)
```

**Original:**
```{r eval=FALSE}
resume <- read.csv("resume.csv")

## organize the data in tables
x <- table(resume$race, resume$call)
x

## one-sided test 
prop.test(x, alternative = "greater")
```

**tidyverse:**
```{r message=FALSE}
resume <- read_csv(qss_data_url("uncertainty", "resume.csv"))

x <- resume %>%
  count(race, call) %>%
  spread(call, n) %>%
  ungroup()
x

prop.test(as.matrix(select(x, -race)), alternative = "greater")
```



**Original:**
```{r eval=FALSE}
## sample size
n0 <- sum(resume$race == "black")
n1 <- sum(resume$race == "white")

## sample proportions
p <- mean(resume$call) # overall
p0 <- mean(resume$call[resume$race == "black"]) # black
p1 <- mean(resume$call[resume$race == "white"]) # white

## point estimate
est <- p1 - p0
est

## standard error
se <- sqrt(p * (1 - p) * (1 / n0 + 1 / n1))
se

## z-statistic
zstat <- est / se
zstat

## one-sided p-value
pnorm(-abs(zstat))
```

**tidyverse:** 
```{r}
## sample size
n0 <- sum(resume$race == "black")
n1 <- sum(resume$race == "white")

## sample proportions
p <- mean(resume$call) # overall
p0 <- mean(filter(resume, race == "black")$call)
p1 <- mean(filter(resume, race == "white")$call)

## point estimate
est <- p1 - p0
est

## standard error
se <- sqrt(p * (1 - p) * (1 / n0 + 1 / n1))
se

## z-statistic
zstat <- est / se
zstat

## one-sided p-value
pnorm(-abs(zstat))
```
The only thing that changed is using `r rdoc("dplyr", "filter")` for selecting the groups.

### Power Analysis

**Original:** 
```{r eval=FALSE}
## set the parameters
n <- 250
p.star <- 0.48 # data generating process
p <- 0.5 # null value
alpha <- 0.05

## critical value
cr.value <- qnorm(1 - alpha / 2) 

## standard errors under the hypothetical data generating process
se.star <- sqrt(p.star * (1 - p.star) / n)

## standard error under the null
se <- sqrt(p * (1 - p) / n)  

## power
pnorm(p - cr.value * se, mean = p.star, sd = se.star) + 
    pnorm(p + cr.value * se, mean = p.star, sd = se.star, lower.tail = FALSE)

## parameters
n1 <- 500
n0 <- 500
p1.star <- 0.05
p0.star <- 0.1

## overall call back rate as a weighted average
p <- (n1 * p1.star + n0 * p0.star) / (n1 + n0) 
## standard error under the null
se <- sqrt(p * (1 - p) * (1 / n1 + 1 / n0)) 
## standard error under the hypothetical data generating process
se.star <- sqrt(p1.star * (1 - p1.star) / n1 + p0.star * (1 - p0.star) / n0)

pnorm(-cr.value * se, mean = p1.star - p0.star, sd = se.star) + 
    pnorm(cr.value * se, mean = p1.star - p0.star, sd = se.star, 
          lower.tail = FALSE)

power.prop.test(n = 500, p1 = 0.05, p2 = 0.1, sig.level = 0.05)
power.prop.test(p1 = 0.05, p2 = 0.1, sig.level = 0.05, power = 0.9)
power.t.test(n = 100, delta = 0.25, sd = 1, type = "one.sample")
power.t.test(power = 0.9, delta = 0.25, sd = 1, type = "one.sample")
power.t.test(delta = 0.25, sd = 1, type = "two.sample", 
             alternative = "one.sided", power = 0.9)
```

None of that code uses data frames, so there's nothing to rewrite.

## Linear Regression Model with Uncertainty

### Linear Regression as a Generative Model

**Original:**
```{r eval=FALSE}
minwage <- read.csv("minwage.csv")

## compute proportion of full employment before minimum wage increase
minwage$fullPropBefore <- minwage$fullBefore /
    (minwage$fullBefore + minwage$partBefore)

## same thing after minimum wage increase
minwage$fullPropAfter <- minwage$fullAfter /
    (minwage$fullAfter + minwage$partAfter)

## an indicator for NJ: 1 if it's located in NJ and 0 if in PA
minwage$NJ <- ifelse(minwage$location == "PA", 0, 1)
```

**tidyverse:**
```{r message=FALSE}
minwage <- read_csv(qss_data_url("uncertainty", "minwage.csv")) %>%
  mutate(fullPropBefore = fullBefore / (fullBefore + partBefore),
         fullPropAfter = fullAfter / (fullAfter + partAfter),
         NJ = as.integer(location == "PA"))
```


**Original:**
```{r eval=FALSE}
fit.minwage <- lm(fullPropAfter ~ -1 + NJ + fullPropBefore + 
                      wageBefore + chain, data = minwage)

## regression result
fit.minwage

fit.minwage1 <- lm(fullPropAfter ~ NJ + fullPropBefore + 
                       wageBefore + chain, data = minwage)
fit.minwage1

predict(fit.minwage, newdata = minwage[1, ])
predict(fit.minwage1, newdata = minwage[1, ])
```

**tidyverse:**
```{r}
fit_minwage <- lm(fullPropAfter ~ -1 + NJ + fullPropBefore +
                    wageBefore + chain, data = minwage)
fit_minwage
fit_minwage1 <- lm(fullPropAfter ~ NJ + fullPropBefore +
                     wageBefore + chain, data = minwage)
fit_minwage1

gather_predictions(slice(minwage, 1), fit_minwage, fit_minwage1) %>%
  select(model, pred)
```


### Inference about coefficients

**Original:**
```{r eval=FALSE}
women <- read.csv("women.csv")
fit.women <- lm(water ~ reserved, data = women)
summary(fit.women)
confint(fit.women) # 95% confidence intervals
```

**tidyverse:** Use the `r rdoc("broom", "tidy")` function to return the coefficients, including confidence intervals, as a data frame:
```{r message=FALSE}
women <- read_csv(qss_data_url("uncertainty", "women.csv"))
fit_women <- lm(water ~ reserved, data = women)
summary(fit_women)
tidy(fit_women)
```


**Original:**
```{r eval=FALSE}
summary(fit.minwage)
confint(fit.minwage)["NJ", ] 
```

**tidyverse:** You need to set `conf.int = TRUE` for `r rdoc("broom", "tidy.lm", "tidy")` to include the confidence interval:
```{r}
summary(fit_minwage)
tidy(fit_minwage, conf.int = TRUE)
```

### Inference about predictions

**original:**
```{r eval=FALSE}
## load the data and subset them into two parties
MPs <- read.csv("MPs.csv")
MPs.labour <- subset(MPs, subset = (party == "labour")) 
MPs.tory <- subset(MPs, subset = (party == "tory"))

## two regressions for labour: negative and positive margin
labour.fit1 <- lm(ln.net ~ margin,
                  data = MPs.labour[MPs.labour$margin < 0, ])
labour.fit2 <- lm(ln.net ~ margin,
                  data = MPs.labour[MPs.labour$margin > 0, ]) 

## two regressions for tory: negative and positive margin
tory.fit1 <- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin < 0, ])
tory.fit2 <- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin > 0, ])
```

**tidyverse:**
```{r message=FALSE}
MPs <- read_csv(qss_data_url("uncertainty", "MPs.csv"))
MPs_labour <- filter(MPs, party == "labour")
MPs_tory <- filter(MPs, party == "tory")
labour_fit1 <- lm(ln.net ~ margin, data = filter(MPs_labour, margin < 0))
labour_fit2 <- lm(ln.net ~ margin, data = filter(MPs_labour, margin > 0))
tory_fit1 <- lm(ln.net ~ margin, data = filter(MPs_tory, margin < 0))
tory_fit2 <- lm(ln.net ~ margin, data = filter(MPs_tory, margin > 0))
```


**original:**
```{r eval=FALSE}
## tory party: prediction at the threshold
tory.y0 <- predict(tory.fit1, interval = "confidence", 
                   newdata = data.frame(margin = 0))
tory.y0

tory.y1 <- predict(tory.fit2, interval = "confidence", 
                   newdata = data.frame(margin = 0))
tory.y1
```

**tidyverse:** Predictions at the threshold. 
The `r pkg_link("broom")` function `r rdoc("broom", "augment")` will return prediction fitted values and standard errors for each value, but not the confidence intervals themselves (we'd have to multiply the correct t-distribution with degrees of freedom.)
So instead, we'll directly use the `r rdoc("stats", "predict.lm", "predict")` function:
```{r}
tory_y0 <-
  predict(tory_fit1, interval = "confidence", 
          newdata = tibble(margin = 0)) %>% as_tibble()
tory_y0

tory_y1 <-
  predict(tory_fit2, interval = "confidence", 
          newdata = tibble(margin = 0)) %>% as_tibble()
tory_y1
```
Alternatively, using `augment` (and assuming a normal distribution since the number of observations is so large its not worth worrying about the t-distribution):
```{r}
tory_y0 <-
  augment(tory_fit1, newdata = tibble(margin = 0)) %>%
  mutate(lwr = .fitted + qnorm(0.025) * .se.fit,
         upr = .fitted + qnorm(0.975) * .se.fit)
tory_y0

tory_y1 <-
  augment(tory_fit2, newdata = tibble(margin = 0)) %>%
  mutate(lwr = .fitted + qnorm(0.025) * .se.fit,
         upr = .fitted + qnorm(0.975) * .se.fit)
tory_y1
```



**Original:**
```{r eval=FALSE}
## range of predictors; min to 0 and 0 to max
y1.range <- seq(from = 0, to = min(MPs.tory$margin), by = -0.01)
y2.range <- seq(from = 0, to = max(MPs.tory$margin), by = 0.01)

## prediction using all the values
tory.y0 <- predict(tory.fit1, interval = "confidence", 
                   newdata = data.frame(margin = y1.range))
tory.y1 <- predict(tory.fit2, interval = "confidence", 
                   newdata = data.frame(margin = y2.range))
```

**tidyverse:**
```{r}
y1_range <- data_grid(filter(MPs_tory, margin <= 0), margin)
tory_y0 <- augment(tory_fit1, newdata = y1_range)


y2_range <- data_grid(filter(MPs_tory, margin >= 0), margin)
tory_y1 <- augment(tory_fit2, newdata = y2_range)

```

**original:**
```{r eval=FALSE}
par(cex = 1.5)   
## plotting the first regression with losers
plot(y1.range, tory.y0[, "fit"], type = "l", xlim = c(-0.5, 0.5),
     ylim = c(10, 15), xlab = "Margin of victory", ylab = "log net wealth")
abline(v = 0, lty = "dotted")
lines(y1.range, tory.y0[, "lwr"], lty = "dashed") # lower CI
lines(y1.range, tory.y0[, "upr"], lty = "dashed") # upper CI

## plotting the second regression with winners
lines(y2.range, tory.y1[, "fit"], lty = "solid")  # point estimates
lines(y2.range, tory.y1[, "lwr"], lty = "dashed") # lower CI 
lines(y2.range, tory.y1[, "upr"], lty = "dashed") # upper CI
```

**tidyverse:**
```{r}
ggplot() +
  geom_ref_line(v = 0) +
  geom_point(aes(y = ln.net, x = margin), data = MPs_tory) +
  # plot losers
  geom_ribbon(aes(x = margin,
                  ymin = .fitted + qnorm(0.025) * .se.fit,
                  ymax = .fitted + qnorm(0.975) * .se.fit),
              data = tory_y0, alpha = 0.3) +  
  geom_line(aes(x = margin, y = .fitted), data = tory_y0) +
  # plot winners
  geom_ribbon(aes(x = margin,
                  ymin = .fitted + qnorm(0.025) * .se.fit,
                  ymax = .fitted + qnorm(0.975) * .se.fit),
              data = tory_y1, alpha = 0.3) +  
  geom_line(aes(x = margin, y = .fitted), data = tory_y1) +
  labs(x = "Margin of vitory", y = "log net wealth")
```

**Original:**
```{r eval=FALSE}
tory.y0 <- predict(tory.fit1, interval = "confidence", se.fit = TRUE, 
                   newdata = data.frame(margin = 0))
tory.y0

tory.y1 <- predict(tory.fit2, interval = "confidence", se.fit = TRUE,
                   newdata = data.frame(margin = 0))
summary(tory.fit1)
```

**tidyverse:**
```{r}
tory_y1 <- augment(tory_fit1, newdata = tibble(margin = 0))
tory_y1
tory_y0 <- augment(tory_fit2, newdata = tibble(margin = 0))
tory_y0
summary(tory_fit1)
summary(tory_fit2)
```


**Original:**
```{r eval=FALSE}
# standard error
se.diff <- sqrt(tory.y0$se.fit^2 + tory.y1$se.fit^2)
se.diff

## point estimate
diff.est <- tory.y1$fit[1, "fit"] - tory.y0$fit[1, "fit"]
diff.est

## confidence interval
CI <- c(diff.est - se.diff * qnorm(0.975), diff.est + se.diff * qnorm(0.975))
CI

## hypothesis test
z.score <- diff.est / se.diff
p.value <- 2 * pnorm(abs(z.score), lower.tail = FALSE) # two-sided p-value
p.value
```


**tidyverse:**
Since we aren't doing anything more with these values, there isn't much benefit in keeping them in data frames.
I just adjust the names to be consistent with earlier code.
```{r}
# standard error
se_diff <- sqrt(tory_y0$.se.fit ^ 2 + tory_y1$.se.fit ^ 2)
se_diff

## point estimate
diff_est <- tory_y1$.fitted - tory_y0$.fitted
diff_est

## confidence interval
CI <- c(diff_est - se_diff * qnorm(0.975),
        diff_est + se_diff * qnorm(0.975))
CI

## hypothesis test
z.score <- diff_est / se_diff
p.value <- 2 * pnorm(abs(z.score), lower.tail = FALSE) # two-sided p-value
p.value
```

