[
["discovery.html", "6 Discovery 6.1 Prerequisites 6.2 Textual data 6.3 Network Data 6.4 Spatial Data in R", " 6 Discovery The idea of tidy data and the common feature of tidyverse packages is that data should be stored in data frames with certain conventions. This works well with naturally tabular data, the type which has been common in social science applications. But there are other domains in which other data structures are more appropriate because they more naturally model the data or processes, or for computational reasons. The three applications in this chapter: text, networks, and spatial data are examples where the tidy data structure is less of an advantage. I will still rely on ggplot2 for plotting, and use tidy verse compatible packages where appropriate. Textual data: tidytext Network data: igraph for network computation, as in the chapter. But several ggplot22 extension packages for plotting the networks. Spatial data: ggplot2 has some built-in support for maps. The map package provides map data. See the R for Data Science section 12.7 Non-tidy data and this post on Non-tidy data by Jeff Leek for more on non-tidy data. 6.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) library(&quot;modelr&quot;) 6.2 Textual data library(&quot;tm&quot;) library(&quot;SnowballC&quot;) library(&quot;tidytext&quot;) This section will primarily use the tidytext package. It is a relatively new package. The tm and quanteda (by Ken Benoit) packages are more established and use the document-term matrix format as described in the QSS chapter. The tidytext package stores everything in a data frame; this may be less efficient than the other packages, but has the benefit of being able to easily take advantage of the tidyverse ecosystem. If your corpus is not too large, this shouldn’t be an issue. See Tidy Text Mining with R for a full introduction to using tidytext. In tidy data, each row is an observation and each column is a variable. In the tidytext package, documents are stored as data frames with one-term-per-row. Original ## load the raw corpus corpus.raw &lt;- Corpus(DirSource(directory = &quot;federalist&quot;, pattern = &quot;fp&quot;)) corpus.raw ## make lower case corpus.prep &lt;- tm_map(corpus.raw, content_transformer(tolower)) ## remove white space corpus.prep &lt;- tm_map(corpus.prep, stripWhitespace) ## remove punctuation corpus.prep &lt;- tm_map(corpus.prep, removePunctuation) ## remove numbers corpus.prep &lt;- tm_map(corpus.prep, removeNumbers) head(stopwords(&quot;english&quot;)) ## remove stop words corpus &lt;- tm_map(corpus.prep, removeWords, stopwords(&quot;english&quot;)) ## finally stem remaining words corpus &lt;- tm_map(corpus, stemDocument) ## the output is truncated here to save space content(corpus[[10]]) # Essay No. 10 We can cast data into the tidytext format either from the Corpus object, or, after processing, from the document-term matrix object. DIR_SOURCE &lt;- file.path(&quot;qss&quot;, &quot;DISCOVERY&quot;, &quot;federalist&quot;) corpus_raw &lt;- Corpus(DirSource(directory = DIR_SOURCE, pattern = &quot;fp&quot;)) corpus_raw #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 85 Use the tidy function to convert it to a data frame with one row per document. corpus_tidy &lt;- tidy(corpus_raw) corpus_tidy #&gt; # A tibble: 85 × 8 #&gt; author datetimestamp description heading id language origin #&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 NA 2017-02-03 17:57:31 NA NA fp01.txt en NA #&gt; 2 NA 2017-02-03 17:57:31 NA NA fp02.txt en NA #&gt; 3 NA 2017-02-03 17:57:31 NA NA fp03.txt en NA #&gt; 4 NA 2017-02-03 17:57:31 NA NA fp04.txt en NA #&gt; 5 NA 2017-02-03 17:57:31 NA NA fp05.txt en NA #&gt; 6 NA 2017-02-03 17:57:31 NA NA fp06.txt en NA #&gt; # ... with 79 more rows, and 1 more variables: text &lt;chr&gt; The text column contains the text of the documents themselves. Since most of the metadata is irrelevant, we’ll delete those columns, keeping only the document (id) and text columns. corpus_tidy &lt;- select(corpus_tidy, id, text) Also, we want to extract the essay number and use that as the document id rather than its file name. corpus_tidy &lt;- mutate(corpus_tidy, document = as.integer(str_extract(id, &quot;\\\\d+&quot;))) %&gt;% select(-id) The tokenizes the document texts. tokens &lt;- corpus_tidy %&gt;% # tokenizes into words and stems them unnest_tokens(word, text, token = &quot;word_stems&quot;) %&gt;% # remove any numbers in the strings mutate(word = str_replace_all(word, &quot;\\\\d+&quot;, &quot;&quot;)) %&gt;% # drop any empty strings filter(word != &quot;&quot;) tokens #&gt; # A tibble: 187,412 × 2 #&gt; document word #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 after #&gt; 2 1 an #&gt; 3 1 unequivoc #&gt; 4 1 experi #&gt; 5 1 of #&gt; 6 1 the #&gt; # ... with 1.874e+05 more rows The unnest_tokens function uses the tokenizers package to tokenize the text. By default, it uses the function which removes punctuation, and lowercases the words. I set the tokenizer to to stem the word, using the SnowballC package. We can remove stop-words with an anti_join on the dataset stop_words data(&quot;stop_words&quot;, package = &quot;tidytext&quot;) tokens &lt;- anti_join(tokens, stop_words, by = &quot;word&quot;) 6.2.1 Document-Term Matrix Original: dtm &lt;- DocumentTermMatrix(corpus) dtm inspect(dtm[1:5, 1:8]) dtm.mat &lt;- as.matrix(dtm) In tokens there is one observation for each token (word) in the each document. This is almost equivalent to a document-term matrix. For a document-term matrix we need documents, and terms as the keys for the data and a column with the number of times the term appeared in the document. dtm &lt;- count(tokens, document, word) head(dtm) #&gt; Source: local data frame [6 x 3] #&gt; Groups: document [1] #&gt; #&gt; document word n #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 abl 1 #&gt; 2 1 absurd 1 #&gt; 3 1 accid 1 #&gt; 4 1 accord 1 #&gt; 5 1 acknowledg 1 #&gt; 6 1 act 1 6.2.2 Topic Discovery Original: library(wordcloud) wordcloud(colnames(dtm.mat), dtm.mat[12, ], max.words = 20) # essay No. 12 wordcloud(colnames(dtm.mat), dtm.mat[24, ], max.words = 20) # essay No. 24 Plot the word-clouds for essays 12 and 24: library(&quot;wordcloud&quot;) filter(dtm, document == 12) %&gt;% {wordcloud(.$word, .$n, max.words = 20)} filter(dtm, document == 24) %&gt;% {wordcloud(.$word, .$n, max.words = 20)} Original: stemCompletion(c(&quot;revenu&quot;, &quot;commerc&quot;, &quot;peac&quot;, &quot;army&quot;), corpus.prep) Original: dtm.tfidf &lt;- weightTfIdf(dtm) # tf-idf calculation dtm.tfidf.mat &lt;- as.matrix(dtm.tfidf) # convert to matrix ## 10 most important words for Paper No. 12 head(sort(dtm.tfidf.mat[12, ], decreasing = TRUE), n = 10) ## 10 most important words for Paper No. 24 head(sort(dtm.tfidf.mat[24, ], decreasing = TRUE), n = 10) tidyverse: Use the function bind_tf_idf to add a column with the tf-idf to the data frame. dtm &lt;- bind_tf_idf(dtm, word, document, n) dtm #&gt; Source: local data frame [38,764 x 6] #&gt; Groups: document [85] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 abl 1 0.00176 0.705 0.001241 #&gt; 2 1 absurd 1 0.00176 1.735 0.003054 #&gt; 3 1 accid 1 0.00176 3.750 0.006601 #&gt; 4 1 accord 1 0.00176 0.754 0.001327 #&gt; 5 1 acknowledg 1 0.00176 1.552 0.002733 #&gt; 6 1 act 1 0.00176 0.400 0.000704 #&gt; # ... with 3.876e+04 more rows The 10 most important words for Paper No. 12 are dtm %&gt;% filter(document == 12) %&gt;% top_n(10, tf_idf) #&gt; Source: local data frame [10 x 6] #&gt; Groups: document [1] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 12 cent 2 0.00240 4.44 0.0107 #&gt; 2 12 coast 3 0.00360 3.75 0.0135 #&gt; 3 12 commerc 8 0.00959 1.11 0.0107 #&gt; 4 12 contraband 3 0.00360 4.44 0.0160 #&gt; 5 12 excis 5 0.00600 2.65 0.0159 #&gt; 6 12 gallon 2 0.00240 4.44 0.0107 #&gt; # ... with 4 more rows and for Paper No. 24, dtm %&gt;% filter(document == 24) %&gt;% top_n(10, tf_idf) #&gt; Source: local data frame [10 x 6] #&gt; Groups: document [1] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 24 armi 7 0.01034 1.26 0.01308 #&gt; 2 24 arsenal 2 0.00295 3.75 0.01108 #&gt; 3 24 dock 3 0.00443 4.44 0.01969 #&gt; 4 24 frontier 3 0.00443 2.83 0.01255 #&gt; 5 24 garrison 6 0.00886 2.83 0.02511 #&gt; 6 24 nearer 2 0.00295 3.34 0.00988 #&gt; # ... with 4 more rows The slightly different results from the book are due to tokenization differences. Original: k &lt;- 4 # number of clusters ## subset The Federalist papers written by Hamilton hamilton &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) dtm.tfidf.hamilton &lt;- dtm.tfidf.mat[hamilton, ] ## run k-means km.out &lt;- kmeans(dtm.tfidf.hamilton, centers = k) km.out$iter # check the convergence; number of iterations may vary ## label each centroid with the corresponding term colnames(km.out$centers) &lt;- colnames(dtm.tfidf.hamilton) for (i in 1:k) { # loop for each cluster cat(&quot;CLUSTER&quot;, i, &quot;\\n&quot;) cat(&quot;Top 10 words:\\n&quot;) # 10 most important terms at the centroid print(head(sort(km.out$centers[i, ], decreasing = TRUE), n = 10)) cat(&quot;\\n&quot;) cat(&quot;Federalist Papers classified: \\n&quot;) # extract essays classified print(rownames(dtm.tfidf.hamilton)[km.out$cluster == i]) cat(&quot;\\n&quot;) } tidyverse: Subset those documents known to have been written by Hamilton. HAMILTON_ESSAYS &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) dtm_hamilton &lt;- filter(dtm, document %in% HAMILTON_ESSAYS) The kmeans function expects the input to be rows for observations and columns for each variable: in our case that would be documents as rows, and words as columns, with the tf-idf as the cell values. We could use spread to do this, but that would be a large matrix. CLUSTERS &lt;- 4 km_out &lt;- kmeans(cast_dtm(dtm_hamilton, document, word, tf_idf), centers = CLUSTERS, nstart = 10) km_out$iter #&gt; [1] 3 Data frame with the unique terms used by Hamilton. I extract these from the column names of the DTM after cast_dtm to ensure that the order is the same as the k-means results. hamilton_words &lt;- tibble(word = colnames(cast_dtm(dtm_hamilton, document, word, tf_idf))) The centers of the clusters is a cluster x word matrix. We want to transpose it and then append columns to hamilton_words so the location of each word in the cluster is listed. dim(km_out$centers) #&gt; [1] 4 3850 hamilton_words &lt;- bind_cols(hamilton_words, as_tibble(t(km_out$centers))) hamilton_words #&gt; # A tibble: 3,850 × 5 #&gt; word `1` `2` `3` `4` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 abl 0.000262 0.000000 0.001004 0.00113 #&gt; 2 absurd 0.000272 0.001430 0.000678 0.00000 #&gt; 3 accid 0.000000 0.000000 0.000292 0.00000 #&gt; 4 accord 0.000319 0.001381 0.000499 0.00000 #&gt; 5 acknowledg 0.000000 0.000767 0.000556 0.00000 #&gt; 6 act 0.000927 0.000838 0.000657 0.00000 #&gt; # ... with 3,844 more rows To find the top 10 words in each centroid, we use top_n with group_by: top_words_cluster &lt;- gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) We can print them out using a for loop for (i in 1:CLUSTERS) { cat(&quot;CLUSTER &quot;, i, &quot;: &quot;, str_c(filter(top_words_cluster, cluster == i)$word, collapse = &quot;, &quot;), &quot;\\n\\n&quot;) } #&gt; CLUSTER 1 : offic, accus, presid, treati, appoint, senat, nomin, governor, impeach, pardon #&gt; #&gt; CLUSTER 2 : court, appeal, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, appel #&gt; #&gt; CLUSTER 3 : confederaci, tax, war, land, revenu, armi, militari, militia, taxat, claus #&gt; #&gt; CLUSTER 4 : presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc This is alternative code that prints out a table: gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) %&gt;% summarise(top_words = str_c(word, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster top_words 1 offic, accus, presid, treati, appoint, senat, nomin, governor, impeach, pardon 2 court, appeal, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, appel 3 confederaci, tax, war, land, revenu, armi, militari, militia, taxat, claus 4 presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc Or to print out the documents in each cluster, enframe(km_out$cluster, &quot;document&quot;, &quot;cluster&quot;) %&gt;% group_by(cluster) %&gt;% summarise(documents = str_c(document, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster documents 1 65, 66, 68, 69, 74, 75, 76, 77, 79 2 81, 82, 83 3 1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60, 61, 70, 71, 72, 73, 78, 80, 84, 85 4 67 6.2.3 Authorship Prediction Original: ## document-term matrix converted to matrix for manipulation dtm1 &lt;- as.matrix(DocumentTermMatrix(corpus.prep)) tfm &lt;- dtm1 / rowSums(dtm1) * 1000 # term frequency per 1000 words ## words of interest words &lt;- c(&quot;although&quot;, &quot;always&quot;, &quot;commonly&quot;, &quot;consequently&quot;, &quot;considerable&quot;, &quot;enough&quot;, &quot;there&quot;, &quot;upon&quot;, &quot;while&quot;, &quot;whilst&quot;) ## select only these words tfm &lt;- tfm[, words] ## essays written by Madison: `hamilton&#39; defined earlier madison &lt;- c(10, 14, 37:48, 58) ## average among Hamilton/Madison essays tfm.ave &lt;- rbind(colSums(tfm[hamilton, ]) / length(hamilton), colSums(tfm[madison, ]) / length(madison)) tfm.ave tidyverse: We’ll create a data-frame with the known MADISON_ESSAYS &lt;- c(10, 14, 37:48, 58) JAY_ESSAYS &lt;- c(2:5, 64) known_essays &lt;- bind_rows(tibble(document = MADISON_ESSAYS, author = &quot;Madison&quot;), tibble(document = HAMILTON_ESSAYS, author = &quot;Hamilton&quot;), tibble(document = JAY_ESSAYS, author = &quot;Jay&quot;)) STYLE_WORDS &lt;- tibble(word = c(&quot;although&quot;, &quot;always&quot;, &quot;commonly&quot;, &quot;consequently&quot;, &quot;considerable&quot;, &quot;enough&quot;, &quot;there&quot;, &quot;upon&quot;, &quot;while&quot;, &quot;whilst&quot;)) hm_tfm &lt;- unnest_tokens(corpus_tidy, word, text) %&gt;% count(document, word) %&gt;% # term freq per 1000 words group_by(document) %&gt;% mutate(count = n / sum(n) * 1000) %&gt;% select(-n) %&gt;% inner_join(STYLE_WORDS, by = &quot;word&quot;) %&gt;% # merge known essays left_join(known_essays, by = &quot;document&quot;) %&gt;% # make wide with each word a column # fill empty values with 0 spread(word, count, fill = 0) Calculate average usage by each author of each word hm_tfm %&gt;% # remove docs with no author filter(!is.na(author)) %&gt;% # convert back to long (tidy) format to make it easier to summarize gather(word, count, -document, -author) %&gt;% # calculate averge document word usage by author group_by(author, word) %&gt;% summarise(avg_count = mean(count)) %&gt;% spread(author, avg_count) %&gt;% knitr::kable() word Hamilton Jay Madison although 0.013 0.584 0.222 always 0.563 0.999 0.166 commonly 0.198 0.139 0.000 consequently 0.019 0.503 0.371 considerable 0.406 0.087 0.133 enough 0.295 0.000 0.000 there 3.303 1.026 0.917 upon 3.291 0.120 0.164 while 0.274 0.207 0.000 whilst 0.005 0.000 0.315 Original: author &lt;- rep(NA, nrow(dtm1)) # a vector with missing values author[hamilton] &lt;- 1 # 1 if Hamilton author[madison] &lt;- -1 # -1 if Madison ## data frame for regression author.data &lt;- data.frame(author = author[c(hamilton, madison)], tfm[c(hamilton, madison), ]) hm.fit &lt;- lm(author ~ upon + there + consequently + whilst, data = author.data) hm.fit hm.fitted &lt;- fitted(hm.fit) # fitted values sd(hm.fitted) tidyverse: author_data &lt;- hm_tfm %&gt;% ungroup() %&gt;% filter(is.na(author) | author != &quot;Jay&quot;) %&gt;% mutate(author2 = case_when(.$author == &quot;Hamilton&quot; ~ 1, .$author == &quot;Madison&quot; ~ -1, TRUE ~ NA_real_)) hm_fit &lt;- lm(author2 ~ upon + there + consequently + whilst, data = author_data) hm_fit #&gt; #&gt; Call: #&gt; lm(formula = author2 ~ upon + there + consequently + whilst, #&gt; data = author_data) #&gt; #&gt; Coefficients: #&gt; (Intercept) upon there consequently whilst #&gt; -0.195 0.213 0.118 -0.596 -0.910 author_data &lt;- author_data %&gt;% add_predictions(hm_fit) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;)) sd(author_data$pred) #&gt; [1] 0.79 These coefficients are a little different, probably due to differences in the tokenization procedure, and in particular, the document size normalization. 6.2.4 Cross-Validation Original: ## proportion of correctly classified essays by Hamilton mean(hm.fitted[author.data$author == 1] &gt; 0) ## proportion of correctly classified essays by Madison mean(hm.fitted[author.data$author == -1] &lt; 0) tidyverse: For cross-validation, I rely on the modelr package function RDoc(&quot;modelr::crossv_kfold&quot;). See the tutorial Cross validation of linear regression with modelr for more on using modelr for cross validation or k-fold cross-validation with modelr and broom. In sample, this regression perfectly predicts the authorship of the documents with known authors. author_data %&gt;% filter(!is.na(author)) %&gt;% group_by(author) %&gt;% summarise(`Proportion Correct` = mean(author == pred_author)) #&gt; # A tibble: 2 × 2 #&gt; author `Proportion Correct` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1 #&gt; 2 Madison 1 Create the cross-validation data-sets using . As in the chapter, I will use a leave-one-out cross-validation, which is a k-fold crossvalidation where k is the number of observations. To simplify this, I define the crossv_loo function that runs crossv_kfold with k = nrow(data). crossv_loo &lt;- function(data, id = &quot;.id&quot;) { crossv_kfold(data, k = nrow(data), id = id) } # leave one out cross-validation object cv &lt;- author_data %&gt;% filter(!is.na(author)) %&gt;% crossv_loo() Now estimate the model for each training dataset models &lt;- purrr::map(cv$train, ~ lm(author2 ~ upon + there + consequently + whilst, data = ., model = FALSE)) Note that I use purrr::map to ensure that the correct map() function is used since the maps package also defines a map. Now calculate the test performance on the held out observation, test &lt;- map2_df(models, cv$test, function(mod, test) { add_predictions(as.data.frame(test), mod) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;), correct = (pred_author == author)) }) test %&gt;% group_by(author) %&gt;% summarise(mean(correct)) #&gt; # A tibble: 2 × 2 #&gt; author `mean(correct)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1.000 #&gt; 2 Madison 0.786 Original: n &lt;- nrow(author.data) hm.classify &lt;- rep(NA, n) # a container vector with missing values for (i in 1:n) { ## fit the model to the data after removing the ith observation sub.fit &lt;- lm(author ~ upon + there + consequently + whilst, data = author.data[-i, ]) # exclude ith row ## predict the authorship for the ith observation hm.classify[i] &lt;- predict(sub.fit, newdata = author.data[i, ]) } ## proportion of correctly classified essays by Hamilton mean(hm.classify[author.data$author == 1] &gt; 0) ## proportion of correctly classified essays by Madison mean(hm.classify[author.data$author == -1] &lt; 0) Original: disputed &lt;- c(49, 50:57, 62, 63) # 11 essays with disputed authorship tf.disputed &lt;- as.data.frame(tfm[disputed, ]) ## prediction of disputed authorship pred &lt;- predict(hm.fit, newdata = tf.disputed) pred # predicted values tidyverse: When adding prediction with add_predictions it added predictions for missing values as well. Table of authorship of disputed papers author_data %&gt;% filter(is.na(author)) %&gt;% select(document, pred, pred_author) %&gt;% knitr::kable() document pred pred_author 18 -0.359 Madison 19 -0.586 Madison 20 -0.055 Madison 49 -0.966 Madison 50 -0.002 Madison 51 -1.522 Madison 52 -0.195 Madison 53 -0.506 Madison 54 -0.520 Madison 55 0.094 Hamilton 56 -0.550 Madison 57 -1.219 Madison 62 -0.944 Madison 63 -0.184 Madison par(cex = 1.25) ## fitted values for essays authored by Hamilton; red squares plot(hamilton, hm.fitted[author.data$author == 1], pch = 15, xlim = c(1, 85), ylim = c(-2, 2), col = &quot;red&quot;, xlab = &quot;Federalist Papers&quot;, ylab = &quot;Predicted values&quot;) abline(h = 0, lty = &quot;dashed&quot;) ## essays authored by Madison; blue circles points(madison, hm.fitted[author.data$author == -1], pch = 16, col = &quot;blue&quot;) ## disputed authorship; black triangles points(disputed, pred, pch = 17) disputed_essays &lt;- filter(author_data, is.na(author))$document ggplot(mutate(author_data, author = fct_explicit_na(factor(author), &quot;Disputed&quot;)), aes(y = document, x = pred, colour = author, shape = author)) + geom_ref_line(v = 0) + geom_point() + scale_y_continuous(breaks = seq(10, 80, by = 10), minor_breaks = seq(5, 80, by = 5)) + scale_color_manual(values = c(&quot;Madison&quot; = &quot;blue&quot;, &quot;Hamilton&quot; = &quot;red&quot;, &quot;Disputed&quot; = &quot;black&quot;)) + scale_shape_manual(values = c(&quot;Madison&quot; = 16, &quot;Hamilton&quot; = 15, &quot;Disputed&quot; = 17)) + labs(colour = &quot;Author&quot;, shape = &quot;Author&quot;, y = &quot;Federalist Papers&quot;, x = &quot;Predicted values&quot;) 6.3 Network Data Network data is area for which the tidyverse is not well suited. The igraph, sna, and network packages are the best in class. See the Social Network Analysis section of the Social Sciences Task View. See this tutorial by Katherin Ognyanova, Static and dynamic nework visualization with R, for a good overview of network visualization with those packages in R. There are several packages that plot networks in ggplot2. ggnetwork ggraph geomnet GGally functions ggnet, ggnet2, and ggnetworkmap. ggCompNet compares the speed of various network plotting packages in R. See this presentation for an overview of some of those packages for data visualization. Examples: Network Visualization Examples with the ggplot2 Package library(&quot;igraph&quot;) library(&quot;intergraph&quot;) library(&quot;GGally&quot;) 6.3.1 Twitter Following Network Original: twitter &lt;- read.csv(&quot;twitter-following.csv&quot;) senator &lt;- read.csv(&quot;twitter-senator.csv&quot;) tidyverse: twitter &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;twitter-following.csv&quot;)) senator &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;twitter-senator.csv&quot;)) Original: n &lt;- nrow(senator) # number of senators ## initialize adjacency matrix twitter.adj &lt;- matrix(0, nrow = n, ncol = n) ## assign screen names to rows and columns colnames(twitter.adj) &lt;- rownames(twitter.adj) &lt;- senator$screen_name ## change `0&#39; to `1&#39; when edge goes from node `i&#39; to node `j&#39; for (i in 1:nrow(twitter)) { twitter.adj[twitter$following[i], twitter$followed[i]] &lt;- 1 } twitter.adj &lt;- graph.adjacency(twitter.adj, mode = &quot;directed&quot;, diag = FALSE) tidyverse: Simply use the function since twitter consists of edges (a link from a senator to another). SInce graph_from_edgelist expects a matrix, convert the data frame to a matrix using . twitter_adj &lt;- graph_from_edgelist(as.matrix(twitter)) original: senator$indegree &lt;- degree(twitter.adj, mode = &quot;in&quot;) senator$outdegree &lt;- degree(twitter.adj, mode = &quot;out&quot;) in.order &lt;- order(senator$indegree, decreasing = TRUE) out.order &lt;- order(senator$outdegree, decreasing = TRUE) ## 3 greatest indegree senator[in.order[1:3], ] ## 3 greatest outdegree senator[out.order[1:3], ] environment(degree) #&gt; &lt;environment: namespace:sna&gt; tidyverse: Add in- and out-degree varibles to the senator data frame: senator &lt;- mutate(senator, indegree = igraph::degree(twitter_adj, mode = &quot;in&quot;), outdegree = igraph::degree(twitter_adj, mode = &quot;out&quot;)) Now find the senators with the 3 greatest in-degrees arrange(senator, desc(indegree)) %&gt;% slice(1:3) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 3 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tom Cotton R AR 64 15 #&gt; 2 Richard J. Durbin D IL 60 87 #&gt; 3 John Barrasso R WY 58 79 or using the function: top_n(senator, 3, indegree) %&gt;% arrange(desc(indegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 5 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tom Cotton R AR 64 15 #&gt; 2 Richard J. Durbin D IL 60 87 #&gt; 3 John Barrasso R WY 58 79 #&gt; 4 Joe Donnelly D IN 58 9 #&gt; 5 Orrin G. Hatch R UT 58 50 The top_n function catches that three senators are tied for 3rd highest outdegree, whereas the simply sorting and slicing cannot. And we can find the senators with the three highest out-degrees similarly, top_n(senator, 3, outdegree) %&gt;% arrange(desc(outdegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 4 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Thad Cochran R MS 55 89 #&gt; 2 Steve Daines R MT 30 88 #&gt; 3 John McCain R AZ 41 88 #&gt; 4 Joe Manchin, III D WV 43 88 original: n &lt;- nrow(senator) ## color: Democrats = `blue&#39;, Republicans = `red&#39;, Independent = `black&#39; col &lt;- rep(&quot;red&quot;, n) col[senator$party == &quot;D&quot;] &lt;- &quot;blue&quot; col[senator$party == &quot;I&quot;] &lt;- &quot;black&quot; ## pch: Democrats = circle, Republicans = diamond, Independent = cross pch &lt;- rep(16, n) pch[senator$party == &quot;D&quot;] &lt;- 17 pch[senator$party == &quot;I&quot;] &lt;- 4 par(cex = 1.25) ## plot for comparing two closeness measures (incoming vs. outgoing) plot(closeness(twitter.adj, mode = &quot;in&quot;), closeness(twitter.adj, mode = &quot;out&quot;), pch = pch, col = col, main = &quot;Closeness&quot;, xlab = &quot;Incoming path&quot;, ylab = &quot;Outgoing path&quot;) ## plot for comparing directed and undirected betweenness plot(betweenness(twitter.adj, directed = TRUE), betweenness(twitter.adj, directed = FALSE), pch = pch, col = col, main = &quot;Betweenness&quot;, xlab = &quot;Directed&quot;, ylab = &quot;Undirected&quot;) tidyverse # Define scales to reuse for the plots scale_colour_parties &lt;- scale_colour_manual(&quot;Party&quot;, values = c(R = &quot;red&quot;, D = &quot;blue&quot;, I = &quot;green&quot;)) scale_shape_parties &lt;- scale_shape_manual(&quot;Party&quot;, values = c(R = 16, D = 17, I = 4)) senator %&gt;% mutate(closeness_in = igraph::closeness(twitter_adj, mode = &quot;in&quot;), closeness_out = igraph::closeness(twitter_adj, mode = &quot;out&quot;)) %&gt;% ggplot(aes(x = closeness_in, y = closeness_out, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + labs(main = &quot;Closeness&quot;, x = &quot;Incoming path&quot;, y = &quot;Outgoing path&quot;) What does the reference line indicate? What does that say about senators twitter networks? senator %&gt;% mutate(betweenness_dir = igraph::betweenness(twitter_adj, directed = TRUE), betweenness_undir = igraph::betweenness(twitter_adj, directed = FALSE)) %&gt;% ggplot(aes(x = betweenness_dir, y = betweenness_undir, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + labs(main = &quot;Betweenness&quot;, x = &quot;Directed&quot;, y = &quot;Undirected&quot;) We’ve covered three different methods of calculating the importance of a node in a network: degree, closeness, and centrality. But what do they mean? What’s the “best” measure of importance? The answer to the the former is “it depends on the question”. There are probably other papers out there on this, but Borgatti (2005) is a good discussion: Borgatti, Stephen. 2005. “Centrality and Network Flow”. Social Networks. DOI Original: senator$pagerank &lt;- page.rank(twitter.adj)$vector par(cex = 1.25) ## `col&#39; parameter is defined earlier plot(twitter.adj, vertex.size = senator$pagerank * 1000, vertex.color = col, vertex.label = NA, edge.arrow.size = 0.1, edge.width = 0.5) Add and plot page-rank: senator &lt;- mutate(senator, page_rank = page_rank(twitter_adj)[[&quot;vector&quot;]]) ggnet(twitter_adj, mode = &quot;target&quot;) 6.4 Spatial Data in R Some resources on plotting spatial data in R: ggplot2 has several map-related functions borders fortify.map map_data ggmap allows ggplot to us a map from Google Maps, OpenStreet Maps or similar as a background for the plot. David Kahle and Hadley Wickham. 2013. ggmap: Spatial Visualization with ggplot2. Journal of Statistical Software Github dkahle/ggmamp tmap is not built on ggplot2 but uses a ggplot2-like API for network data. leaflet is an R interface to a popular javascript mapping library. Here are few tutorials on plotting spatial data in ggplot2: Making Maps with R Plotting Data on a World Map Introduciton to Spatial Data and ggplot2 library(&quot;ggrepel&quot;) data(&quot;us.cities&quot;, package = &quot;maps&quot;) glimpse(us.cities) #&gt; Observations: 1,005 #&gt; Variables: 6 #&gt; $ name &lt;chr&gt; &quot;Abilene TX&quot;, &quot;Akron OH&quot;, &quot;Alameda CA&quot;, &quot;Albany GA... #&gt; $ country.etc &lt;chr&gt; &quot;TX&quot;, &quot;OH&quot;, &quot;CA&quot;, &quot;GA&quot;, &quot;NY&quot;, &quot;OR&quot;, &quot;NM&quot;, &quot;LA&quot;, &quot;V... #&gt; $ pop &lt;int&gt; 113888, 206634, 70069, 75510, 93576, 45535, 494962... #&gt; $ lat &lt;dbl&gt; 32.5, 41.1, 37.8, 31.6, 42.7, 44.6, 35.1, 31.3, 38... #&gt; $ long &lt;dbl&gt; -99.7, -81.5, -122.3, -84.2, -73.8, -123.1, -106.6... #&gt; $ capital &lt;int&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... usa_map &lt;- map_data(&quot;usa&quot;) capitals &lt;- filter(us.cities, capital == 2, !country.etc %in% c(&quot;HI&quot;, &quot;AK&quot;)) ggplot() + geom_map(map = usa_map) + borders(database = &quot;usa&quot;) + geom_point(aes(x = long, y = lat, size = pop), data = capitals) + # scale size area ensures: 0 = no area scale_size_area() + coord_quickmap() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;US State Capitals&quot;, size = &quot;Population&quot;) cal_cities &lt;- filter(us.cities, country.etc == &quot;CA&quot;) %&gt;% top_n(7, pop) ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;) + geom_point(aes(x = long, y = lat), data = cal_cities) + geom_text_repel(aes(x = long, y = lat, label = name), data = cal_cities) + coord_quickmap() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) 6.4.1 Colors in R For more resources on using colors in R R4DS chapter Graphics for Communication ggplot2 book Chapter “Scales” Jenny Bryan Using colors in R Achim Zeileis, Kurt Hornik, Paul Murrell (2009). Escaping RGBland: Selecting Colors for Statistical Graphics. Computational Statistics &amp; Data Analysis DOI colorspace vignette Maureen Stone Choosing Colors for Data Visualization ColorBrewer A website with a variety of palettes, primarily designed for maps, but also useful in data viz. Stephen Few Practical Rules for Using Color in Charts Why Should Engineers and Scientists by Worried About Color? A Better Default Colormap for Matplotlib A SciPy 2015 talk that describes how the viridis was created. Evaluation of Artery Visualizations for Heart Disease Diagnosis Using the wrong color scale can be deadly … literally. The python package matplotlib has a good discussion of colormaps. Peter Kovesi Good Color Maps: How to Design Them. See the viridis, ggthemes, dichromat, and pals packages for color palettes. Use scale_identity for the color and alpha scales since the values of the variables are the values of the scale itself (the color names, and the alpha values). ggplot(tibble(x = rep(1:4, each = 2), y = x + rep(c(0, 0.2), times = 2), colour = rep(c(&quot;black&quot;, &quot;red&quot;), each = 4), alpha = c(1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5)), aes(x = x, y = y, colour = colour, alpha = alpha)) + geom_point(size = 15) + scale_color_identity() + scale_alpha_identity() + theme_bw() + theme(panel.grid = element_blank()) 6.4.2 United States Presidential Elections Original: pres08 &lt;- read.csv(&quot;pres08.csv&quot;) ## two-party vote share pres08$Dem &lt;- pres08$Obama / (pres08$Obama + pres08$McCain) pres08$Rep &lt;- pres08$McCain / (pres08$Obama + pres08$McCain) ## color for California cal.color &lt;- rgb(red = pres08$Rep[pres08$state == &quot;CA&quot;], blue = pres08$Dem[pres08$state == &quot;CA&quot;], green = 0) tidyverse: pres08 &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;pres08.csv&quot;)) %&gt;% mutate(Dem = Obama / (Obama + McCain), Rep = McCain / (Obama + McCain)) Original: ## California as a blue state map(database = &quot;state&quot;, regions = &quot;California&quot;, col = &quot;blue&quot;, fill = TRUE) ## California as a purple state map(database = &quot;state&quot;, regions = &quot;California&quot;, col = cal.color, fill = TRUE) tidyverse: ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = &quot;blue&quot;) + coord_quickmap() + theme_void() cal_color &lt;- filter(pres08, state == &quot;CA&quot;) %&gt;% {rgb(red = .$Rep, green = 0, blue = .$Dem)} ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = cal_color) + coord_quickmap() + theme_void() # America as red and blue states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ((pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = ifelse(pres08$Rep[i] &gt; pres08$Dem[i], &quot;red&quot;, &quot;blue&quot;), fill = TRUE, add = TRUE) } } ## America as purple states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ((pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = rgb(red = pres08$Rep[i], blue = pres08$Dem[i], green = 0), fill = TRUE, add = TRUE) } } states &lt;- map_data(&quot;state&quot;) %&gt;% left_join(mutate(pres08, state.name = str_to_lower(state.name)), by = c(&quot;region&quot; = &quot;state.name&quot;)) %&gt;% # drops DC filter(!is.na(EV)) %&gt;% mutate(party = if_else(Dem &gt; Rep, &quot;Dem&quot;, &quot;Rep&quot;), color = map2_chr(Dem, Rep, ~ rgb(blue = .x, red = .y, green = 0))) ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = party)) + coord_quickmap() + scale_fill_manual(values = c(&quot;Rep&quot; = &quot;red&quot;, &quot;Dem&quot; = &quot;blue&quot;)) + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) For plotting the purple states, I use since the color column contains the RGB values to use in the plot: ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = color)) + coord_quickmap() + scale_fill_identity() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) However, plotting purple states is not a good data visualization. Even though the colors are a proportional mixture of red and blue, human visual perception doesn’t work that way. The proportion of the democratic vote is best thought of a diverging scale with 0.5 is midpoint. And since the Democratic Party is associated with the color blue and the Republican Party is associated with the color red. The Color Brewer palette RdBu is an example: ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = Dem)) + scale_fill_distiller(&quot;% Obama&quot;, direction = 1, limits = c(0, 1), type = &quot;div&quot;, palette = &quot;RdBu&quot;) + coord_quickmap() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) 6.4.3 Expansion of Walmart Original: walmart &lt;- read.csv(&quot;walmart.csv&quot;) ## red = WalMartStore, blue = SuperCenter, green = DistributionCenter walmart$storecolors &lt;- NA # create an empty vector walmart$storecolors[walmart$type == &quot;Wal-MartStore&quot;] &lt;- rgb(red = 1, green = 0, blue = 0, alpha = 1/3) walmart$storecolors[walmart$type == &quot;SuperCenter&quot;] &lt;- rgb(red = 0, green = 0, blue = 1, alpha = 1/3) walmart$storecolors[walmart$type == &quot;DistributionCenter&quot;] &lt;- rgb(red = 0, green = 1, blue = 0, alpha = 1/3) ## larger circles for DistributionCenter walmart$storesize &lt;- ifelse(walmart$type == &quot;DistributionCenter&quot;, 1, 0.5) tidyverse: We don’t need to do the direct mapping since walmart &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;walmart.csv&quot;)) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = mutate(walmart, size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)), alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + guides(color = guide_legend(override.aes = list(alpha = 1))) + theme_void() We don’t need to worry about colors since ggplot handles that. I use guides to so that the colors or not transparent in the legend (see R for Data Science chapterGraphics for communication). To make a plot showing all Walmart stores opened up through that year, I write a function, that takes the year and dataset as parameters. Since I am calling the function for its side effect (printing the plot) rather than the value it returns, I use the walk function rather than map. See R for Data Science, Chapter 21.8: Walk for more information. map_walmart &lt;- function(year, .data) { .data &lt;- filter(.data, opendate &lt; make_date(year, 1, 1)) %&gt;% mutate(size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = .data, alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + guides(color = guide_legend(override.aes = list(alpha = 1))) + theme_void() + ggtitle(year) } years &lt;- c(1975, 1985, 1995, 2005) walk(years, ~ print(map_walmart(.x, walmart))) 6.4.4 Animation in R For easy animation with ggplot2, use the gganimate package. Note that the gganimate package is not on CRAN, so you have to install it with the devtools package: install.packages(&quot;cowplot&quot;) devtools::install_github(&quot;dgrtwo/animate&quot;) library(&quot;gganimate&quot;) An animation is a series of frames. The gganimate package works by adding a frame aesthetic to ggplots, and function will animate the plot. I use frame = year(opendate) to have the animation use each year as a frame, and cumulative = TRUE so that the previous years are shown. walmart_animated &lt;- ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, fill = type, frame = year(opendate), cumulative = TRUE), data = walmart) + coord_quickmap() + theme_void() gganimate(walmart_animated) unnamed-chunk-84 "]
]
