[
["index.html", "QSS Tidyverse Code 1 Preface", " QSS Tidyverse Code Jeffrey B. Arnold 2017-01-14 1 Preface This is tidyverse R code to supplement the book, Quantitative Social Science: An Introduction, by Kosuke Imai, to be published by Princeton University Press in March 2017. The R code included with the text of QSS and the supplementary materials relies mostly on base R functions. This translates the code examples provided with QSS to tidyverse R code. Tidyverse refers to a set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, and a few others) that share common data representations, especially the use of data frames for return values. The book R for Data Science by Hadley Wickham and Garrett Grolemond is an introduction. I wrote this code while teaching course that employed both texts in order to make the excellent examples and statistical material in QSS more compatible with the modern data science R approach in R4DS. "],
["introduction.html", "2 Introduction 2.1 Prerequisites 2.2 Introduction to R", " 2 Introduction 2.1 Prerequisites library(&quot;tidyverse&quot;) We also load the haven package to load Stata dta files, library(&quot;haven&quot;) and the rio package to load multiple types of files, library(&quot;rio&quot;) 2.2 Introduction to R 2.2.1 Data Files Don’t use setwd() within scripts. It is much better to organize your code in projects. When reading from csv files use readr::read_csv instead of the base R function read.csv. It is slightly faster, and returns a tibble instead of a data frame. See r4ds Ch 11: Data Import for more dicussion. We also can load it directly from a URL. UNpop &lt;- read_csv(&quot;https://raw.githubusercontent.com/jrnold/qss/master/INTRO/UNpop.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; year = col_integer(), #&gt; world.pop = col_integer() #&gt; ) class(UNpop) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; UNpop #&gt; # A tibble: 7 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; # ... with 1 more rows The single bracket, [, is useful to select rows and columns in simple cases, but the dplyr functions slice() to select rows by number, filter to select by certain criteria, or select() to select columns. UNpop[c(1, 2, 3), ] #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 is equivalent to UNpop %&gt;% slice(1:3) #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 UNpop[, &quot;world.pop&quot;] #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows is almost equivalent to select(UNpop, world.pop) #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows However, note that select() always returns a tibble, and never a vector, even if only one column is selected. Also, note that since world.pop is a tibble, using [ also returns tibbles rather than a vector if it is only one column. UNpop[1:3, &quot;year&quot;] #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 is almost equivalent to UNpop %&gt;% slice(1:3) %&gt;% select(year) #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 For this example using these functions and %&gt;% to chain them together may seem a little excessive, but later we will see how chaining simple functions togther like that becomes a very powerful way to build up complicated logic. UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] #&gt; [1] 2525779 3691173 5320817 6916183 can be rewritten as UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% select(world.pop) #&gt; # A tibble: 4 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3691173 #&gt; 3 5320817 #&gt; 4 6916183 The function n() when used in a dplyr functions returns the number of rows in the data frame (or the number of rows in the group if used with group_by). 2.2.2 Saving Objects Do not save the workspace using save.image. This is an extremely bad idea for reproducibility. See r4ds Ch 8. You should uncheck the options in RStudio to avoid saving and rstoring from .RData files. This will help ensure that your R code runs the way you think it does, instead of depending on some long forgotten code that is only saved in the workspace image. Everything important should be in a script. Anything saved or loaded from file should be done explicitly. As with reading CSVs, use the readr package functions. In this case, write_csv writes a csv file write_csv(UNpop, &quot;UNpop.csv&quot;) 2.2.3 Packages Instead of foreign for reading and writing Stata and SPSS files, use haven. One reason to do so is that it is better maintained. The R function read.dta does not read files created by the most recent versions of Stata (13+). read_dta(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; # A tibble: 7 × 2 #&gt; year world_pop #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; # ... with 1 more rows There is also the equivalent write_dta function to create Stata datasets. write_dta(UNpop, &quot;UNpop.dta&quot;) Also see the rio package which makes loading data even easier with smart defaults. You can use the import function to load many types of files: import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.csv&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.RData&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; year world_pop #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; 7 2010 6916 2.2.4 Style Guide Follow Hadley Wickham’s Style Guide not the Google R style guide. In addition to lintr, the R package formatR has methods to clean up your code. "],
["causality.html", "3 Causality 3.1 Prerequistes 3.2 Racial Discrimination in the Labor Market 3.3 Subsetting Data in R 3.4 Causal Affects and the Counterfactual 3.5 Observational Studies 3.6 Descriptive Statistics for a Single Variable", " 3 Causality 3.1 Prerequistes library(&quot;tidyverse&quot;) 3.2 Racial Discrimination in the Labor Market The code in the book uses table and addmargins to construct the table. However, this can be done easily with dplyr using grouping and summarizing. resume_url &lt;- &quot;https://raw.githubusercontent.com/jrnold/qss/master/CAUSALITY/resume.csv&quot; resume &lt;- read_csv(resume_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; firstname = col_character(), #&gt; sex = col_character(), #&gt; race = col_character(), #&gt; call = col_integer() #&gt; ) In addition to the functions shown in the text, dim(resume) #&gt; [1] 4870 4 summary(resume) #&gt; firstname sex race call #&gt; Length:4870 Length:4870 Length:4870 Min. :0.00 #&gt; Class :character Class :character Class :character 1st Qu.:0.00 #&gt; Mode :character Mode :character Mode :character Median :0.00 #&gt; Mean :0.08 #&gt; 3rd Qu.:0.00 #&gt; Max. :1.00 head(resume) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Allison female white 0 #&gt; 2 Kristen female white 0 #&gt; 3 Lakisha female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Carrie female white 0 #&gt; 6 Jay male white 0 we can also use glimpse to get a quick understanding of the variables in the data frame, glimpse(resume) #&gt; Observations: 4,870 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... For each combination of race and call let’s count the observations: race_call_tab &lt;- resume %&gt;% group_by(race, call) %&gt;% count() race_call_tab #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race call n #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 0 2278 #&gt; 2 black 1 157 #&gt; 3 white 0 2200 #&gt; 4 white 1 235 If we want to calculate callback rates by race: race_call_rate &lt;- race_call_tab %&gt;% group_by(race) %&gt;% mutate(call_rate = n / sum(n)) %&gt;% filter(call == 1) %&gt;% select(race, call_rate) race_call_rate #&gt; Source: local data frame [2 x 2] #&gt; Groups: race [2] #&gt; #&gt; race call_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black 0.0645 #&gt; 2 white 0.0965 If we want the overall callback rate, we can calculate it from the original data, resume %&gt;% summarise(call_back = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_back #&gt; &lt;dbl&gt; #&gt; 1 0.0805 3.3 Subsetting Data in R 3.3.1 Subsetting The dplyr function filter is a much improved version of subset. To select black individuals in the data: resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) dim(resumeB) #&gt; [1] 2435 4 head(resumeB) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Lakisha female black 0 #&gt; 2 Latonya female black 0 #&gt; 3 Kenya female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Tyrone male black 0 #&gt; 6 Aisha female black 0 And to calculate the callback rate resumeB %&gt;% summarise(call_rate = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_rate #&gt; &lt;dbl&gt; #&gt; 1 0.0645 To keep call and firstname variables and those with black-sounding first names. resumeBf &lt;- resume %&gt;% filter(race == &quot;black&quot;, sex == &quot;female&quot;) %&gt;% select(call, firstname) head(resumeBf) #&gt; # A tibble: 6 × 2 #&gt; call firstname #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 0 Lakisha #&gt; 2 0 Latonya #&gt; 3 0 Kenya #&gt; 4 0 Latonya #&gt; 5 0 Aisha #&gt; 6 0 Aisha Now we can calculate the gender gap by group. One way to do this is to calculate the call back rates for both sexes of black sounding names, resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) %&gt;% group_by(sex) %&gt;% summarise(black_rate = mean(call)) and white-sounding names resumeW &lt;- resume %&gt;% filter(race == &quot;white&quot;) %&gt;% group_by(sex) %&gt;% summarise(white_rate = mean(call)) resumeW #&gt; # A tibble: 2 × 2 #&gt; sex white_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 0.0989 #&gt; 2 male 0.0887 Then, merge resumeB and resumeW on sex and calculate the difference for both sexes. inner_join(resumeB, resumeW, by = &quot;sex&quot;) %&gt;% mutate(race_gap = white_rate - black_rate) #&gt; # A tibble: 2 × 4 #&gt; sex black_rate white_rate race_gap #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This seems to be a little more code, but we didn’t duplicate as much as in QSS, and this would easily scale to more than two categories. A way to do this using the spread and gather functions from tidy are, First, caclulate the resume_race_sex &lt;- resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) head(resume_race_sex) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [2] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 Now, use spread() to make each value of race a new column: library(&quot;tidyr&quot;) resume_sex &lt;- resume_race_sex %&gt;% ungroup() %&gt;% spread(race, call) resume_sex #&gt; # A tibble: 2 × 3 #&gt; sex black white #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 #&gt; 2 male 0.0583 0.0887 Now we can calculate the race wage differences by sex as before, resume_sex %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This could be combined into a single chain with only six lines of code: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% ungroup() %&gt;% spread(race, call) %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 3.3.2 Simple conditional statements See the dlpyr functions if_else, recode and case_when. The function if_else is like ifelse bue corrects for some weird behavior that ifelse has in certain cases. resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1, 0)) %&gt;% group_by(BlackFemale, race, sex) %&gt;% count() #&gt; Source: local data frame [4 x 4] #&gt; Groups: BlackFemale, race [?] #&gt; #&gt; BlackFemale race sex n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 black male 549 #&gt; 2 0 white female 1860 #&gt; 3 0 white male 575 #&gt; 4 1 black female 1886 3.3.3 Factor Variables See R4DS Chapter 15 “Factors” and the package forcats The code in this section works, but can be simplified by using the function case_when which works in exactly thease cases. resume %&gt;% mutate(type = as.factor(case_when( .$race == &quot;black&quot; &amp; .$sex == &quot;female&quot; ~ &quot;BlackFemale&quot;, .$race == &quot;black&quot; &amp; .$sex == &quot;male&quot; ~ &quot;BlackMale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;female&quot; ~ &quot;WhiteFemale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;male&quot; ~ &quot;WhiteMale&quot;, TRUE ~ as.character(NA) ))) #&gt; # A tibble: 4,870 × 5 #&gt; firstname sex race call type #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fctr&gt; #&gt; 1 Allison female white 0 WhiteFemale #&gt; 2 Kristen female white 0 WhiteFemale #&gt; 3 Lakisha female black 0 BlackFemale #&gt; 4 Latonya female black 0 BlackFemale #&gt; 5 Carrie female white 0 WhiteFemale #&gt; 6 Jay male white 0 WhiteMale #&gt; # ... with 4,864 more rows Since the logic of this is so simple, we can create this variable by using str_c to combine the vectors of sex and race, after using str_to_title to capitalize them first. library(stringr) resume &lt;- resume %&gt;% mutate(type = str_c(str_to_title(race), str_to_title(sex))) Some of the reasons given for using factors in this chapter are not as important given the functionality in modern tidyverse packages. For example, there is no reason to use tapply, as that can use group_by and summarise, resume %&gt;% group_by(type) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 × 2 #&gt; type call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 BlackFemale 0.0663 #&gt; 2 BlackMale 0.0583 #&gt; 3 WhiteFemale 0.0989 #&gt; 4 WhiteMale 0.0887 What’s nice about this approach is that we wouldn’t have needed to create the factor variable first, resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 We can use that same appraoch to calculate the mean of firstnames, and use arrange to sort in ascending order. resume %&gt;% group_by(firstname) %&gt;% summarise(call = mean(call)) %&gt;% arrange(call) #&gt; # A tibble: 36 × 2 #&gt; firstname call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aisha 0.0222 #&gt; 2 Rasheed 0.0299 #&gt; 3 Keisha 0.0383 #&gt; 4 Tremayne 0.0435 #&gt; 5 Kareem 0.0469 #&gt; 6 Darnell 0.0476 #&gt; # ... with 30 more rows 3.4 Causal Affects and the Counterfactual Load the data using the readr function read_csv social_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot; social &lt;- read_csv(social_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2006 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) summary(social) #&gt; sex yearofbirth primary2004 messages #&gt; Length:305866 Min. :1900 Min. :0.000 Length:305866 #&gt; Class :character 1st Qu.:1947 1st Qu.:0.000 Class :character #&gt; Mode :character Median :1956 Median :0.000 Mode :character #&gt; Mean :1956 Mean :0.401 #&gt; 3rd Qu.:1965 3rd Qu.:1.000 #&gt; Max. :1986 Max. :1.000 #&gt; primary2006 hhsize #&gt; Min. :0.000 Min. :1.00 #&gt; 1st Qu.:0.000 1st Qu.:2.00 #&gt; Median :0.000 Median :2.00 #&gt; Mean :0.312 Mean :2.18 #&gt; 3rd Qu.:1.000 3rd Qu.:2.00 #&gt; Max. :1.000 Max. :8.00 Use a grouped summarize instead of tapply, gotv_by_group &lt;- social %&gt;% group_by(messages) %&gt;% summarize(turnout = mean(primary2006)) gotv_by_group #&gt; # A tibble: 4 × 2 #&gt; messages turnout #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Get the turnout for the control group gotv_control &lt;- (filter(gotv_by_group, messages == &quot;Control&quot;))[[&quot;turnout&quot;]] Subtract the control group turnout from all groups gotv_by_group %&gt;% mutate(diff_control = turnout - gotv_control) #&gt; # A tibble: 4 × 3 #&gt; messages turnout diff_control #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.0179 #&gt; 2 Control 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.0257 #&gt; 4 Neighbors 0.378 0.0813 We could have also done this in one step like, gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 We can compare the differences of variables across the groups easily using a grouped summarize gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Pro-tip The summarise_at functions allows you summarize one-or-more columns with one-or-more functions. In addition to age, 2004 turnout, and household size, we’ll also compare propotion female, social %&gt;% group_by(messages) %&gt;% mutate(age = 2006 - yearofbirth, female = (sex == &quot;female&quot;)) %&gt;% select(-age, -sex) %&gt;% summarise_all(mean) #&gt; # A tibble: 4 × 6 #&gt; messages yearofbirth primary2004 primary2006 hhsize female #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 1956 0.399 0.315 2.19 0.500 #&gt; 2 Control 1956 0.400 0.297 2.18 0.499 #&gt; 3 Hawthorne 1956 0.403 0.322 2.18 0.499 #&gt; 4 Neighbors 1956 0.407 0.378 2.19 0.500 3.5 Observational Studies Load the minwage dataset from its URL using readr::read_csv: minwage_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/minwage.csv&quot; minwage &lt;- read_csv(minwage_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; chain = col_character(), #&gt; location = col_character(), #&gt; wageBefore = col_double(), #&gt; wageAfter = col_double(), #&gt; fullBefore = col_double(), #&gt; fullAfter = col_double(), #&gt; partBefore = col_double(), #&gt; partAfter = col_double() #&gt; ) glimpse(minwage) #&gt; Observations: 358 #&gt; Variables: 8 #&gt; $ chain &lt;chr&gt; &quot;wendys&quot;, &quot;wendys&quot;, &quot;burgerking&quot;, &quot;burgerking&quot;, &quot;kf... #&gt; $ location &lt;chr&gt; &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA... #&gt; $ wageBefore &lt;dbl&gt; 5.00, 5.50, 5.00, 5.00, 5.25, 5.00, 5.00, 5.00, 5.0... #&gt; $ wageAfter &lt;dbl&gt; 5.25, 4.75, 4.75, 5.00, 5.00, 5.00, 4.75, 5.00, 4.5... #&gt; $ fullBefore &lt;dbl&gt; 20.0, 6.0, 50.0, 10.0, 2.0, 2.0, 2.5, 40.0, 8.0, 10... #&gt; $ fullAfter &lt;dbl&gt; 0.0, 28.0, 15.0, 26.0, 3.0, 2.0, 1.0, 9.0, 7.0, 18.... #&gt; $ partBefore &lt;dbl&gt; 20.0, 26.0, 35.0, 17.0, 8.0, 10.0, 20.0, 30.0, 27.0... #&gt; $ partAfter &lt;dbl&gt; 36, 3, 18, 9, 12, 9, 25, 32, 39, 10, 20, 4, 13, 20,... summary(minwage) #&gt; chain location wageBefore wageAfter #&gt; Length:358 Length:358 Min. :4.25 Min. :4.25 #&gt; Class :character Class :character 1st Qu.:4.25 1st Qu.:5.05 #&gt; Mode :character Mode :character Median :4.50 Median :5.05 #&gt; Mean :4.62 Mean :4.99 #&gt; 3rd Qu.:4.99 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :6.25 #&gt; fullBefore fullAfter partBefore partAfter #&gt; Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 2.1 1st Qu.: 2.0 1st Qu.:11.0 1st Qu.:11.0 #&gt; Median : 6.0 Median : 6.0 Median :16.2 Median :17.0 #&gt; Mean : 8.5 Mean : 8.4 Mean :18.8 Mean :18.7 #&gt; 3rd Qu.:12.0 3rd Qu.:12.0 3rd Qu.:25.0 3rd Qu.:25.0 #&gt; Max. :60.0 Max. :40.0 Max. :60.0 Max. :60.0 First, calcualte the proportion of restraunts by state whose hourly wages were less than the minimum wage in NJ, $5.05, for wageBefore and wageAfter: Since the NJ minimum wage was $5.05, we’ll define a variable with that value. Even if you use them only once or twice, it is a good idea to put values like this in variables. It makes your code closer to self-documenting.n NJ_MINWAGE &lt;- 5.05 Later, it will be easier to understand wageAfter &lt; NJ_MINWAGE without any comments than it would be to understand wageAfter &lt; 5.05. In the latter case you’d have to remember that the new NJ minimum wage was 5.05 and that’s why you were using that value. This is an example of a magic number: try to avoid them. Note that location has multiple values: PA and four regions of NJ. So we’ll add a state variable to the data. minwage %&gt;% count(location) #&gt; # A tibble: 5 × 2 #&gt; location n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 centralNJ 45 #&gt; 2 northNJ 146 #&gt; 3 PA 67 #&gt; 4 shoreNJ 33 #&gt; 5 southNJ 67 We can extract the state from the final two characters of the location variable using the stringr function str_sub (R4DS Ch 14: Strings): library(stringr) minwage &lt;- mutate(minwage, state = str_sub(location, -2L)) Alternatively, since everything is either PA or NJ minwage &lt;- mutate(minwage, state = if_else(location == &quot;PA&quot;, &quot;PA&quot;, &quot;NJ&quot;)) Let’s confirm that the restraunts followed the law: minwage %&gt;% group_by(state) %&gt;% summarise(prop_after = mean(wageAfter &lt; NJ_MINWAGE), prop_Before = mean(wageBefore &lt; NJ_MINWAGE)) #&gt; # A tibble: 2 × 3 #&gt; state prop_after prop_Before #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.00344 0.911 #&gt; 2 PA 0.95522 0.940 Create a variable for the proportion of full-time employees in NJ and PA minwage &lt;- minwage %&gt;% mutate(totalAfter = fullAfter + partAfter, fullPropAfter = fullAfter / totalAfter) Now calculate the average for each state: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state #&gt; # A tibble: 2 × 2 #&gt; state fullPropAfter #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ 0.320 #&gt; 2 PA 0.272 We could compute the difference by (filter(full_prop_by_state, state == &quot;NJ&quot;)[[&quot;fullPropAfter&quot;]] - filter(full_prop_by_state, state == &quot;PA&quot;)[[&quot;fullPropAfter&quot;]]) #&gt; [1] 0.0481 or using tidyr functions spread (R4DS Ch 11: Tidy Data): spread(full_prop_by_state, state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 1 × 3 #&gt; NJ PA diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.32 0.272 0.0481 3.5.1 Confounding Bias We can calculate the proportion of fast-food restraunts in each chain in each state: chains_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% count(chain) %&gt;% mutate(prop = n / sum(n)) We can easily compare these using a simple dot-plot: ggplot(chains_by_state, aes(x = chain, y = prop, colour = state)) + geom_point() + coord_flip() In the QSS text, only Burger King restraunts are compared. However, dplyr makes this easy. All we have to do is change the group_by statement we used last time, and add chain to it: full_prop_by_state_chain &lt;- minwage %&gt;% group_by(state, chain) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state_chain #&gt; Source: local data frame [8 x 3] #&gt; Groups: state [?] #&gt; #&gt; state chain fullPropAfter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ burgerking 0.358 #&gt; 2 NJ kfc 0.328 #&gt; 3 NJ roys 0.283 #&gt; 4 NJ wendys 0.260 #&gt; 5 PA burgerking 0.321 #&gt; 6 PA kfc 0.236 #&gt; # ... with 2 more rows We can plot and compare the proportions easily in this format. In general, ordering categorical variables alphabetically is useless, so we’ll order the chains by the average of the NJ and PA fullPropAfter, using forcats::fct_reorder: ggplot(full_prop_by_state_chain, aes(x = forcats::fct_reorder(chain, fullPropAfter), y = fullPropAfter, colour = state)) + geom_point() + coord_flip() + labs(x = &quot;chains&quot;) To calculate the differences, we need to get the data frame The join method. Create New Jersey and Pennsylvania datasets with chain and prop full employed columns. Merge the two datasets on chain. chains_nj &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(-state) %&gt;% rename(NJ = fullPropAfter) chains_pa &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;PA&quot;) %&gt;% select(-state) %&gt;% rename(PA = fullPropAfter) full_prop_state_chain_diff &lt;- full_join(chains_nj, chains_pa, by = &quot;chain&quot;) %&gt;% mutate(diff = NJ - PA) full_prop_state_chain_diff #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 Q: In the code above why did I remove the state variable and rename the fullPropAfter variable before merging? What happens if I didn’t? The spread/gather method. We can also use the spread and gather functions from tidyr. In this example it is much more compact code. full_prop_by_state_chain %&gt;% spread(state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 3.5.2 Before and After and Difference-in-Difference Designs To compute the estimates in the before and after design first create a variable for the difference before and after the law passed. minwage &lt;- minwage %&gt;% mutate(totalBefore = fullBefore + partBefore, fullPropBefore = fullBefore / totalBefore) The before-and-after analysis is the difference between the full-time employment before and after the minimum wage law passed looking only at NJ: filter(minwage, state == &quot;NJ&quot;) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) #&gt; # A tibble: 1 × 1 #&gt; diff #&gt; &lt;dbl&gt; #&gt; 1 0.0239 The difference-in-differences design uses the difference in the before-and-after differences for each state. diff_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) filter(diff_by_state, state == &quot;NJ&quot;)[[&quot;diff&quot;]] - filter(diff_by_state, state == &quot;PA&quot;)[[&quot;diff&quot;]] #&gt; [1] 0.0616 Let’s create a single dataset with the mean values of each state before and after to visually look at each of these designs: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(fullPropAfter, fullPropBefore), mean) %&gt;% gather(period, fullProp, -state) %&gt;% mutate(period = recode(period, fullPropAfter = 1, fullPropBefore = 0)) full_prop_by_state #&gt; # A tibble: 4 × 3 #&gt; state period fullProp #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 1 0.320 #&gt; 2 PA 1 0.272 #&gt; 3 NJ 0 0.297 #&gt; 4 PA 0 0.310 ggplot(full_prop_by_state, aes(x = period, y = fullProp, colour = state)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(0, 1), labels = c(&quot;Before&quot;, &quot;After&quot;)) 3.6 Descriptive Statistics for a Single Variable To calculate the summary for the variables wageBefore and wageAfter: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(wageBefore, wageAfter) %&gt;% summary() #&gt; wageBefore wageAfter #&gt; Min. :4.25 Min. :5.00 #&gt; 1st Qu.:4.25 1st Qu.:5.05 #&gt; Median :4.50 Median :5.05 #&gt; Mean :4.61 Mean :5.08 #&gt; 3rd Qu.:4.87 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :5.75 We calculate the IQR for each state’s wages after the passage of the law using the same grouped summarise as we used before: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter = IQR(wageAfter), wageBefore = IQR(wageBefore)) #&gt; # A tibble: 2 × 3 #&gt; state wageAfter wageBefore #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.000 0.62 #&gt; 2 PA 0.575 0.75 Calculate the variance and standard deviation of wageAfter and wageBefore for each state: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter_sd = sd(wageAfter), wageAfter_var = var(wageAfter), wageBefore_sd = sd(wageBefore), wageBefore_var = var(wageBefore)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageAfter_var wageBefore_sd wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.0112 0.343 0.118 #&gt; 2 PA 0.359 0.1291 0.358 0.128 or, more compactly, using summarise_at: minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(wageAfter, wageBefore), funs(sd, var)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageBefore_sd wageAfter_var wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.343 0.0112 0.118 #&gt; 2 PA 0.359 0.358 0.1291 0.128 "],
["measurement.html", "4 Measurement 4.1 Prerequisites 4.2 Measuring Civilian Victimization during Wartime 4.3 Visualizing the Univariate Distribution 4.4 Survey Sampling 4.5 load village data 4.6 Measuring Political Polarization 4.7 Clustering", " 4 Measurement 4.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;broom&quot;) 4.2 Measuring Civilian Victimization during Wartime afghan_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan.csv&quot; afghan &lt;- read_csv(afghan_url) Summarize the variables of interest afghan %&gt;% select(age, educ.years, employed, income) %&gt;% summary() #&gt; age educ.years employed income #&gt; Min. :15.0 Min. : 0 Min. :0.000 Length:2754 #&gt; 1st Qu.:22.0 1st Qu.: 0 1st Qu.:0.000 Class :character #&gt; Median :30.0 Median : 1 Median :1.000 Mode :character #&gt; Mean :32.4 Mean : 4 Mean :0.583 #&gt; 3rd Qu.:40.0 3rd Qu.: 8 3rd Qu.:1.000 #&gt; Max. :80.0 Max. :18 Max. :1.000 With income, read_csv never converts strings to factors by default. To get a summary of the different levels, either convert it to a factor (R4DS Ch 15), or use count() afghan %&gt;% count(income) #&gt; # A tibble: 6 × 2 #&gt; income n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 10,001-20,000 616 #&gt; 2 2,001-10,000 1420 #&gt; 3 20,001-30,000 93 #&gt; 4 less than 2,000 457 #&gt; 5 over 30,000 14 #&gt; 6 &lt;NA&gt; 154 Count the number a proportion of respondents who answer that they were harmed by the ISF (violent.exp.ISAF) and (violent.exp.taliban) respectively, afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 9 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.48293 #&gt; 2 0 1 354 0.12854 #&gt; 3 0 NA 22 0.00799 #&gt; 4 1 0 475 0.17248 #&gt; 5 1 1 526 0.19099 #&gt; 6 1 NA 22 0.00799 #&gt; # ... with 3 more rows We need to use ungroup() in order to ensure that sum(n) sums over the entire dataset as opposed to only within categories of violent.exp.ISAF. Unlike prop.table, the code above does not drop missing values. We can drop those values by adding a filter verb and using !is.na() to test for missing values in those variables: afghan %&gt;% filter(!is.na(violent.exp.ISAF), !is.na(violent.exp.taliban)) %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 4 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.495 #&gt; 2 0 1 354 0.132 #&gt; 3 1 0 475 0.177 #&gt; 4 1 1 526 0.196 4.2.1 Handling Missing Data in R We already observed the issues with NA values in calculating the proportion answering the “experienced violence” questions. You can filter rows with specific variables having missing values using filter as shown above. Howeer, na.omit works with tibbles just like any other data frame. na.omit(afghan) #&gt; # A tibble: 2,554 × 11 #&gt; province district village.id age educ.years employed income #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Logar Baraki Barak 80 26 10 0 2,001-10,000 #&gt; 2 Logar Baraki Barak 80 49 3 1 2,001-10,000 #&gt; 3 Logar Baraki Barak 80 60 0 1 2,001-10,000 #&gt; 4 Logar Baraki Barak 80 34 14 1 2,001-10,000 #&gt; 5 Logar Baraki Barak 80 21 12 1 2,001-10,000 #&gt; 6 Logar Baraki Barak 80 42 6 1 10,001-20,000 #&gt; # ... with 2,548 more rows, and 4 more variables: violent.exp.ISAF &lt;int&gt;, #&gt; # violent.exp.taliban &lt;int&gt;, list.group &lt;chr&gt;, list.response &lt;int&gt; 4.3 Visualizing the Univariate Distribution 4.3.1 Barplot library(forcats) afghan &lt;- afghan %&gt;% mutate(violent.exp.ISAF.fct = fct_explicit_na(fct_recode(factor(violent.exp.ISAF), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the ISAF&quot;) afghan &lt;- afghan %&gt;% mutate(violent.exp.taliban.fct = fct_explicit_na(fct_recode(factor(violent.exp.taliban), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the Taliban&quot;) TODO This plot could improved by plotting the two values simultaneously to be able to better compare them. dodged bar plot dot-plot This will require creating a data frame that has the following columns: perpetrator (ISAF, Taliban), response (No Harm, Harm, No response). See the section on Tidy Data and spread gather. TODO Compare them by region, ? 4.3.2 Boxplot ggplot(afghan, aes(x = 1, y = age)) + geom_boxplot() + labs(y = &quot;Age&quot;, x = &quot;&quot;) + ggtitle(&quot;Distribution of Age&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_boxplot() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;) + ggtitle(&quot;Education by Provice&quot;) Helmand and Uruzgan have much lower levels of education than the other provicnces, and also report higher levels of violence. afghan %&gt;% group_by(province) %&gt;% summarise(educ.years = mean(educ.years, na.rm = TRUE), violent.exp.taliban = mean(violent.exp.taliban, na.rm = TRUE), violent.exp.ISAF = mean(violent.exp.ISAF, na.rm = TRUE)) %&gt;% arrange(educ.years) #&gt; # A tibble: 5 × 4 #&gt; province educ.years violent.exp.taliban violent.exp.ISAF #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 1.04 0.4545 0.496 #&gt; 2 Helmand 1.60 0.5042 0.541 #&gt; 3 Khost 5.79 0.2332 0.242 #&gt; 4 Kunar 5.93 0.3030 0.399 #&gt; 5 Logar 6.70 0.0802 0.144 4.3.3 Printing and saving graphics Use the function ggsave() to save ggplot graphics. Also, RMarkdown files have their own means of creating and saving plots created by code-chunks. 4.4 Survey Sampling 4.4.1 The Role of Randomization 4.5 load village data afghan_village_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan-village.csv&quot; afghan.village &lt;- read_csv(afghan_village_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; altitude = col_double(), #&gt; population = col_integer(), #&gt; village.surveyed = col_integer() #&gt; ) Box-plots of altitude ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = altitude)) + geom_boxplot() + labs(y = &quot;Altitude (meter)&quot;, x = &quot;&quot;) Boxplots log-population values of sampled and non-sampled ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = log(population))) + geom_boxplot() + labs(y = &quot;log(population)&quot;, x = &quot;&quot;) You can also compare these distributions by plotting their densities: ggplot(afghan.village, aes(colour = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), x = log(population))) + geom_density() + geom_rug() + labs(x = &quot;log(population)&quot;, colour = &quot;&quot;) 4.5.1 Non-response and other sources of bias Calculate the rates of non-response by province to violent.exp.ISAF and violent.exp.taliban: afghan %&gt;% group_by(province) %&gt;% summarise(ISAF = mean(is.na(violent.exp.ISAF)), taliban = mean(is.na(violent.exp.taliban))) %&gt;% arrange(-ISAF) #&gt; # A tibble: 5 × 3 #&gt; province ISAF taliban #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 0.02067 0.06202 #&gt; 2 Helmand 0.01637 0.03041 #&gt; 3 Khost 0.00476 0.00635 #&gt; 4 Kunar 0.00000 0.00000 #&gt; 5 Logar 0.00000 0.00000 Calculat the proportion who support the ISAF using the difference in means between the ISAF and control groups: (mean(filter(afghan, list.group == &quot;ISAF&quot;)$list.response) - mean(filter(afghan, list.group == &quot;control&quot;)$list.response)) #&gt; [1] 0.049 To calculate the table responses to the list expriment in the control, ISAF, and taliban groups&gt; afghan %&gt;% group_by(list.response, list.group) %&gt;% count() %T&gt;% glimpse() %&gt;% spread(list.group, n, fill = 0) #&gt; Observations: 12 #&gt; Variables: 3 #&gt; $ list.response &lt;int&gt; 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4 #&gt; $ list.group &lt;chr&gt; &quot;control&quot;, &quot;ISAF&quot;, &quot;control&quot;, &quot;ISAF&quot;, &quot;taliban&quot;,... #&gt; $ n &lt;int&gt; 188, 174, 265, 278, 433, 265, 260, 287, 200, 182... #&gt; Source: local data frame [5 x 4] #&gt; Groups: list.response [5] #&gt; #&gt; list.response control ISAF taliban #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 188 174 0 #&gt; 2 1 265 278 433 #&gt; 3 2 265 260 287 #&gt; 4 3 200 182 198 #&gt; 5 4 0 24 0 4.6 Measuring Political Polarization congress_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/congress.csv&quot; congress &lt;- read_csv(congress_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; congress = col_integer(), #&gt; district = col_integer(), #&gt; state = col_character(), #&gt; party = col_character(), #&gt; name = col_character(), #&gt; dwnom1 = col_double(), #&gt; dwnom2 = col_double() #&gt; ) glimpse(congress) #&gt; Observations: 14,552 #&gt; Variables: 7 #&gt; $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8... #&gt; $ district &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 98, 98, 1, 2, 3, 4, 5, ... #&gt; $ state &lt;chr&gt; &quot;USA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;A... #&gt; $ party &lt;chr&gt; &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Demo... #&gt; $ name &lt;chr&gt; &quot;TRUMAN&quot;, &quot;BOYKIN F.&quot;, &quot;GRANT G.&quot;, &quot;ANDREWS G.&quot;, &quot;... #&gt; $ dwnom1 &lt;dbl&gt; -0.276, -0.026, -0.042, -0.008, -0.082, -0.170, -0.12... #&gt; $ dwnom2 &lt;dbl&gt; 0.016, 0.796, 0.999, 1.005, 1.066, 0.870, 0.990, 0.89... To create the scatterplot in 3.6, we can congress %&gt;% filter(congress %in% c(80, 112), party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) congress %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) #&gt; Warning: Removed 2 rows containing missing values (geom_point). congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = congress, y = dwnom1, colour = fct_reorder2(party, congress, dwnom1))) + geom_line() + labs(y = &quot;DW-NOMINATE score (1st Dimension)&quot;, x = &quot;Congress&quot;, colour = &quot;Party&quot;) 4.6.1 Correlation Let’s plot the Gini coefficient usgini_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/USGini.csv&quot; USGini &lt;- read_csv(usgini_url) #&gt; Warning: Missing column names filled in: &#39;X1&#39; [1] #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_integer(), #&gt; year = col_integer(), #&gt; gini = col_double() #&gt; ) ggplot(USGini, aes(x = year, y = gini)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Gini coefficient&quot;) + ggtitle(&quot;Income Inequality&quot;) To calculate a measure of party polarization take the code used in the plot of Republican and Democratic party median ideal points and adapt it to calculate the difference in the party medians: party_polarization &lt;- congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% spread(party, dwnom1) %&gt;% mutate(polarization = Republican - Democrat) party_polarization #&gt; Source: local data frame [33 x 4] #&gt; Groups: congress [33] #&gt; #&gt; congress Democrat Republican polarization #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 80 -0.146 0.276 0.421 #&gt; 2 81 -0.195 0.264 0.459 #&gt; 3 82 -0.180 0.265 0.445 #&gt; 4 83 -0.181 0.261 0.442 #&gt; 5 84 -0.209 0.261 0.471 #&gt; 6 85 -0.214 0.250 0.464 #&gt; # ... with 27 more rows ggplot(party_polarization, aes(x = congress, y = polarization)) + geom_point() + ggtitle(&quot;Political Polarization&quot;) + labs(x = &quot;Year&quot;, y = &quot;Republican median − Democratic median&quot;) 4.6.2 Quantile-Quantile Plot To create histogram plots similar congress %&gt;% filter(congress == 112, party %in% c(&quot;Republican&quot;, &quot;Democrat&quot;)) %&gt;% ggplot(aes(x = dwnom2, y = ..density..)) + geom_histogram() + facet_grid(. ~ party) + labs(x = &quot;racial liberalism/conservatism dimension&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot2 includes a stat_qq which can be used to create qq-plots but it is more suited to comparing a sample distribution with a theoretical distibution, usually the normal one. However, we can calculate one by hand, which may give more insight into exactly what the qq-plot is doing. party_qtiles &lt;- tibble( probs = seq(0, 1, by = 0.01), Democrat = quantile(filter(congress, congress == 112, party == &quot;Democrat&quot;)$dwnom2, probs = probs), Republican = quantile(filter(congress, congress == 112, party == &quot;Republican&quot;)$dwnom2, probs = probs) ) party_qtiles #&gt; # A tibble: 101 × 3 #&gt; probs Democrat Republican #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00 -0.925 -1.381 #&gt; 2 0.01 -0.672 -0.720 #&gt; 3 0.02 -0.619 -0.566 #&gt; 4 0.03 -0.593 -0.526 #&gt; 5 0.04 -0.567 -0.468 #&gt; 6 0.05 -0.560 -0.436 #&gt; # ... with 95 more rows The plot looks different than the one in the text since the x- and y-scales are in the original values instead of z-scores (see the next section). party_qtiles %&gt;% ggplot(aes(x = Democrat, y = Republican)) + geom_point() + geom_abline() + coord_fixed() 4.7 Clustering 4.7.1 Matrices While matrices are great for numerical computations, such as when you are implementing algorithms, generally keeping data in data frames is more convenient for data wrangling. 4.7.2 Lists See R4DS Chapter 20: Vectors, Chapter 21: Iteration and the purrr package for more powerful methods of computing on lists. 4.7.3 k-means algorithms TODO A good visualization of the k-means algorithm and a simple, naive implementation in R. Calculate the clusters by the 80th and 112th congresses, k80two.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 2, nstart = 5) Add the cluster ids to datasets congress80 &lt;- congress %&gt;% filter(congress == 80) %&gt;% mutate(cluster2 = factor(k80two.out$cluster)) We will also create a data sets with the cluster centroids. These are in the centers element of the cluster object. k80two.out$centers #&gt; dwnom1 dwnom2 #&gt; 1 0.1468 -0.339 #&gt; 2 -0.0484 0.783 To make it easier to use with ggplot, we need to convert this to a data frame. The tidy function from the broom package: k80two.clusters &lt;- tidy(k80two.out) k80two.clusters #&gt; x1 x2 size withinss cluster #&gt; 1 0.1468 -0.339 311 54.9 1 #&gt; 2 -0.0484 0.783 135 10.9 2 Plot the ideal points and clusters ggplot() + geom_point(data = congress80, aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80two.clusters, mapping = aes(x = x1, y = x2)) We can also plot, congress80 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [5 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 62 #&gt; 2 Democrat 2 132 #&gt; 3 Other 1 2 #&gt; 4 Republican 1 247 #&gt; 5 Republican 2 3 And now we can repeat these steps for the 112th congress: k112two.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 2, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112two.out$cluster)) k112two.clusters &lt;- tidy(k112two.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112two.clusters, mapping = aes(x = x1, y = x2)) congress112 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [3 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 200 #&gt; 2 Republican 1 1 #&gt; 3 Republican 2 242 Now repeat the same with four clusters on the 80th congress: k80four.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 4, nstart = 5) congress80 &lt;- filter(congress, congress == 80) %&gt;% mutate(cluster2 = factor(k80four.out$cluster)) k80four.clusters &lt;- tidy(k80four.out) ggplot() + geom_point(data = congress80, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80four.clusters, mapping = aes(x = x1, y = x2), size = 3) and on the 112th congress: k112four.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 4, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112four.out$cluster)) k112four.clusters &lt;- tidy(k112four.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112four.clusters, mapping = aes(x = x1, y = x2), size = 3) "],
["prediction.html", "5 Prediction 5.1 Prerequisites 5.2 Loops in R 5.3 General Conditional Statements in R 5.4 Poll Predictions 5.5 Linear Regression 5.6 Regression and Causation 5.7 Regression Discontinuity Design", " 5 Prediction 5.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) The packages modelr and broom are used to wrangle the results of linear regressions, library(&quot;broom&quot;) library(&quot;modelr&quot;) #&gt; #&gt; Attaching package: &#39;modelr&#39; #&gt; The following object is masked from &#39;package:broom&#39;: #&gt; #&gt; bootstrap 5.2 Loops in R For more on looping and iteration in R see the R for Data Science chapter Iteration. RStudio provides many features to help debugging, which will be useful in for loops and function: see this article for an example. 5.3 General Conditional Statements in R TODO Is this in R4DS, find good cites. If you are using conditional statements to assign values for data frame, see the dplyr functions if_else, recode, and case_when 5.4 Poll Predictions Load the election polls and election results for 2008, polls08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;polls08.csv&quot;)) glimpse(polls08) #&gt; Observations: 1,332 #&gt; Variables: 5 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;,... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;SurveyUSA-2&quot;, &quot;Ca... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34, 37, 36, 36, 3... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47, 55, 51, 49, 5... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 2008-10-06, 2008... pres08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;pres08.csv&quot;)) glimpse(pres08) #&gt; Observations: 51 #&gt; Variables: 5 #&gt; $ state.name &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo... #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE... #&gt; $ Obama &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36,... #&gt; $ McCain &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, ... #&gt; $ EV &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, ... Compute Obama’s margin in polls and final election polls08 &lt;- polls08 %&gt;% mutate(margin = Obama - McCain) pres08 &lt;- pres08 %&gt;% mutate(margin = Obama - McCain) To work with dates, the R package lubridate makes wrangling them much easier. See the R for Data Science chapter Dates and Times. The function ymd will convert character strings like year-month-day and more into dates, as long as the order is (year, month, day). See dmy, ymd, and others for other ways to convert strings to dates. x &lt;- ymd(&quot;2008-11-04&quot;) y &lt;- ymd(&quot;2008/9/1&quot;) x - y #&gt; Time difference of 64 days However, note that in polls08, the date middate is already a date object, class(polls08$middate) #&gt; [1] &quot;Date&quot; The function read_csv by default will check character vectors to see if they have patterns that appear to be dates, and if so, will parse those columns as dates. We’ll create a variable for election day ELECTION_DAY &lt;- ymd(&quot;2008-11-04&quot;) and add a new column to poll08 with the days to the election polls08 &lt;- mutate(polls08, ELECTION_DAY - middate) Although the code in the chapter uses a for loop, there is no reason to do so. We can accomplish the same task by merging the election results data to the polling data by state. polls_w_results &lt;- left_join(polls08, select(pres08, state, elec_margin = margin), by = &quot;state&quot;) %&gt;% mutate(error = elec_margin - margin) glimpse(polls_w_results) #&gt; Observations: 1,332 #&gt; Variables: 9 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;Sur... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34,... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47,... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 20... #&gt; $ margin &lt;int&gt; -25, -20, -27, -20, -21, -30, -22, -27,... #&gt; $ ELECTION_DAY - middate &lt;time&gt; 8 days, 20 days, 27 days, 29 days, 43 ... #&gt; $ elec_margin &lt;int&gt; -21, -21, -21, -21, -21, -21, -21, -21,... #&gt; $ error &lt;int&gt; 4, -1, 6, -1, 0, 9, 1, 6, -1, -8, -3, -... To get the last poll in each state, arrange and filter on middate last_polls &lt;- polls_w_results %&gt;% arrange(state, desc(middate)) %&gt;% group_by(state) %&gt;% slice(1) last_polls #&gt; Source: local data frame [51 x 9] #&gt; Groups: state [51] #&gt; #&gt; state Pollster Obama McCain middate margin #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 AK Research 2000-3 39 58 2008-10-29 -19 #&gt; 2 AL SurveyUSA-2 36 61 2008-10-27 -25 #&gt; 3 AR ARG-4 44 51 2008-10-29 -7 #&gt; 4 AZ ARG-3 46 50 2008-10-29 -4 #&gt; 5 CA SurveyUSA-3 60 36 2008-10-30 24 #&gt; 6 CO ARG-3 52 45 2008-10-29 7 #&gt; # ... with 45 more rows, and 3 more variables: `ELECTION_DAY - #&gt; # middate` &lt;time&gt;, elec_margin &lt;int&gt;, error &lt;int&gt; Challenge: Instead of using the last poll, use the average of polls in the last week? Last month? How do the margins on the polls change over the election period? To simplify things for later, let’s define a function rmse which calculates the root mean squared error, as defined in the book. See the R for Data Science chapter Functions for more on writing functions. rmse &lt;- function(actual, pred) { sqrt(mean((actual - pred) ^ 2)) } Now we can use rmse() to calculate the RMSE for all the final polls: rmse(last_polls$margin, last_polls$elec_margin) #&gt; [1] 5.88 Or since we already have a variable error, sqrt(mean(last_polls$error ^ 2)) #&gt; [1] 5.88 The mean prediction error is mean(last_polls$error) #&gt; [1] 1.08 This is slightly different than what is in the book due to the difference in the poll used as the final poll; many states have many polls on the last day. I’ll choose binwidths of 1%, since that is fairly interpretable: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 1, boundary = 0) The text uses bindwidths of 5%: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 5, boundary = 0) Challenge: What other ways could you visualize the results? How would you show all states? What about plotting the absolute or squared errors instead of the errors? Challenge: What happens to prediction error if you average polls? Consider averaging back over time? What happens if you take the averages of the state poll average and average of all polls - does that improve prediction? To create a scatterplot using the state abbreviations instead of points use geom_text instead of geom_point. ggplot(last_polls, aes(x = margin, y = elec_margin, label = state)) + geom_text() + geom_abline(col = &quot;red&quot;) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + coord_fixed() + labs(x = &quot;Poll Results&quot;, y = &quot;Actual Election Results&quot;) We can create a confusion matrix as follow. Create a new columns classification which shows whether how the poll’s classification was related to the actual election outcome (“true positive”, “false positive”, “false negative”, “false positive”). This can be accomplished easily with the dplyr funtion case_when. Note: You need to use . to refer to the data frame when using case_when with a mutate function. last_polls &lt;- last_polls %&gt;% ungroup() %&gt;% mutate(classification = case_when( (.$margin &gt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;true positive&quot;, (.$margin &gt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;false positive&quot;, (.$margin &lt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;true negative&quot;, (.$margin &lt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;false negative&quot; )) No simply count the last_polls %&gt;% group_by(classification) %&gt;% count() #&gt; # A tibble: 4 × 2 #&gt; classification n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 false negative 2 #&gt; 2 false positive 1 #&gt; 3 true negative 21 #&gt; 4 true positive 27 Which states were incorrectly predicted? last_polls %&gt;% filter(classification %in% c(&quot;false positive&quot;, &quot;false negative&quot;)) %&gt;% select(state, margin, elec_margin, classification) %&gt;% arrange(desc(elec_margin)) #&gt; # A tibble: 3 × 4 #&gt; state margin elec_margin classification #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 IN -5 1 false negative #&gt; 2 NC -1 1 false negative #&gt; 3 MO 1 -1 false positive What was the difference in the poll prediction of electoral votes and actual electoral votes. We hadn’t included the variable EV when we first merged, but that’s no problem, we’ll just merge again in order to grab that variable: last_polls %&gt;% left_join(select(pres08, state, EV), by = &quot;state&quot;) %&gt;% summarise(EV_pred = sum((margin &gt; 0) * EV), EV_actual = sum((elec_margin &gt; 0) * EV)) #&gt; # A tibble: 1 × 2 #&gt; EV_pred EV_actual #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 349 364 To look at predictions of the # load the data pollsUS08 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pollsUS08.csv&quot;)) %&gt;% mutate(DaysToElection = ELECTION_DAY - middate) We’ll produce the seven-day averages slightly differently than the method used in the text. For all dates in the data, we’ll calculate the moving average. all_dates &lt;- seq(min(polls08$middate), ELECTION_DAY, by = &quot;days&quot;) # Number of poll days to use POLL_DAYS &lt;- 7 pop_vote_avg &lt;- vector(length(all_dates), mode = &quot;list&quot;) for (i in seq_along(all_dates)) { date &lt;- all_dates[i] # summarise the seven day week_data &lt;- pollsUS08 %&gt;% filter(as.integer(middate - date) &lt;= 0, as.integer(middate - date) &gt; -POLL_DAYS) %&gt;% summarise(Obama = mean(Obama, na.rm = TRUE), McCain = mean(McCain, na.rm = TRUE)) # add date for the observation week_data$date &lt;- date pop_vote_avg[[i]] &lt;- week_data } pop_vote_avg &lt;- bind_rows(pop_vote_avg) It is easier to plot this if the data are tidy, with Obama and McCain as categories of a column candidate. pop_vote_avg_tidy &lt;- pop_vote_avg %&gt;% gather(candidate, share, -date, na.rm = TRUE) pop_vote_avg_tidy #&gt; # A tibble: 598 × 3 #&gt; date candidate share #&gt; * &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2008-01-09 Obama 49 #&gt; 2 2008-01-10 Obama 46 #&gt; 3 2008-01-11 Obama 45 #&gt; 4 2008-01-12 Obama 45 #&gt; 5 2008-01-13 Obama 45 #&gt; 6 2008-01-14 Obama 45 #&gt; # ... with 592 more rows ggplot(pop_vote_avg_tidy, aes(x = date, y = share, colour = forcats::fct_reorder2(candidate, date, share))) + geom_point() + geom_line() Challenge read R for Data Science chapter Iteration and use the function map_df instead of a for loop. The 7-day average is similar to the simple method used by Real Clear Politics. The RCP average is simply the average of all polls in their data for the last seven days. Sites like 538 and the Huffington Post on the other hand, also use what amounts to averaging polls, but using more sophisticated statistical methods to assign different weights to different polls. Challenge Why do we need to use different polls for the popular vote data? Why not simply average all the state polls? What would you have to do? Would the overall popular vote be useful in predicting state-level polling, or vice-versa? How would you use them? 5.5 Linear Regression 5.5.1 Facial Appearance and Election Outcomes Load the face dataset: face &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;face.csv&quot;)) Add Democrat and Republican vote shares, and the difference in shares: face &lt;- face %&gt;% mutate(d.share = d.votes / (d.votes + r.votes), r.share = r.votes / (d.votes + r.votes), diff.share = d.share - r.share) Plot facial competence vs. vote share: ggplot(face, aes(x = d.comp, y = diff.share, colour = w.party)) + geom_point() + labs(x = &quot;Competence scores for Democrats&quot;, y = &quot;Democratic margin in vote share&quot;) 5.5.2 Correlation 5.5.3 Least Squares Run the linear regression fit &lt;- lm(diff.share ~ d.comp, data = face) summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = diff.share ~ d.comp, data = face) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.675 -0.166 0.014 0.177 0.743 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.312 0.066 -4.73 6.2e-06 *** #&gt; d.comp 0.660 0.127 5.19 8.9e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.266 on 117 degrees of freedom #&gt; Multiple R-squared: 0.187, Adjusted R-squared: 0.18 #&gt; F-statistic: 27 on 1 and 117 DF, p-value: 8.85e-07 There are many functions to get data out of the lm model. In addition to these, the broom package provides three functions: glance, tidy, and augment that always return data frames. The function glance returns a one-row data-frame summary of the model, glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.187 0.18 0.266 27 8.85e-07 2 -10.5 27 35.3 #&gt; deviance df.residual #&gt; 1 8.31 117 The function tidy returns a data frame in which each row is a coefficient, tidy(fit) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) -0.312 0.066 -4.73 6.24e-06 #&gt; 2 d.comp 0.660 0.127 5.19 8.85e-07 The function augment returns the original data with fitted values, residuals, and other observation level stats from the model appended to it. augment(fit) %&gt;% head() #&gt; diff.share d.comp .fitted .se.fit .resid .hat .sigma .cooksd #&gt; 1 0.2101 0.565 0.0606 0.0266 0.1495 0.00996 0.267 0.001600 #&gt; 2 0.1194 0.342 -0.0864 0.0302 0.2059 0.01286 0.267 0.003938 #&gt; 3 0.0499 0.612 0.0922 0.0295 -0.0423 0.01229 0.268 0.000158 #&gt; 4 0.1965 0.542 0.0454 0.0256 0.1511 0.00922 0.267 0.001510 #&gt; 5 0.4958 0.680 0.1370 0.0351 0.3588 0.01737 0.266 0.016307 #&gt; 6 -0.3495 0.321 -0.1006 0.0319 -0.2490 0.01433 0.267 0.006436 #&gt; .std.resid #&gt; 1 0.564 #&gt; 2 0.778 #&gt; 3 -0.160 #&gt; 4 0.570 #&gt; 5 1.358 #&gt; 6 -0.941 We can plot the results of the bivariate linear regression as follows: ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_abline(slope = coef(fit)[&quot;d.comp&quot;], intercept = coef(fit)[&quot;(Intercept)&quot;]) A more general way to plot the predictions of the model against the data is to use the methods described in Ch 23.3.3 of R4DS. Create an evenly spaced grid of values of d.comp, and add predictions of the model to it. grid &lt;- face %&gt;% data_grid(d.comp) %&gt;% add_predictions(fit) head(grid) #&gt; # A tibble: 6 × 2 #&gt; d.comp pred #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0640 -0.270 #&gt; 2 0.0847 -0.256 #&gt; 3 0.0893 -0.253 #&gt; 4 0.1145 -0.237 #&gt; 5 0.1148 -0.236 #&gt; 6 0.1639 -0.204 Now we can plot the regression line and the original data just like any other plot. ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_line(data = grid, mapping = aes(x = d.comp, y = pred)) This method is more complicated than the geom_abline method for a bivariate regerssion, but will work for more complicated models, while the geom_abline method won’t. Note that geom_smooth can be used to add a regression line to a data-set. ggplot(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The argument method = &quot;lm&quot; specifies that the function lm is to be used to generate fitted values. It is equivalent to running the regression lm(y ~ x) and plotting the regression line, where y and x are the aesthetics specified by the mappings. The argument se = FALSE tells the function not to plot the confidence interval of the regresion (discussed later). 5.5.4 Regresion towards the mean 5.5.5 Merging Data Sets in R See the R for Data Science chapter Relational data. Merge - or intter join the data frames by state: pres12 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pres12.csv&quot;)) full_join(pres08, pres12, by = &quot;state&quot;) #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama.x McCain EV.x margin Obama.y Romney EV.y #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows Note: this could be a question. How would you change the .x, .y suffixes to something more informative To avoid the duplicate names, or change them, you can rename before merging, or use the suffix argument: pres &lt;- full_join(pres08, pres12, by = &quot;state&quot;, suffix = c(&quot;_08&quot;, &quot;_12&quot;)) pres #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama_08 McCain EV_08 margin Obama_12 Romney EV_12 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows The dplyr equivalent functions for cbind is bind_cols. pres &lt;- pres %&gt;% mutate(Obama2008.z = scale(Obama_08), Obama2012.z = scale(Obama_12)) Scatterplot of states with vote shares in 2008 and 2012 ggplot(pres, aes(x = Obama2008.z, y = Obama2012.z, label = state)) + geom_text() + geom_abline() + coord_fixed() + scale_x_continuous(&quot;Obama&#39;s standardized vote share in 2008&quot;, limits = c(-4, 4)) + scale_y_continuous(&quot;Obama&#39;s standardized vote share in 2012&quot;, limits = c(-4, 4)) To calcualte the bottom and top quartiles pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.25)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.583 pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.75)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.5 Challenge: Why is it important to standardize the vote shares? 5.5.6 Model Fit florida &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;florida.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; county = col_character(), #&gt; Clinton96 = col_integer(), #&gt; Dole96 = col_integer(), #&gt; Perot96 = col_integer(), #&gt; Bush00 = col_integer(), #&gt; Gore00 = col_integer(), #&gt; Buchanan00 = col_integer() #&gt; ) fit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida) summary(fit2) #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -612.7 -66.0 1.9 32.9 2301.7 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.34575 49.75931 0.03 0.98 #&gt; Perot96 0.03592 0.00434 8.28 9.5e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 316 on 65 degrees of freedom #&gt; Multiple R-squared: 0.513, Adjusted R-squared: 0.506 #&gt; F-statistic: 68.5 on 1 and 65 DF, p-value: 9.47e-12 In addition to, summary(fit2)$r.squared #&gt; [1] 0.513 we can get the R2 squared value from the data frame glance returns: glance(fit2) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.513 0.506 316 68.5 9.47e-12 2 -480 966 972 #&gt; deviance df.residual #&gt; 1 6506118 65 We can get predictions and residuals on the original data frame using the modelr functions add_residuals and add_predictions florida &lt;- florida %&gt;% add_predictions(fit2) %&gt;% add_residuals(fit2) glimpse(florida) #&gt; Observations: 67 #&gt; Variables: 9 #&gt; $ county &lt;chr&gt; &quot;Alachua&quot;, &quot;Baker&quot;, &quot;Bay&quot;, &quot;Bradford&quot;, &quot;Brevard&quot;, &quot;... #&gt; $ Clinton96 &lt;int&gt; 40144, 2273, 17020, 3356, 80416, 320736, 1794, 2712... #&gt; $ Dole96 &lt;int&gt; 25303, 3684, 28290, 4038, 87980, 142834, 1717, 2783... #&gt; $ Perot96 &lt;int&gt; 8072, 667, 5922, 819, 25249, 38964, 630, 7783, 7244... #&gt; $ Bush00 &lt;int&gt; 34124, 5610, 38637, 5414, 115185, 177323, 2873, 354... #&gt; $ Gore00 &lt;int&gt; 47365, 2392, 18850, 3075, 97318, 386561, 2155, 2964... #&gt; $ Buchanan00 &lt;int&gt; 263, 73, 248, 65, 570, 788, 90, 182, 270, 186, 122,... #&gt; $ pred &lt;dbl&gt; 291.3, 25.3, 214.0, 30.8, 908.2, 1400.7, 24.0, 280.... #&gt; $ resid &lt;dbl&gt; -2.83e+01, 4.77e+01, 3.40e+01, 3.42e+01, -3.38e+02,... There are now two new columns in florida, pred with the fitted values (predictions), and resid with the residuals. Use fit2_augment to create a residual plot: fit2_resid_plot &lt;- ggplot(florida, aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) fit2_resid_plot Note, we use the function geom_refline to add a reference line at 0. Let’s add some labels to points, who is that outlier? fit2_resid_plot + geom_label(aes(label = county)) The outlier county is “Palm Beach” arrange(florida) %&gt;% arrange(desc(abs(resid))) %&gt;% select(county, resid) %&gt;% head() #&gt; # A tibble: 6 × 2 #&gt; county resid #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 PalmBeach 2302 #&gt; 2 Broward -613 #&gt; 3 Lee -357 #&gt; 4 Brevard -338 #&gt; 5 Miami-Dade -329 #&gt; 6 Pinellas -317 Ted Mosby dressed as a Hanging Chad in “How I Met Your Mother” Data without Palm Beach florida_pb &lt;- filter(florida, county != &quot;PalmBeach&quot;) fit3 &lt;- lm(Buchanan00 ~ Perot96, data = florida_pb) summary(fit3) #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida_pb) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -206.7 -43.5 -16.0 26.9 269.0 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 45.84193 13.89275 3.3 0.0016 ** #&gt; Perot96 0.02435 0.00127 19.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 87.7 on 64 degrees of freedom #&gt; Multiple R-squared: 0.851, Adjusted R-squared: 0.849 #&gt; F-statistic: 366 on 1 and 64 DF, p-value: &lt;2e-16 \\(R^2\\) or coefficient of determination glance(fit3) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.851 0.849 87.7 366 3.61e-28 2 -388 782 788 #&gt; deviance df.residual #&gt; 1 492803 64 florida_pb %&gt;% add_residuals(fit3) %&gt;% add_predictions(fit3) %&gt;% ggplot(aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + ylim(-750, 2500) + xlim(0, 1500) + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) Create predictions for both models using data_grid and gather_predictions: florida_grid &lt;- florida %&gt;% data_grid(Perot96) %&gt;% gather_predictions(fit2, fit3) %&gt;% mutate(model = fct_recode(model, &quot;Regression\\n with Palm Beach&quot; = &quot;fit2&quot;, &quot;Regression\\n without Palm Beach&quot; = &quot;fit3&quot;)) Note this is an example of using non-syntactic column names in a tibble, as discussed in Chapter 10 of R for data science. ggplot() + geom_point(data = florida, mapping = aes(x = Perot96, y = Buchanan00)) + geom_line(data = florida_grid, mapping = aes(x = Perot96, y = pred, colour = model)) + geom_label(data = filter(florida, county == &quot;PalmBeach&quot;), mapping = aes(x = Perot96, y = Buchanan00, label = county), vjust = &quot;top&quot;, hjust = &quot;right&quot;) + geom_text(data = tibble(label = unique(florida_grid$model), x = c(20000, 31000), y = c(1000, 300)), mapping = aes(x = x, y = y, label = label, colour = label)) + labs(x = &quot;Perot&#39;s Vote in 1996&quot;, y = &quot;Buchanan&#39;s Votes in 1996&quot;) + theme(legend.position = &quot;none&quot;) See Graphics for communication in R for Data Science on labels and annotations in plots. 5.6 Regression and Causation Load data women &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;women.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; GP = col_integer(), #&gt; village = col_integer(), #&gt; reserved = col_integer(), #&gt; female = col_integer(), #&gt; irrigation = col_integer(), #&gt; water = col_integer() #&gt; ) proportion of female politicians in reserved GP vs. unreserved GP women %&gt;% group_by(reserved) %&gt;% summarise(prop_female = mean(female)) #&gt; # A tibble: 2 × 2 #&gt; reserved prop_female #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0.0748 #&gt; 2 1 1.0000 The diff in diff estimator ## drinking-water facilities ## irrigation facilities mean(women$irrigation[women$reserved == 1]) - mean(women$irrigation[women$reserved == 0]) #&gt; [1] -0.369 Mean values of irrigation and water in reserved and non-reserved districts. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) #&gt; # A tibble: 2 × 3 #&gt; reserved irrigation water #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 3.39 14.7 #&gt; 2 1 3.02 24.0 The difference between the two groups can be calculated with the function diff, which calculates the difference between subsequent observations. This works as long as we are careful about which group is first or second. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% summarise(diff_irrigation = diff(irrigation), diff_water = diff(water)) #&gt; # A tibble: 1 × 2 #&gt; diff_irrigation diff_water #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.369 9.25 The other way uses tidyr spread and gather, women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% gather(variable, value, -reserved) %&gt;% spread(reserved, value) %&gt;% mutate(diff = `1` - `0`) #&gt; # A tibble: 2 × 4 #&gt; variable `0` `1` diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 irrigation 3.39 3.02 -0.369 #&gt; 2 water 14.74 23.99 9.252 Now each row is an outcome variable of interest, and there are columns for the treatment (1) and control (0) groups, and the difference (diff). lm(water ~ reserved, data = women) %&gt;% summary() #&gt; #&gt; Call: #&gt; lm(formula = water ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -23.99 -14.74 -7.86 2.26 316.01 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 14.74 2.29 6.45 4.2e-10 *** #&gt; reserved 9.25 3.95 2.34 0.02 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 33.4 on 320 degrees of freedom #&gt; Multiple R-squared: 0.0169, Adjusted R-squared: 0.0138 #&gt; F-statistic: 5.49 on 1 and 320 DF, p-value: 0.0197 lm(irrigation ~ reserved, data = women) %&gt;% summary() #&gt; #&gt; Call: #&gt; lm(formula = irrigation ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.39 -3.39 -3.02 -1.02 86.61 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.388 0.650 5.21 3.3e-07 *** #&gt; reserved -0.369 1.122 -0.33 0.74 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 9.51 on 320 degrees of freedom #&gt; Multiple R-squared: 0.000338, Adjusted R-squared: -0.00279 #&gt; F-statistic: 0.108 on 1 and 320 DF, p-value: 0.742 5.6.1 Regression with multiple predictors social &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;social.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2008 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) glimpse(social) #&gt; Observations: 305,866 #&gt; Variables: 6 #&gt; $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;mal... #&gt; $ yearofbirth &lt;int&gt; 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 19... #&gt; $ primary2004 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,... #&gt; $ messages &lt;chr&gt; &quot;Civic Duty&quot;, &quot;Civic Duty&quot;, &quot;Hawthorne&quot;, &quot;Hawthorn... #&gt; $ primary2008 &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,... #&gt; $ hhsize &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 1,... levels(social$messages) #&gt; NULL fit &lt;- lm(primary2008 ~ messages, data = social) fit #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ messages, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesControl messagesHawthorne #&gt; 0.31454 -0.01790 0.00784 #&gt; messagesNeighbors #&gt; 0.06341 Create indicator variables social &lt;- social %&gt;% mutate(Control = as.integer(messages == &quot;Control&quot;), Hawthorne = as.integer(messages == &quot;Hawthorne&quot;), Neighbors = as.integer(messages == &quot;Neighbors&quot;)) alternatively, create these using a for loop. This is easier to understand and less prone to typo. for (i in unique(social$messages)) { social[[i]] &lt;- as.integer(social[[&quot;messages&quot;]] == i) } We created a variable for each level of messages even though we will exclude one of them. lm(primary2008 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) Control Hawthorne Neighbors #&gt; 0.31454 -0.01790 0.00784 0.06341 Create predictions for each unique value of messages unique_messages &lt;- data_grid(social, messages) %&gt;% add_predictions(fit) unique_messages #&gt; # A tibble: 4 × 2 #&gt; messages pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Compare to the sample averages social %&gt;% group_by(messages) %&gt;% summarise(mean(primary2008)) #&gt; # A tibble: 4 × 2 #&gt; messages `mean(primary2008)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Linear regression without intercept. fit.noint &lt;- lm(primary2008 ~ -1 + messages, data = social) fit.noint #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ -1 + messages, data = social) #&gt; #&gt; Coefficients: #&gt; messagesCivic Duty messagesControl messagesHawthorne #&gt; 0.315 0.297 0.322 #&gt; messagesNeighbors #&gt; 0.378 Calculating the regression average effect is also easier if we make the control group the first level so all regression coefficients are comparisons to it. Use fct_relevel to make “Control” fit.control &lt;- mutate(social, messages = fct_relevel(messages, &quot;Control&quot;)) %&gt;% lm(primary2008 ~ messages, data = .) fit.control #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ messages, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesCivic Duty messagesHawthorne #&gt; 0.2966 0.0179 0.0257 #&gt; messagesNeighbors #&gt; 0.0813 Difference in means social %&gt;% group_by(messages) %&gt;% summarise(primary2008 = mean(primary2008)) %&gt;% mutate(Control = primary2008[messages == &quot;Control&quot;], diff = primary2008 - Control) #&gt; # A tibble: 4 × 4 #&gt; messages primary2008 Control diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Adjusted R-squared is included in the output of broom::glance() glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC #&gt; 1 0.00328 0.00327 0.463 336 1.06e-217 4 -198247 396504 #&gt; BIC deviance df.residual #&gt; 1 396557 65468 305862 glance(fit)$adj.r.squared #&gt; [1] 0.00327 5.6.2 Heterogenous Treatment Effects Average treatment effect (ate) among those who voted in 2004 primary ate &lt;- social %&gt;% group_by(primary2004, messages) %&gt;% summarise(primary2008 = mean(primary2008)) %&gt;% spread(messages, primary2008) %&gt;% mutate(ate_Neighbors = Neighbors - Control) %&gt;% select(primary2004, Neighbors, Control, ate_Neighbors) ate #&gt; Source: local data frame [2 x 4] #&gt; Groups: primary2004 [2] #&gt; #&gt; primary2004 Neighbors Control ate_Neighbors #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.306 0.237 0.0693 #&gt; 2 1 0.482 0.386 0.0965 Difference in ATE in 2004 voters and non-voters diff(ate$ate_Neighbors) #&gt; [1] 0.0272 social.neighbor &lt;- social %&gt;% filter((messages == &quot;Control&quot;) | (messages == &quot;Neighbors&quot;)) fit.int &lt;- lm(primary2008 ~ primary2004 + messages + primary2004:messages, data = social.neighbor) fit.int #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ primary2004 + messages + primary2004:messages, #&gt; data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 lm(primary2008 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 social.neighbor &lt;- social.neighbor %&gt;% mutate(age = 2008 - yearofbirth) summary(social.neighbor$age) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 22.0 43.0 52.0 51.8 61.0 108.0 fit.age &lt;- lm(primary2008 ~ age * messages, data = social.neighbor) fit.age #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ age * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) age messagesNeighbors #&gt; 0.089477 0.003998 0.048573 #&gt; age:messagesNeighbors #&gt; 0.000628 Calculate average treatment effects ate.age &lt;- crossing(age = seq(from = 25, to = 85, by = 20), messages = c(&quot;Neighbors&quot;, &quot;Control&quot;)) %&gt;% add_predictions(fit.age) %&gt;% spread(messages, pred) %&gt;% mutate(diff = Neighbors - Control) ate.age #&gt; # A tibble: 4 × 4 #&gt; age Control Neighbors diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 25 0.189 0.254 0.0643 #&gt; 2 45 0.269 0.346 0.0768 #&gt; 3 65 0.349 0.439 0.0894 #&gt; 4 85 0.429 0.531 0.1020 You can use poly for polynomials instead of age + I(age ^ 2). Though note that the coefficients will be be different since by default poly calculates orthogonal polynomials instead of the natural (raw) polynomials. However, you really shouldn’t interpet the coefficients directly anyways, so this should matter. fit.age2 &lt;- lm(primary2008 ~ poly(age, 2) * messages, data = social.neighbor) fit.age2 #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ poly(age, 2) * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) poly(age, 2)1 #&gt; 0.2966 27.6665 #&gt; poly(age, 2)2 messagesNeighbors #&gt; -10.2832 0.0816 #&gt; poly(age, 2)1:messagesNeighbors poly(age, 2)2:messagesNeighbors #&gt; 4.5820 -5.5124 Create a data frame of combinations of ages and messages using data_grid, which means that we only need to specify the variables, and not the specific values, y.hat &lt;- data_grid(social.neighbor, age, messages) %&gt;% add_predictions(fit.age2) ggplot(y.hat, aes(x = age, y = pred, colour = str_c(messages, &quot; condition&quot;))) + geom_line() + labs(colour = &quot;&quot;, y = &quot;predicted turnout rates&quot;) + theme(legend.position = &quot;bottom&quot;) y.hat %&gt;% spread(messages, pred) %&gt;% mutate(ate = Neighbors - Control) %&gt;% filter(age &gt; 20, age &lt; 90) %&gt;% ggplot(aes(x = age, y = ate)) + geom_line() + labs(y = &quot;estimated average treatment effect&quot;) + ylim(0, 0.1) 5.7 Regression Discontinuity Design Original code MPs &lt;- read.csv(&quot;MPs.csv&quot;) MPs.labour &lt;- subset(MPs, subset = (party == &quot;labour&quot;)) MPs.tory &lt;- subset(MPs, subset = (party == &quot;tory&quot;)) ## two regressions for Labour: negative and positive margin labour.fit1 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &lt; 0, ]) labour.fit2 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &gt; 0, ]) ## two regressions for Tory: negative and positive margin tory.fit1 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &lt; 0, ]) tory.fit2 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &gt; 0, ]) Tidyverse MPs &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;MPs.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; firstname = col_character(), #&gt; party = col_character(), #&gt; ln.gross = col_double(), #&gt; ln.net = col_double(), #&gt; yob = col_integer(), #&gt; yod = col_integer(), #&gt; margin.pre = col_double(), #&gt; region = col_character(), #&gt; margin = col_double() #&gt; ) MPs_labour &lt;- filter(MPs, party == &quot;labour&quot;) MPs_tory &lt;- filter(MPs, party == &quot;tory&quot;) labour_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &lt; 0)) labour_fit2 &lt;- lm(ln.net ~ margin, MPs_labour, margin &gt; 0) tory_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &gt; 0)) Original code ## Labour: range of predictions y1l.range &lt;- c(min(MPs.labour$margin), 0) # min to 0 y2l.range &lt;- c(0, max(MPs.labour$margin)) # 0 to max ## prediction y1.labour &lt;- predict(labour.fit1, newdata = data.frame(margin = y1l.range)) y2.labour &lt;- predict(labour.fit2, newdata = data.frame(margin = y2l.range)) ## Tory: range of predictions y1t.range &lt;- c(min(MPs.tory$margin), 0) # min to 0 y2t.range &lt;- c(0, max(MPs.tory$margin)) # 0 to max ## predict outcome y1.tory &lt;- predict(tory.fit1, newdata = data.frame(margin = y1t.range)) y2.tory &lt;- predict(tory.fit2, newdata = data.frame(margin = y2t.range)) Tidyverse code Use data_grid to generate a grid for predictions. y1_labour &lt;- filter(MPs_labour, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit1) y2_labour &lt;- filter(MPs_labour, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit2) y1_tory &lt;- filter(MPs_tory, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit1) y2_tory &lt;- filter(MPs_tory, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit2) Original code ## scatterplot with regression lines for labour plot(MPs.labour$margin, MPs.labour$ln.net, main = &quot;Labour&quot;, xlim = c(-0.5, 0.5), ylim = c(6, 18), xlab = &quot;margin of victory&quot;, ylab = &quot;log net wealth at death&quot;) abline(v = 0, lty = &quot;dashed&quot;) ## add regression lines lines(y1l.range, y1.labour, col = &quot;red&quot;) lines(y2l.range, y2.labour, col = &quot;red&quot;) ## scatterplot with regression lines for tory plot(MPs.tory$margin, MPs.tory$ln.net, main = &quot;Tory&quot;, xlim = c(-0.5, 0.5), ylim = c(6, 18), xlab = &quot;margin of victory&quot;, ylab = &quot;log net wealth at death&quot;) abline(v = 0, lty = &quot;dashed&quot;) ## add regression lines lines(y1t.range, y1.tory, col = &quot;red&quot;) lines(y2t.range, y2.tory, col = &quot;red&quot;) Tidyverse code Labour politicians ggplot() + geom_ref_line(v = 0) + geom_point(data = MPs_labour, mapping = aes(x = margin, y = ln.net)) + geom_line(data = y1_labour, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + geom_line(data = y2_labour, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;, title = &quot;Labour&quot;) Tory politicians ggplot() + geom_ref_line(v = 0) + geom_point(data = MPs_tory, mapping = aes(x = margin, y = ln.net)) + geom_line(data = y1_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + geom_line(data = y2_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;, title = &quot;labour&quot;) We can actually produce this plot easily without running the regressions, by using geom_smooth: ggplot(mutate(MPs, winner = (margin &gt; 0)), aes(x = margin, y = ln.net)) + geom_ref_line(v = 0) + geom_point() + geom_smooth(method = lm, se = FALSE, mapping = aes(group = winner)) + facet_grid(party ~ .) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;) Original code ## average net wealth for Tory MP tory.MP &lt;- exp(y2.tory[1]) tory.MP ## 1 ## 533813.5 ## average net wealth for Tory non-MP tory.nonMP &lt;- exp(y1.tory[2]) tory.nonMP ## 2 ## 278762.5 ## causal effect in pounds tory.MP - tory.nonMP ## 1 ## 255050.9 Tidyverse code In the previous code, I didn’t directly compute the the average net wealth at 0, so I’ll need to do that here. I’ll use gather_predictions to add predictions for multiple models: spread_predictions(data_frame(margin = 0), tory_fit1, tory_fit2) %&gt;% mutate(rd_est = tory_fit2 - tory_fit1) #&gt; # A tibble: 1 × 4 #&gt; margin tory_fit1 tory_fit2 rd_est #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 12.5 13.2 0.65 Original code ## two regressions for Tory: negative and positive margin tory.fit3 &lt;- lm(margin.pre ~ margin, data = MPs.tory[MPs.tory$margin &lt; 0, ]) tory.fit4 &lt;- lm(margin.pre ~ margin, data = MPs.tory[MPs.tory$margin &gt; 0, ]) ## the difference between two intercepts is the estimated effect coef(tory.fit4)[1] - coef(tory.fit3)[1] ## (Intercept) ## -0.01725578 Tidyverse code tory_fit3 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit4 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &gt; 0)) (filter(tidy(tory_fit3), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]] - filter(tidy(tory_fit4), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]]) #&gt; [1] 0.0173 "],
["discovery.html", "6 Discovery 6.1 Prerequisites", " 6 Discovery 6.1 Prerequisites library(&quot;tidyverse&quot;) "],
["probability.html", "7 Probability 7.1 Prerequisites 7.2 Probability 7.3 Conditional Probability 7.4 Large Sample Theorems", " 7 Probability 7.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;stringr&quot;) 7.2 Probability Original code k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse birthday &lt;- function(k) { logdenom &lt;- k * log(365) + lfactorial(365 - k) lognumer &lt;- lfactorial(365) pr &lt;- 1 - exp(lognumer - logdenom) pr } bday &lt;- tibble(k = 1:50, pr = birthday(k)) ggplot(bday, aes(x = k , y = pr)) + geom_hline(yintercept = 0.5, colour = &quot;white&quot;, size = 2) + geom_point() + scale_y_continuous(&quot;Probability that at least two\\n people have the same birthday&quot;, limits = c(0, 1)) + labs(x = &quot;Number of people&quot;) Note: The logarithm is used for numerical stability. Basically, “floating-point” numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues. See “Falling Into the Floating Point Trap” in The R Inforno for a summary of floating point numbers. See these John Fox posts 1 2 for an example of numerical stability gone wrong. Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/. 7.2.1 Sampling without replacement Original code: k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse code: Instead of using a for loop, we could do the simulations using a functional as described in R for Data Science. Define the function sim_bdays to randomly sample k birthdays, and returns TRUE if there are any duplicates. sim_bdays &lt;- function(k) { days &lt;- sample(1:365, k, replace = TRUE) length(unique(days)) &lt; k } Set the parameters for 1,000 simulations, and 23 individuals. We use map_lgl since sim_bdays returns a logical value (TRUE, FALSE): sims &lt;- 1000 k &lt;- 23 map_lgl(seq_len(sims), ~ sim_bdays(k)) %&gt;% mean() #&gt; [1] 0.486 7.2.2 Combinations Original code choose(84, 6) #&gt; [1] 4.06e+08 7.3 Conditional Probability 7.3.1 Conditional, Marginal, and Joint Probabilities Original: FLVoters &lt;- read.csv(&quot;FLVoters.csv&quot;) dim(FLVoters) # before removal of missing data ## [1] 10000 6 FLVoters &lt;- na.omit(FLVoters) dim(FLVoters) # after removal tidyverse FLVoters &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLVoters.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; county = col_integer(), #&gt; VTD = col_integer(), #&gt; age = col_integer(), #&gt; gender = col_character(), #&gt; race = col_character() #&gt; ) dim(FLVoters) #&gt; [1] 10000 6 FLVoters &lt;- FLVoters %&gt;% na.omit() original: margin.race &lt;- prop.table(table(FLVoters$race)) margin.race margin.gender &lt;- prop.table(table(FLVoters$gender)) margin.gender tidyverse: Instead of using prop.base, we calculate the probabilities with a data frame. Marginal probabilities of race, margin_race &lt;- FLVoters %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) margin_race #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 175 0.01920 #&gt; 2 black 1194 0.13102 #&gt; 3 hispanic 1192 0.13080 #&gt; 4 native 29 0.00318 #&gt; 5 other 310 0.03402 #&gt; 6 white 6213 0.68177 Marginal probabilities of gender: margin_gender &lt;- FLVoters %&gt;% count(gender) %&gt;% mutate(prop = n / sum(n)) margin_gender #&gt; # A tibble: 2 × 3 #&gt; gender n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f 4883 0.536 #&gt; 2 m 4230 0.464 Original: prop.table(table(FLVoters$race[FLVoters$gender == &quot;f&quot;])) tidyverse: FLVoters %&gt;% filter(gender == &quot;f&quot;) %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 83 0.01700 #&gt; 2 black 678 0.13885 #&gt; 3 hispanic 666 0.13639 #&gt; 4 native 17 0.00348 #&gt; 5 other 158 0.03236 #&gt; 6 white 3281 0.67192 Original: joint.p &lt;- prop.table(table(race = FLVoters$race, gender = FLVoters$gender)) joint.p #&gt; gender #&gt; race f m #&gt; asian 0.00911 0.01010 #&gt; black 0.07440 0.05662 #&gt; hispanic 0.07308 0.05772 #&gt; native 0.00187 0.00132 #&gt; other 0.01734 0.01668 #&gt; white 0.36004 0.32174 tidyverse: joint_p &lt;- FLVoters %&gt;% count(gender, race) %&gt;% # needed because it is still grouped by gender ungroup() %&gt;% mutate(prop = n / sum(n)) joint_p #&gt; # A tibble: 12 × 4 #&gt; gender race n prop #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f asian 83 0.00911 #&gt; 2 f black 678 0.07440 #&gt; 3 f hispanic 666 0.07308 #&gt; 4 f native 17 0.00187 #&gt; 5 f other 158 0.01734 #&gt; 6 f white 3281 0.36004 #&gt; # ... with 6 more rows We can convert the data frame to have gender as columns: joint_p %&gt;% ungroup() %&gt;% select(-n) %&gt;% spread(gender, prop) #&gt; # A tibble: 6 × 3 #&gt; race f m #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian 0.00911 0.01010 #&gt; 2 black 0.07440 0.05662 #&gt; 3 hispanic 0.07308 0.05772 #&gt; 4 native 0.00187 0.00132 #&gt; 5 other 0.01734 0.01668 #&gt; 6 white 0.36004 0.32174 Original: rowSums(joint.p) tidyverse: Sum over race: joint_p %&gt;% group_by(race) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 6 × 2 #&gt; race prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 asian 0.01920 #&gt; 2 black 0.13102 #&gt; 3 hispanic 0.13080 #&gt; 4 native 0.00318 #&gt; 5 other 0.03402 #&gt; 6 white 0.68177 Original: colSums(joint.p) tidyverse: Sum over gender joint_p %&gt;% group_by(gender) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 2 × 2 #&gt; gender prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 f 0.536 #&gt; 2 m 0.464 Original: FLVoters$age.group &lt;- NA # initialize a variable FLVoters$age.group[FLVoters$age &lt;= 20] &lt;- 1 FLVoters$age.group[FLVoters$age &gt; 20 &amp; FLVoters$age &lt;= 40] &lt;- 2 FLVoters$age.group[FLVoters$age &gt; 40 &amp; FLVoters$age &lt;= 60] &lt;- 3 FLVoters$age.group[FLVoters$age &gt; 60] &lt;- 4 tidyverse: Use the cut FLVoters &lt;- FLVoters %&gt;% mutate(age_group = cut(age, c(0, 20, 40, 60, Inf), right = TRUE, labels = c(&quot;&lt;= 20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;&gt; 60&quot;))) Original: joint3 &lt;- prop.table(table(race = FLVoters$race, age.group = FLVoters$age.group, gender = FLVoters$gender)) joint3 tidyverse: joint3 &lt;- FLVoters %&gt;% count(race, age_group, gender) %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) joint3 #&gt; # A tibble: 47 × 5 #&gt; race age_group gender n prop #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian &lt;= 20 f 1 0.000110 #&gt; 2 asian &lt;= 20 m 2 0.000219 #&gt; 3 asian 20-40 f 24 0.002634 #&gt; 4 asian 20-40 m 26 0.002853 #&gt; 5 asian 40-60 f 38 0.004170 #&gt; 6 asian 40-60 m 47 0.005157 #&gt; # ... with 41 more rows original: margin.age &lt;- prop.table(table(FLVoters$age.group)) margin.age joint3[&quot;black&quot;, 4, &quot;f&quot;] / margin.age[4] tidyverse: Marginal probabilities by age groups margin_age &lt;- FLVoters %&gt;% count(age_group) %&gt;% mutate(prop = n / sum(n)) margin_age #&gt; # A tibble: 4 × 3 #&gt; age_group n prop #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 161 0.0177 #&gt; 2 20-40 2469 0.2709 #&gt; 3 40-60 3285 0.3605 #&gt; 4 &gt; 60 3198 0.3509 Calculate the probabilities that each group is in a given age group, and show $P( | ) left_join(joint3, select(margin_age, age_group, margin_age = prop), by = &quot;age_group&quot;) %&gt;% mutate(prob_age_group = prop / margin_age) %&gt;% filter(race == &quot;black&quot;, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;) %&gt;% select(race, age_group, gender, prob_age_group) #&gt; # A tibble: 1 × 4 #&gt; race age_group gender prob_age_group #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black &gt; 60 f 0.0538 Original: joint2 &lt;- prop.table(table(age.group = FLVoters$age.group, gender = FLVoters$gender)) joint2 joint2[4, &quot;f&quot;] joint3[&quot;black&quot;, 4, &quot;f&quot;] / joint2[4, &quot;f&quot;] tidyverse: Two-way joint probability table for age group and gender joint2 &lt;- FLVoters %&gt;% count(age_group, gender) %&gt;% ungroup() %&gt;% mutate(prob_age_gender = n / sum(n)) joint2 #&gt; # A tibble: 8 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 f 88 0.00966 #&gt; 2 &lt;= 20 m 73 0.00801 #&gt; 3 20-40 f 1304 0.14309 #&gt; 4 20-40 m 1165 0.12784 #&gt; 5 40-60 f 1730 0.18984 #&gt; 6 40-60 m 1555 0.17064 #&gt; # ... with 2 more rows The joint probability \\(P(\\text{age} &gt; 60 \\land \\text{female})\\), filter(joint2, age_group == &quot;&gt; 60&quot;, gender == &quot;f&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f 1761 0.193 The onditional probablities \\(P(race | gender, age)\\), condprob_race &lt;- left_join(joint3, select(joint2, -n), by = c(&quot;age_group&quot;, &quot;gender&quot;)) %&gt;% mutate(prob_race = prop / prob_age_gender) %&gt;% arrange(age_group, gender) %&gt;% select(age_group, gender, race, prob_race) Each row is the \\(P(race | age_group, gender)\\), so \\(P(\\text{black} | \\text{female} \\land \\text{age} &gt; 60)\\), filter(condprob_race, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;, race == &quot;black&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender race prob_race #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f black 0.0977 7.3.2 Independence Original code plot(c(margin.race * margin.gender[&quot;f&quot;]), # product of marginal probs. c(joint.p[, &quot;f&quot;]), # joint probabilities xlim = c(0, 0.4), ylim = c(0, 0.4), xlab = &quot;P(race) * P(female)&quot;, ylab = &quot;P(race and female)&quot;) abline(0, 1) # 45 degree line Tidyverse Create a table with the products of margins of race and age. Using the function crossing to create a tibble with all combinations of race and gender and the indep prob race_gender_indep &lt;- crossing(select(margin_race, race, prob_race = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(prob_indep = prob_race * prob_gender) %&gt;% left_join(select(joint_p, gender, race, prob = prop), by = c(&quot;gender&quot;, &quot;race&quot;)) %&gt;% select(race, gender, everything()) race_gender_indep #&gt; # A tibble: 12 × 6 #&gt; race gender prob_race prob_gender prob_indep prob #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian f 0.0192 0.536 0.01029 0.00911 #&gt; 2 asian m 0.0192 0.464 0.00891 0.01010 #&gt; 3 black f 0.1310 0.536 0.07021 0.07440 #&gt; 4 black m 0.1310 0.464 0.06082 0.05662 #&gt; 5 hispanic f 0.1308 0.536 0.07009 0.07308 #&gt; 6 hispanic m 0.1308 0.464 0.06071 0.05772 #&gt; # ... with 6 more rows ggplot(race_gender_indep, aes(x = prob_indep, y = prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(. ~ gender) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(x = expression(P(&quot;race&quot;) * P(&quot;gender&quot;)), y = expression(P(&quot;race and gender&quot;))) Original code ## joint independence plot(c(joint3[, 4, &quot;f&quot;]), # joint probability margin.race * margin.age[4] * margin.gender[&quot;f&quot;], # product of marginals xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;joint independence&quot;, xlab = &quot;P(race and above 60 and female)&quot;, ylab = &quot;P(race) * P(above 60) * P(female)&quot;) abline(0, 1) ## conditional independence given female plot(c(joint3[, 4, &quot;f&quot;]) / margin.gender[&quot;f&quot;], # joint prob. given female ## product of marginals (joint.p[, &quot;f&quot;] / margin.gender[&quot;f&quot;]) * (joint2[4, &quot;f&quot;] / margin.gender[&quot;f&quot;]), xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;marginal independence&quot;, xlab = &quot;P(race and above 60 | female)&quot;, ylab = &quot;P(race | female) * P(above 60 | female)&quot;) abline(0, 1) While the original code only calculates joint-independence value for values of age &gt; 60, and female, this calculates the joint probabilities for all combinations of the three variables, and facets by age and gender. joint_indep &lt;- # all combinations of race, age, gender crossing(select(margin_race, race, prob_race = prop), select(margin_age, age_group, prob_age = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(indep_prob = prob_race * prob_age * prob_gender) %&gt;% left_join(select(joint3, race, age_group, gender, prob = prop), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% replace_na(list(prob = 0)) ggplot(joint_indep, aes(x = prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age and gender)&quot;, y = &quot;P(race) * P(age) * P(gender)&quot;, title = &quot;Joint Independence&quot;) The original code only calculates the conditional independence given female; this calculates conditional independence for all values of gender: cond_gender &lt;- left_join(select(joint3, race, age_group, gender, joint_prob = prop), select(margin_gender, gender, prob_gender = prop), by = c(&quot;gender&quot;)) %&gt;% mutate(cond_prob = joint_prob / prob_gender) # P(race | gender) prob_race_gender &lt;- left_join(select(joint_p, race, gender, prob_race_gender = prop), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_race = prob_race_gender / prob_gender) # P(age | gender) prob_age_gender &lt;- left_join(select(joint2, age_group, gender, prob_age_gender), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_age = prob_age_gender / prob_gender) # indep prob of race and age indep_cond_gender &lt;- full_join(select(prob_race_gender, race, gender, prob_race), select(prob_age_gender, age_group, gender, prob_age), by = &quot;gender&quot;) %&gt;% mutate(indep_prob = prob_race * prob_age) inner_join(select(indep_cond_gender, race, age_group, gender, indep_prob), select(cond_gender, race, age_group, gender, cond_prob), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% ggplot(aes(x = cond_prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age | gender)&quot;, y = &quot;P(race | gender) * P(age | gender)&quot;, title = &quot;Marginal independence&quot;) Orginal code for the Monty-Hall problem sims &lt;- 1000 doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) result.switch &lt;- result.noswitch &lt;- rep(NA, sims) for (i in 1:sims) { ## randomly choose the initial door first &lt;- sample(1:3, size = 1) result.noswitch[i] &lt;- doors[first] remain &lt;- doors[-first] # remaining two doors ## Monty chooses one door with a goat monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) result.switch[i] &lt;- remain[-monty] } mean(result.noswitch == &quot;car&quot;) This code doesn’t rely on any non-tidyverse code, but the following is another way to write it. First, create a function for a single iteration. This will save the results as data frame. choose_door &lt;- function(.id) { # put doors inside the function to ensure it doesn&#39;t get changed doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) first &lt;- sample(1:3, 1) remain &lt;- doors[-first] monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) ret &lt;- tribble(~strategy, ~result, &quot;no switch&quot;, doors[first], &quot;switch&quot;, remain[-monty]) ret[[&quot;.id&quot;]] &lt;- .id ret } Now use map_df to run choose_door multiple times, and then summarize the results: sims &lt;- 1000 results &lt;- map_df(seq_len(sims), choose_door) results %&gt;% group_by(strategy) %&gt;% summarise(pct_win = mean(result == &quot;car&quot;)) #&gt; # A tibble: 2 × 2 #&gt; strategy pct_win #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 no switch 0.35 #&gt; 2 switch 0.65 Can we make this even more general? What about a function that takes a data frame as its input with the choices? … 7.3.3 Predicting Race Using Surname and Residence Location Start with the Census names files: cnames &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;names.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; count = col_integer(), #&gt; pctwhite = col_double(), #&gt; pctblack = col_double(), #&gt; pctapi = col_double(), #&gt; pcthispanic = col_double(), #&gt; pctothers = col_double() #&gt; ) glimpse(cnames) #&gt; Observations: 151,671 #&gt; Variables: 7 #&gt; $ surname &lt;chr&gt; &quot;SMITH&quot;, &quot;JOHNSON&quot;, &quot;WILLIAMS&quot;, &quot;BROWN&quot;, &quot;JONES&quot;, ... #&gt; $ count &lt;int&gt; 2376206, 1857160, 1534042, 1380145, 1362755, 11278... #&gt; $ pctwhite &lt;dbl&gt; 73.34, 61.55, 48.52, 60.72, 57.69, 85.80, 64.73, 6... #&gt; $ pctblack &lt;dbl&gt; 22.22, 33.80, 46.72, 34.54, 37.73, 10.41, 30.77, 0... #&gt; $ pctapi &lt;dbl&gt; 0.40, 0.42, 0.37, 0.41, 0.35, 0.42, 0.40, 1.43, 0.... #&gt; $ pcthispanic &lt;dbl&gt; 1.56, 1.50, 1.60, 1.64, 1.44, 1.43, 1.58, 90.82, 9... #&gt; $ pctothers &lt;dbl&gt; 2.48, 2.73, 2.79, 2.69, 2.79, 1.94, 2.52, 1.09, 0.... For each surname, contains variables with the probability that it belongs to an individual of a given race (pctwhite, pctblack, …). We want to find the most-likely race for a given surname, by finding the race with the maximum proportion. Instead of dealing with multiple variables, it is easier to use max on a single variable, so we will rearrange the data to in order to use a grouped summarise, and then merge the new variable back to the original datasets. Note, this code takes a while. most_likely &lt;- cnames %&gt;% select(-count) %&gt;% gather(race_pred, pct, -surname) %&gt;% # remove pct prefix mutate(race_pred = str_replace(race_pred, &quot;^pct&quot;, &quot;&quot;)) %&gt;% # group by surname group_by(surname) %&gt;% filter(pct == max(pct)) %&gt;% # if there are ties, keep only one obs slice(1) %&gt;% # don&#39;t need pct anymore select(-pct) %&gt;% mutate(race_pred = factor(race_pred), # need to convert to factor first, else errors race_pred = fct_recode(race_pred, &quot;asian&quot; = &quot;api&quot;, &quot;other&quot; = &quot;others&quot;)) Now merge that back to the original cnames cnames &lt;- left_join(cnames, most_likely, by = &quot;surname&quot;) Intead of using match, use inner_join to merge the surnames to FLVoters FLVoters &lt;- inner_join(FLVoters, cnames, by = &quot;surname&quot;) dim(FLVoters) #&gt; [1] 8022 14 FLVoters also includes a “native” category that the surname dataset does not. FLVoters &lt;- FLVoters %&gt;% mutate(race2 = fct_recode(race, other = &quot;native&quot;)) Check that the levels of race and race_pred are the same: FLVoters %&gt;% count(race2) #&gt; # A tibble: 5 × 2 #&gt; race2 n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 asian 140 #&gt; 2 black 1078 #&gt; 3 hispanic 1023 #&gt; 4 other 277 #&gt; 5 white 5504 FLVoters %&gt;% count(race_pred) #&gt; # A tibble: 5 × 2 #&gt; race_pred n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 white 6516 #&gt; 2 black 258 #&gt; 3 hispanic 1121 #&gt; 4 asian 120 #&gt; 5 other 7 Now we can calculate True positive rate for all races FLVoters %&gt;% group_by(race2) %&gt;% summarise(tp = mean(race2 == race_pred)) %&gt;% arrange(desc(tp)) #&gt; # A tibble: 5 × 2 #&gt; race2 tp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 white 0.95022 #&gt; 2 hispanic 0.84653 #&gt; 3 asian 0.56429 #&gt; 4 black 0.16048 #&gt; 5 other 0.00361 and the False discovery rate for all races, FLVoters %&gt;% group_by(race_pred) %&gt;% summarise(fp = mean(race2 != race_pred)) %&gt;% arrange(desc(fp)) #&gt; # A tibble: 5 × 2 #&gt; race_pred fp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 other 0.857 #&gt; 2 asian 0.342 #&gt; 3 black 0.329 #&gt; 4 hispanic 0.227 #&gt; 5 white 0.197 Now add get residence data FLCensus &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLCensusVTD.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; county = col_integer(), #&gt; VTD = col_integer(), #&gt; total.pop = col_integer(), #&gt; white = col_double(), #&gt; black = col_double(), #&gt; hispanic = col_double(), #&gt; api = col_double(), #&gt; others = col_double() #&gt; ) \\(P(race)\\) in Florida: race.prop &lt;- FLCensus %&gt;% select(total.pop, white, black, api, hispanic, others) %&gt;% gather(race, pct, -total.pop) %&gt;% group_by(race) %&gt;% summarise(mean = weighted.mean(pct, weights = total.pop)) %&gt;% arrange(desc(mean)) race.prop #&gt; # A tibble: 5 × 2 #&gt; race mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 white 0.6045 #&gt; 2 hispanic 0.2128 #&gt; 3 black 0.1394 #&gt; 4 api 0.0219 #&gt; 5 others 0.0214 7.3.4 Predicting Election Outcomes with Uncertainty Original code pres08 &lt;- read.csv(&quot;pres08.csv&quot;) ## two-party vote share pres08$p &lt;- pres08$Obama / (pres08$Obama + pres08$McCain) Tidyverse code pres08 &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;pres08.csv&quot;)) %&gt;% mutate(p = Obama / (Obama + McCain)) #&gt; Parsed with column specification: #&gt; cols( #&gt; state.name = col_character(), #&gt; state = col_character(), #&gt; Obama = col_integer(), #&gt; McCain = col_integer(), #&gt; EV = col_integer() #&gt; ) Original code n.states &lt;- nrow(pres08) # number of states n &lt;- 1000 # number of respondents sims &lt;- 10000 # number of simulations ## Obama&#39;s electoral votes Obama.ev &lt;- rep(NA, sims) for (i in 1:sims) { ## samples number of votes for Obama in each state draws &lt;- rbinom(n.states, size = n, prob = pres08$p) ## sums state&#39;s electoral college votes if Obama wins the majority Obama.ev[i] &lt;- sum(pres08$EV[draws &gt; n/2]) } hist(Obama.ev, freq = FALSE, main = &quot;Prediction of Election Outcome&quot;, xlab = &quot;Obama&#39;s Electoral College Votes&quot;) abline(v = 364, col = &quot;red&quot;) # actual result Tidyverse code Write a function to simulate the elections. df is the data frame (pres08) with the state, EV, and p columns. n_draws is the size of the binomial distribution to draw from. .id is the simulation number. sim_election &lt;- function(.id, df, n_draws = 1000) { # For each state randomly sample mutate(df, draws = rbinom(n(), n_draws, p)) %&gt;% filter(draws &gt; (n_draws / 2)) %&gt;% summarise(EV = sum(EV), .id = .id) } Now simulate the election 10,000 times: sims &lt;- 10000 sim_results &lt;- map_df(seq_len(sims), ~ sim_election(.x, pres08, n_draws = 1000)) In the 2008 election, Obama received 364 electoral votes ELECTION_EV &lt;- 364 And plot them, ggplot(sim_results, aes(x = EV)) + geom_histogram(binwidth = 10) + geom_vline(xintercept = ELECTION_EV, colour = &quot;red&quot;, size = 2) Original code mean(Obama.ev) ## [1] 352.1646 ## probability of binomial random variable taking greater than n/2 votes sum(pres08$EV * pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE)) ## [1] 352.1388 ## approximate variance using Monte Carlo draws var(Obama.ev) ## [1] 268.7592 ## theoretical variance pres08$pb &lt;- pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE) V &lt;- sum(pres08$pb*(1-pres08$pb)*pres08$EV^2) V ## [1] 268.8008 ## approximate standard deviation using Monte Carlo draws sd(Obama.ev) ## [1] 16.39388 ## theoretical standard deviation sqrt(V) ## [1] 16.39515 tidyverse code Simulation mean, variance, and standard deviations: sim_results %&gt;% select(EV) %&gt;% summarise_all(funs(mean, var, sd)) #&gt; # A tibble: 1 × 3 #&gt; mean var sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 270 16.4 Theoretical probabilities # we cannot use n, because mutate will look for n() first. n_draws &lt;- 1000 pres08 %&gt;% mutate(pb = pbinom(n_draws / 2, size = n_draws, prob = p, lower.tail = FALSE)) %&gt;% summarise(mean = sum(pb * EV), V = sum(pb * (1 - pb) * EV ^ 2), sd = sqrt(V)) #&gt; # A tibble: 1 × 3 #&gt; mean V sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 269 16.4 7.4 Large Sample Theorems 7.4.1 Law of Large Numbers "],
["uncertainty.html", "8 Uncertainty 8.1 Prerequisites 8.2 Estimation 8.3 Hypothesis Testing 8.4 Linear Regrssion Model with Uncertainty", " 8 Uncertainty 8.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;broom&quot;) #&gt; #&gt; Attaching package: &#39;broom&#39; #&gt; The following object is masked from &#39;package:modelr&#39;: #&gt; #&gt; bootstrap 8.2 Estimation 8.3 Hypothesis Testing 8.4 Linear Regrssion Model with Uncertainty "]
]
