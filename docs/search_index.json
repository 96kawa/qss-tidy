[
["index.html", "QSS Tidyverse Code 1 Preface 1.1 Colonphon", " QSS Tidyverse Code Jeffrey B. Arnold 2017-01-17 1 Preface This is tidyverse R code to supplement the book, Quantitative Social Science: An Introduction, by Kosuke Imai, to be published by Princeton University Press in March 2017. The R code included with the text of QSS and the supplementary materials relies mostly on base R functions. This translates the code examples provided with QSS to tidyverse R code. Tidyverse refers to a set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, and a few others) that share common data representations, especially the use of data frames for return values. The book R for Data Science by Hadley Wickham and Garrett Grolemond is an introduction. To install the R packages used in this work run the following code: # install.packages(&quot;devtools&quot;) install_github(&quot;jrnold/qss-tidy&quot;) It install’s the qsstidy package which contains no code or data, but will install the needed dependencies. I wrote this code while teaching course that employed both texts in order to make the excellent examples and statistical material in QSS more compatible with the modern data science R approach in R4DS. 1.1 Colonphon The source of the book is available here and was built with versions of packages below: #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.3.2 (2016-10-31) #&gt; system x86_64, darwin13.4.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/Los_Angeles #&gt; date 2017-01-17 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; assertthat 0.1 2013-12-06 CRAN (R 3.3.0) #&gt; backports 1.0.4 2016-10-24 CRAN (R 3.3.0) #&gt; bookdown 0.3.6 2017-01-08 Github (rstudio/bookdown@35c5684) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.3.2) #&gt; DBI 0.5-1 2016-09-10 CRAN (R 3.3.0) #&gt; devtools 1.12.0.9000 2017-01-17 Github (hadley/devtools@1ce84b0) #&gt; digest 0.6.11 2017-01-03 CRAN (R 3.3.2) #&gt; dplyr * 0.5.0 2016-06-24 CRAN (R 3.3.0) #&gt; evaluate 0.10 2016-10-11 CRAN (R 3.3.0) #&gt; ggplot2 * 2.2.1 2016-12-30 cran (@2.2.1) #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.3.0) #&gt; htmltools 0.3.5 2016-03-21 CRAN (R 3.3.0) #&gt; knitr 1.15.1 2016-11-22 CRAN (R 3.3.2) #&gt; lazyeval 0.2.0 2016-06-12 CRAN (R 3.3.0) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.3.0) #&gt; memoise 1.0.0 2016-01-29 CRAN (R 3.3.0) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.3.0) #&gt; pkgbuild 0.0.0.9000 2017-01-17 Github (r-pkgs/pkgbuild@65eace0) #&gt; pkgload 0.0.0.9000 2017-01-17 Github (r-pkgs/pkgload@def2b10) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.3.0) #&gt; purrr * 0.2.2 2016-06-18 CRAN (R 3.3.0) #&gt; R6 2.2.0 2016-10-05 CRAN (R 3.3.0) #&gt; Rcpp 0.12.9 2017-01-17 Github (RcppCore/Rcpp@f57467b) #&gt; readr * 1.0.0 2016-08-03 CRAN (R 3.3.0) #&gt; rmarkdown 1.3 2016-12-21 cran (@1.3) #&gt; rprojroot 1.2 2017-01-16 cran (@1.2) #&gt; scales 0.4.1 2016-11-09 CRAN (R 3.3.2) #&gt; stringi 1.1.2 2016-10-01 CRAN (R 3.3.0) #&gt; stringr * 1.1.0 2016-08-19 CRAN (R 3.3.1) #&gt; tibble * 1.2 2016-08-26 CRAN (R 3.3.0) #&gt; tidyr * 0.6.1 2017-01-10 cran (@0.6.1) #&gt; tidyverse * 1.0.0 2016-09-09 CRAN (R 3.3.0) #&gt; withr 1.0.2 2016-06-20 CRAN (R 3.3.0) #&gt; yaml 2.1.14 2016-11-12 CRAN (R 3.3.2) "],
["introduction.html", "2 Introduction 2.1 Prerequisites 2.2 Introduction to R", " 2 Introduction 2.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats We also load the haven package to load Stata dta files, library(&quot;haven&quot;) and the rio package to load multiple types of files, library(&quot;rio&quot;) 2.2 Introduction to R 2.2.1 Data Files Don’t use setwd() within scripts. It is much better to organize your code in projects. When reading from csv files use readr::read_csv instead of the base R function read.csv. It is slightly faster, and returns a tibble instead of a data frame. See r4ds Ch 11: Data Import for more dicussion. We also can load it directly from a URL. UNpop &lt;- read_csv(&quot;https://raw.githubusercontent.com/jrnold/qss/master/INTRO/UNpop.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; year = col_integer(), #&gt; world.pop = col_integer() #&gt; ) class(UNpop) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; UNpop #&gt; # A tibble: 7 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; # ... with 1 more rows The single bracket, [, is useful to select rows and columns in simple cases, but the dplyr functions slice() to select rows by number, filter to select by certain criteria, or select() to select columns. UNpop[c(1, 2, 3), ] #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 is equivalent to UNpop %&gt;% slice(1:3) #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 UNpop[, &quot;world.pop&quot;] #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows is almost equivalent to select(UNpop, world.pop) #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows However, note that select() always returns a tibble, and never a vector, even if only one column is selected. Also, note that since world.pop is a tibble, using [ also returns tibbles rather than a vector if it is only one column. UNpop[1:3, &quot;year&quot;] #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 is almost equivalent to UNpop %&gt;% slice(1:3) %&gt;% select(year) #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 For this example using these functions and %&gt;% to chain them together may seem a little excessive, but later we will see how chaining simple functions togther like that becomes a very powerful way to build up complicated logic. UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] #&gt; [1] 2525779 3691173 5320817 6916183 can be rewritten as UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% select(world.pop) #&gt; # A tibble: 4 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3691173 #&gt; 3 5320817 #&gt; 4 6916183 The function n() when used in a dplyr functions returns the number of rows in the data frame (or the number of rows in the group if used with group_by). 2.2.2 Saving Objects Do not save the workspace using save.image. This is an extremely bad idea for reproducibility. See r4ds Ch 8. You should uncheck the options in RStudio to avoid saving and rstoring from .RData files. This will help ensure that your R code runs the way you think it does, instead of depending on some long forgotten code that is only saved in the workspace image. Everything important should be in a script. Anything saved or loaded from file should be done explicitly. As with reading CSVs, use the readr package functions. In this case, write_csv writes a csv file write_csv(UNpop, &quot;UNpop.csv&quot;) 2.2.3 Packages Instead of foreign for reading and writing Stata and SPSS files, use haven. One reason to do so is that it is better maintained. The R function read.dta does not read files created by the most recent versions of Stata (13+). read_dta(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; # A tibble: 7 × 2 #&gt; year world_pop #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; # ... with 1 more rows There is also the equivalent write_dta function to create Stata datasets. write_dta(UNpop, &quot;UNpop.dta&quot;) Also see the rio package which makes loading data even easier with smart defaults. You can use the import function to load many types of files: import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.csv&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.RData&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; year world_pop #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; 7 2010 6916 2.2.4 Style Guide Follow Hadley Wickham’s Style Guide not the Google R style guide. In addition to lintr, the R package formatR has methods to clean up your code. "],
["causality.html", "3 Causality 3.1 Prerequistes 3.2 Racial Discrimination in the Labor Market 3.3 Subsetting Data in R 3.4 Causal Affects and the Counterfactual 3.5 Observational Studies 3.6 Descriptive Statistics for a Single Variable", " 3 Causality 3.1 Prerequistes library(&quot;tidyverse&quot;) 3.2 Racial Discrimination in the Labor Market The code in the book uses table and addmargins to construct the table. However, this can be done easily with dplyr using grouping and summarizing. resume_url &lt;- &quot;https://raw.githubusercontent.com/jrnold/qss/master/CAUSALITY/resume.csv&quot; resume &lt;- read_csv(resume_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; firstname = col_character(), #&gt; sex = col_character(), #&gt; race = col_character(), #&gt; call = col_integer() #&gt; ) In addition to the functions shown in the text, dim(resume) #&gt; [1] 4870 4 summary(resume) #&gt; firstname sex race call #&gt; Length:4870 Length:4870 Length:4870 Min. :0.00 #&gt; Class :character Class :character Class :character 1st Qu.:0.00 #&gt; Mode :character Mode :character Mode :character Median :0.00 #&gt; Mean :0.08 #&gt; 3rd Qu.:0.00 #&gt; Max. :1.00 head(resume) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Allison female white 0 #&gt; 2 Kristen female white 0 #&gt; 3 Lakisha female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Carrie female white 0 #&gt; 6 Jay male white 0 we can also use glimpse to get a quick understanding of the variables in the data frame, glimpse(resume) #&gt; Observations: 4,870 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... For each combination of race and call let’s count the observations: race_call_tab &lt;- resume %&gt;% group_by(race, call) %&gt;% count() race_call_tab #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race call n #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 0 2278 #&gt; 2 black 1 157 #&gt; 3 white 0 2200 #&gt; 4 white 1 235 If we want to calculate callback rates by race: race_call_rate &lt;- race_call_tab %&gt;% group_by(race) %&gt;% mutate(call_rate = n / sum(n)) %&gt;% filter(call == 1) %&gt;% select(race, call_rate) race_call_rate #&gt; Source: local data frame [2 x 2] #&gt; Groups: race [2] #&gt; #&gt; race call_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black 0.0645 #&gt; 2 white 0.0965 If we want the overall callback rate, we can calculate it from the original data, resume %&gt;% summarise(call_back = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_back #&gt; &lt;dbl&gt; #&gt; 1 0.0805 3.3 Subsetting Data in R 3.3.1 Subsetting The dplyr function filter is a much improved version of subset. To select black individuals in the data: resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) dim(resumeB) #&gt; [1] 2435 4 head(resumeB) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Lakisha female black 0 #&gt; 2 Latonya female black 0 #&gt; 3 Kenya female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Tyrone male black 0 #&gt; 6 Aisha female black 0 And to calculate the callback rate resumeB %&gt;% summarise(call_rate = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_rate #&gt; &lt;dbl&gt; #&gt; 1 0.0645 To keep call and firstname variables and those with black-sounding first names. resumeBf &lt;- resume %&gt;% filter(race == &quot;black&quot;, sex == &quot;female&quot;) %&gt;% select(call, firstname) head(resumeBf) #&gt; # A tibble: 6 × 2 #&gt; call firstname #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 0 Lakisha #&gt; 2 0 Latonya #&gt; 3 0 Kenya #&gt; 4 0 Latonya #&gt; 5 0 Aisha #&gt; 6 0 Aisha Now we can calculate the gender gap by group. One way to do this is to calculate the call back rates for both sexes of black sounding names, resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) %&gt;% group_by(sex) %&gt;% summarise(black_rate = mean(call)) and white-sounding names resumeW &lt;- resume %&gt;% filter(race == &quot;white&quot;) %&gt;% group_by(sex) %&gt;% summarise(white_rate = mean(call)) resumeW #&gt; # A tibble: 2 × 2 #&gt; sex white_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 0.0989 #&gt; 2 male 0.0887 Then, merge resumeB and resumeW on sex and calculate the difference for both sexes. inner_join(resumeB, resumeW, by = &quot;sex&quot;) %&gt;% mutate(race_gap = white_rate - black_rate) #&gt; # A tibble: 2 × 4 #&gt; sex black_rate white_rate race_gap #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This seems to be a little more code, but we didn’t duplicate as much as in QSS, and this would easily scale to more than two categories. A way to do this using the spread and gather functions from tidy are, First, calculate the resume_race_sex &lt;- resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) head(resume_race_sex) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [2] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 Now, use spread() to make each value of race a new column: library(&quot;tidyr&quot;) resume_sex &lt;- resume_race_sex %&gt;% ungroup() %&gt;% spread(race, call) resume_sex #&gt; # A tibble: 2 × 3 #&gt; sex black white #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 #&gt; 2 male 0.0583 0.0887 Now we can calculate the race wage differences by sex as before, resume_sex %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This could be combined into a single chain with only six lines of code: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% ungroup() %&gt;% spread(race, call) %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 3.3.2 Simple conditional statements See the dlpyr functions if_else, recode and case_when. The function if_else is like ifelse but corrects for some weird behavior that ifelse has in certain cases. resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1, 0)) %&gt;% group_by(BlackFemale, race, sex) %&gt;% count() #&gt; Source: local data frame [4 x 4] #&gt; Groups: BlackFemale, race [?] #&gt; #&gt; BlackFemale race sex n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 black male 549 #&gt; 2 0 white female 1860 #&gt; 3 0 white male 575 #&gt; 4 1 black female 1886 3.3.3 Factor Variables See R4DS Chapter 15 “Factors” and the package forcats The code in this section works, but can be simplified by using the function case_when which works in exactly these cases. resume %&gt;% mutate(type = as.factor(case_when( .$race == &quot;black&quot; &amp; .$sex == &quot;female&quot; ~ &quot;BlackFemale&quot;, .$race == &quot;black&quot; &amp; .$sex == &quot;male&quot; ~ &quot;BlackMale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;female&quot; ~ &quot;WhiteFemale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;male&quot; ~ &quot;WhiteMale&quot;, TRUE ~ as.character(NA) ))) #&gt; # A tibble: 4,870 × 5 #&gt; firstname sex race call type #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fctr&gt; #&gt; 1 Allison female white 0 WhiteFemale #&gt; 2 Kristen female white 0 WhiteFemale #&gt; 3 Lakisha female black 0 BlackFemale #&gt; 4 Latonya female black 0 BlackFemale #&gt; 5 Carrie female white 0 WhiteFemale #&gt; 6 Jay male white 0 WhiteMale #&gt; # ... with 4,864 more rows Since the logic of this is so simple, we can create this variable by using str_c to combine the vectors of sex and race, after using str_to_title to capitalize them first. library(stringr) resume &lt;- resume %&gt;% mutate(type = str_c(str_to_title(race), str_to_title(sex))) Some of the reasons given for using factors in this chapter are not as important given the functionality in modern tidyverse packages. For example, there is no reason to use tapply, as that can use group_by and summarise, resume %&gt;% group_by(type) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 × 2 #&gt; type call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 BlackFemale 0.0663 #&gt; 2 BlackMale 0.0583 #&gt; 3 WhiteFemale 0.0989 #&gt; 4 WhiteMale 0.0887 What’s nice about this approach is that we wouldn’t have needed to create the factor variable first, resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 We can use that same approach to calculate the mean of firstnames, and use arrange to sort in ascending order. resume %&gt;% group_by(firstname) %&gt;% summarise(call = mean(call)) %&gt;% arrange(call) #&gt; # A tibble: 36 × 2 #&gt; firstname call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aisha 0.0222 #&gt; 2 Rasheed 0.0299 #&gt; 3 Keisha 0.0383 #&gt; 4 Tremayne 0.0435 #&gt; 5 Kareem 0.0469 #&gt; 6 Darnell 0.0476 #&gt; # ... with 30 more rows 3.4 Causal Affects and the Counterfactual Load the data using the readr function read_csv social_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot; social &lt;- read_csv(social_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2006 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) summary(social) #&gt; sex yearofbirth primary2004 messages #&gt; Length:305866 Min. :1900 Min. :0.000 Length:305866 #&gt; Class :character 1st Qu.:1947 1st Qu.:0.000 Class :character #&gt; Mode :character Median :1956 Median :0.000 Mode :character #&gt; Mean :1956 Mean :0.401 #&gt; 3rd Qu.:1965 3rd Qu.:1.000 #&gt; Max. :1986 Max. :1.000 #&gt; primary2006 hhsize #&gt; Min. :0.000 Min. :1.00 #&gt; 1st Qu.:0.000 1st Qu.:2.00 #&gt; Median :0.000 Median :2.00 #&gt; Mean :0.312 Mean :2.18 #&gt; 3rd Qu.:1.000 3rd Qu.:2.00 #&gt; Max. :1.000 Max. :8.00 Use a grouped summarize instead of tapply, gotv_by_group &lt;- social %&gt;% group_by(messages) %&gt;% summarize(turnout = mean(primary2006)) gotv_by_group #&gt; # A tibble: 4 × 2 #&gt; messages turnout #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Get the turnout for the control group gotv_control &lt;- (filter(gotv_by_group, messages == &quot;Control&quot;))[[&quot;turnout&quot;]] Subtract the control group turnout from all groups gotv_by_group %&gt;% mutate(diff_control = turnout - gotv_control) #&gt; # A tibble: 4 × 3 #&gt; messages turnout diff_control #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.0179 #&gt; 2 Control 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.0257 #&gt; 4 Neighbors 0.378 0.0813 We could have also done this in one step like, gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 We can compare the differences of variables across the groups easily using a grouped summarize gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Pro-tip The summarise_at functions allows you summarize one-or-more columns with one-or-more functions. In addition to age, 2004 turnout, and household size, we’ll also compare proportion female, social %&gt;% group_by(messages) %&gt;% mutate(age = 2006 - yearofbirth, female = (sex == &quot;female&quot;)) %&gt;% select(-age, -sex) %&gt;% summarise_all(mean) #&gt; # A tibble: 4 × 6 #&gt; messages yearofbirth primary2004 primary2006 hhsize female #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 1956 0.399 0.315 2.19 0.500 #&gt; 2 Control 1956 0.400 0.297 2.18 0.499 #&gt; 3 Hawthorne 1956 0.403 0.322 2.18 0.499 #&gt; 4 Neighbors 1956 0.407 0.378 2.19 0.500 3.5 Observational Studies Load the minwage dataset from its URL using readr::read_csv: minwage_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/minwage.csv&quot; minwage &lt;- read_csv(minwage_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; chain = col_character(), #&gt; location = col_character(), #&gt; wageBefore = col_double(), #&gt; wageAfter = col_double(), #&gt; fullBefore = col_double(), #&gt; fullAfter = col_double(), #&gt; partBefore = col_double(), #&gt; partAfter = col_double() #&gt; ) glimpse(minwage) #&gt; Observations: 358 #&gt; Variables: 8 #&gt; $ chain &lt;chr&gt; &quot;wendys&quot;, &quot;wendys&quot;, &quot;burgerking&quot;, &quot;burgerking&quot;, &quot;kf... #&gt; $ location &lt;chr&gt; &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA... #&gt; $ wageBefore &lt;dbl&gt; 5.00, 5.50, 5.00, 5.00, 5.25, 5.00, 5.00, 5.00, 5.0... #&gt; $ wageAfter &lt;dbl&gt; 5.25, 4.75, 4.75, 5.00, 5.00, 5.00, 4.75, 5.00, 4.5... #&gt; $ fullBefore &lt;dbl&gt; 20.0, 6.0, 50.0, 10.0, 2.0, 2.0, 2.5, 40.0, 8.0, 10... #&gt; $ fullAfter &lt;dbl&gt; 0.0, 28.0, 15.0, 26.0, 3.0, 2.0, 1.0, 9.0, 7.0, 18.... #&gt; $ partBefore &lt;dbl&gt; 20.0, 26.0, 35.0, 17.0, 8.0, 10.0, 20.0, 30.0, 27.0... #&gt; $ partAfter &lt;dbl&gt; 36, 3, 18, 9, 12, 9, 25, 32, 39, 10, 20, 4, 13, 20,... summary(minwage) #&gt; chain location wageBefore wageAfter #&gt; Length:358 Length:358 Min. :4.25 Min. :4.25 #&gt; Class :character Class :character 1st Qu.:4.25 1st Qu.:5.05 #&gt; Mode :character Mode :character Median :4.50 Median :5.05 #&gt; Mean :4.62 Mean :4.99 #&gt; 3rd Qu.:4.99 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :6.25 #&gt; fullBefore fullAfter partBefore partAfter #&gt; Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 2.1 1st Qu.: 2.0 1st Qu.:11.0 1st Qu.:11.0 #&gt; Median : 6.0 Median : 6.0 Median :16.2 Median :17.0 #&gt; Mean : 8.5 Mean : 8.4 Mean :18.8 Mean :18.7 #&gt; 3rd Qu.:12.0 3rd Qu.:12.0 3rd Qu.:25.0 3rd Qu.:25.0 #&gt; Max. :60.0 Max. :40.0 Max. :60.0 Max. :60.0 First, calculate the proportion of restaurants by state whose hourly wages were less than the minimum wage in NJ, $5.05, for wageBefore and wageAfter: Since the NJ minimum wage was $5.05, we’ll define a variable with that value. Even if you use them only once or twice, it is a good idea to put values like this in variables. It makes your code closer to self-documenting.n NJ_MINWAGE &lt;- 5.05 Later, it will be easier to understand wageAfter &lt; NJ_MINWAGE without any comments than it would be to understand wageAfter &lt; 5.05. In the latter case you’d have to remember that the new NJ minimum wage was 5.05 and that’s why you were using that value. This is an example of a magic number: try to avoid them. Note that location has multiple values: PA and four regions of NJ. So we’ll add a state variable to the data. minwage %&gt;% count(location) #&gt; # A tibble: 5 × 2 #&gt; location n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 centralNJ 45 #&gt; 2 northNJ 146 #&gt; 3 PA 67 #&gt; 4 shoreNJ 33 #&gt; 5 southNJ 67 We can extract the state from the final two characters of the location variable using the stringr function str_sub (R4DS Ch 14: Strings): library(stringr) minwage &lt;- mutate(minwage, state = str_sub(location, -2L)) Alternatively, since everything is either PA or NJ minwage &lt;- mutate(minwage, state = if_else(location == &quot;PA&quot;, &quot;PA&quot;, &quot;NJ&quot;)) Let’s confirm that the restaurants followed the law: minwage %&gt;% group_by(state) %&gt;% summarise(prop_after = mean(wageAfter &lt; NJ_MINWAGE), prop_Before = mean(wageBefore &lt; NJ_MINWAGE)) #&gt; # A tibble: 2 × 3 #&gt; state prop_after prop_Before #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.00344 0.911 #&gt; 2 PA 0.95522 0.940 Create a variable for the proportion of full-time employees in NJ and PA minwage &lt;- minwage %&gt;% mutate(totalAfter = fullAfter + partAfter, fullPropAfter = fullAfter / totalAfter) Now calculate the average for each state: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state #&gt; # A tibble: 2 × 2 #&gt; state fullPropAfter #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ 0.320 #&gt; 2 PA 0.272 We could compute the difference by (filter(full_prop_by_state, state == &quot;NJ&quot;)[[&quot;fullPropAfter&quot;]] - filter(full_prop_by_state, state == &quot;PA&quot;)[[&quot;fullPropAfter&quot;]]) #&gt; [1] 0.0481 or using tidyr functions spread (R4DS Ch 11: Tidy Data): spread(full_prop_by_state, state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 1 × 3 #&gt; NJ PA diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.32 0.272 0.0481 3.5.1 Confounding Bias We can calculate the proportion of fast-food restaurants in each chain in each state: chains_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% count(chain) %&gt;% mutate(prop = n / sum(n)) We can easily compare these using a simple dot-plot: ggplot(chains_by_state, aes(x = chain, y = prop, colour = state)) + geom_point() + coord_flip() In the QSS text, only Burger King restaurants are compared. However, dplyr makes this easy. All we have to do is change the group_by statement we used last time, and add chain to it: full_prop_by_state_chain &lt;- minwage %&gt;% group_by(state, chain) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state_chain #&gt; Source: local data frame [8 x 3] #&gt; Groups: state [?] #&gt; #&gt; state chain fullPropAfter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ burgerking 0.358 #&gt; 2 NJ kfc 0.328 #&gt; 3 NJ roys 0.283 #&gt; 4 NJ wendys 0.260 #&gt; 5 PA burgerking 0.321 #&gt; 6 PA kfc 0.236 #&gt; # ... with 2 more rows We can plot and compare the proportions easily in this format. In general, ordering categorical variables alphabetically is useless, so we’ll order the chains by the average of the NJ and PA fullPropAfter, using forcats::fct_reorder: ggplot(full_prop_by_state_chain, aes(x = forcats::fct_reorder(chain, fullPropAfter), y = fullPropAfter, colour = state)) + geom_point() + coord_flip() + labs(x = &quot;chains&quot;) To calculate the differences, we need to get the data frame The join method. Create New Jersey and Pennsylvania datasets with chain and prop full employed columns. Merge the two datasets on chain. chains_nj &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(-state) %&gt;% rename(NJ = fullPropAfter) chains_pa &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;PA&quot;) %&gt;% select(-state) %&gt;% rename(PA = fullPropAfter) full_prop_state_chain_diff &lt;- full_join(chains_nj, chains_pa, by = &quot;chain&quot;) %&gt;% mutate(diff = NJ - PA) full_prop_state_chain_diff #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 Q: In the code above why did I remove the state variable and rename the fullPropAfter variable before merging? What happens if I didn’t? The spread/gather method. We can also use the spread and gather functions from tidyr. In this example it is much more compact code. full_prop_by_state_chain %&gt;% spread(state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 3.5.2 Before and After and Difference-in-Difference Designs To compute the estimates in the before and after design first create a variable for the difference before and after the law passed. minwage &lt;- minwage %&gt;% mutate(totalBefore = fullBefore + partBefore, fullPropBefore = fullBefore / totalBefore) The before-and-after analysis is the difference between the full-time employment before and after the minimum wage law passed looking only at NJ: filter(minwage, state == &quot;NJ&quot;) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) #&gt; # A tibble: 1 × 1 #&gt; diff #&gt; &lt;dbl&gt; #&gt; 1 0.0239 The difference-in-differences design uses the difference in the before-and-after differences for each state. diff_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) filter(diff_by_state, state == &quot;NJ&quot;)[[&quot;diff&quot;]] - filter(diff_by_state, state == &quot;PA&quot;)[[&quot;diff&quot;]] #&gt; [1] 0.0616 Let’s create a single dataset with the mean values of each state before and after to visually look at each of these designs: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(fullPropAfter, fullPropBefore), mean) %&gt;% gather(period, fullProp, -state) %&gt;% mutate(period = recode(period, fullPropAfter = 1, fullPropBefore = 0)) full_prop_by_state #&gt; # A tibble: 4 × 3 #&gt; state period fullProp #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 1 0.320 #&gt; 2 PA 1 0.272 #&gt; 3 NJ 0 0.297 #&gt; 4 PA 0 0.310 ggplot(full_prop_by_state, aes(x = period, y = fullProp, colour = state)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(0, 1), labels = c(&quot;Before&quot;, &quot;After&quot;)) 3.6 Descriptive Statistics for a Single Variable To calculate the summary for the variables wageBefore and wageAfter: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(wageBefore, wageAfter) %&gt;% summary() #&gt; wageBefore wageAfter #&gt; Min. :4.25 Min. :5.00 #&gt; 1st Qu.:4.25 1st Qu.:5.05 #&gt; Median :4.50 Median :5.05 #&gt; Mean :4.61 Mean :5.08 #&gt; 3rd Qu.:4.87 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :5.75 We calculate the IQR for each state’s wages after the passage of the law using the same grouped summarise as we used before: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter = IQR(wageAfter), wageBefore = IQR(wageBefore)) #&gt; # A tibble: 2 × 3 #&gt; state wageAfter wageBefore #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.000 0.62 #&gt; 2 PA 0.575 0.75 Calculate the variance and standard deviation of wageAfter and wageBefore for each state: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter_sd = sd(wageAfter), wageAfter_var = var(wageAfter), wageBefore_sd = sd(wageBefore), wageBefore_var = var(wageBefore)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageAfter_var wageBefore_sd wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.0112 0.343 0.118 #&gt; 2 PA 0.359 0.1291 0.358 0.128 or, more compactly, using summarise_at: minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(wageAfter, wageBefore), funs(sd, var)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageBefore_sd wageAfter_var wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.343 0.0112 0.118 #&gt; 2 PA 0.359 0.358 0.1291 0.128 "],
["measurement.html", "4 Measurement 4.1 Prerequisites 4.2 Measuring Civilian Victimization during Wartime 4.3 Visualizing the Univariate Distribution 4.4 Survey Sampling 4.5 load village data 4.6 Measuring Political Polarization 4.7 Clustering", " 4 Measurement 4.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats library(&quot;forcats&quot;) library(&quot;broom&quot;) 4.2 Measuring Civilian Victimization during Wartime afghan_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan.csv&quot; afghan &lt;- read_csv(afghan_url) Summarize the variables of interest afghan %&gt;% select(age, educ.years, employed, income) %&gt;% summary() #&gt; age educ.years employed income #&gt; Min. :15.0 Min. : 0 Min. :0.000 Length:2754 #&gt; 1st Qu.:22.0 1st Qu.: 0 1st Qu.:0.000 Class :character #&gt; Median :30.0 Median : 1 Median :1.000 Mode :character #&gt; Mean :32.4 Mean : 4 Mean :0.583 #&gt; 3rd Qu.:40.0 3rd Qu.: 8 3rd Qu.:1.000 #&gt; Max. :80.0 Max. :18 Max. :1.000 With income, read_csv never converts strings to factors by default. To get a summary of the different levels, either convert it to a factor (R4DS Ch 15), or use count() afghan %&gt;% count(income) #&gt; # A tibble: 6 × 2 #&gt; income n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 10,001-20,000 616 #&gt; 2 2,001-10,000 1420 #&gt; 3 20,001-30,000 93 #&gt; 4 less than 2,000 457 #&gt; 5 over 30,000 14 #&gt; 6 &lt;NA&gt; 154 Count the number a proportion of respondents who answer that they were harmed by the ISF (violent.exp.ISAF) and (violent.exp.taliban) respectively, afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 9 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.48293 #&gt; 2 0 1 354 0.12854 #&gt; 3 0 NA 22 0.00799 #&gt; 4 1 0 475 0.17248 #&gt; 5 1 1 526 0.19099 #&gt; 6 1 NA 22 0.00799 #&gt; # ... with 3 more rows We need to use ungroup() in order to ensure that sum(n) sums over the entire dataset as opposed to only within categories of violent.exp.ISAF. Unlike prop.table, the code above does not drop missing values. We can drop those values by adding a filter verb and using !is.na() to test for missing values in those variables: afghan %&gt;% filter(!is.na(violent.exp.ISAF), !is.na(violent.exp.taliban)) %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 4 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.495 #&gt; 2 0 1 354 0.132 #&gt; 3 1 0 475 0.177 #&gt; 4 1 1 526 0.196 4.2.1 Handling Missing Data in R We already observed the issues with NA values in calculating the proportion answering the “experienced violence” questions. You can filter rows with specific variables having missing values using filter as shown above. Howeer, na.omit works with tibbles just like any other data frame. na.omit(afghan) #&gt; # A tibble: 2,554 × 11 #&gt; province district village.id age educ.years employed income #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Logar Baraki Barak 80 26 10 0 2,001-10,000 #&gt; 2 Logar Baraki Barak 80 49 3 1 2,001-10,000 #&gt; 3 Logar Baraki Barak 80 60 0 1 2,001-10,000 #&gt; 4 Logar Baraki Barak 80 34 14 1 2,001-10,000 #&gt; 5 Logar Baraki Barak 80 21 12 1 2,001-10,000 #&gt; 6 Logar Baraki Barak 80 42 6 1 10,001-20,000 #&gt; # ... with 2,548 more rows, and 4 more variables: violent.exp.ISAF &lt;int&gt;, #&gt; # violent.exp.taliban &lt;int&gt;, list.group &lt;chr&gt;, list.response &lt;int&gt; 4.3 Visualizing the Univariate Distribution 4.3.1 Barplot library(forcats) afghan &lt;- afghan %&gt;% mutate(violent.exp.ISAF.fct = fct_explicit_na(fct_recode(factor(violent.exp.ISAF), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the ISAF&quot;) afghan &lt;- afghan %&gt;% mutate(violent.exp.taliban.fct = fct_explicit_na(fct_recode(factor(violent.exp.taliban), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the Taliban&quot;) TODO This plot could improved by plotting the two values simultaneously to be able to better compare them. dodged bar plot dot-plot This will require creating a data frame that has the following columns: perpetrator (ISAF, Taliban), response (No Harm, Harm, No response). See the section on Tidy Data and spread gather. TODO Compare them by region, ? 4.3.2 Boxplot ggplot(afghan, aes(x = 1, y = age)) + geom_boxplot() + labs(y = &quot;Age&quot;, x = &quot;&quot;) + ggtitle(&quot;Distribution of Age&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_boxplot() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;) + ggtitle(&quot;Education by Provice&quot;) Helmand and Uruzgan have much lower levels of education than the other provicnces, and also report higher levels of violence. afghan %&gt;% group_by(province) %&gt;% summarise(educ.years = mean(educ.years, na.rm = TRUE), violent.exp.taliban = mean(violent.exp.taliban, na.rm = TRUE), violent.exp.ISAF = mean(violent.exp.ISAF, na.rm = TRUE)) %&gt;% arrange(educ.years) #&gt; # A tibble: 5 × 4 #&gt; province educ.years violent.exp.taliban violent.exp.ISAF #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 1.04 0.4545 0.496 #&gt; 2 Helmand 1.60 0.5042 0.541 #&gt; 3 Khost 5.79 0.2332 0.242 #&gt; 4 Kunar 5.93 0.3030 0.399 #&gt; 5 Logar 6.70 0.0802 0.144 4.3.3 Printing and saving graphics Use the function ggsave() to save ggplot graphics. Also, RMarkdown files have their own means of creating and saving plots created by code-chunks. 4.4 Survey Sampling 4.4.1 The Role of Randomization 4.5 load village data afghan_village_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan-village.csv&quot; afghan.village &lt;- read_csv(afghan_village_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; altitude = col_double(), #&gt; population = col_integer(), #&gt; village.surveyed = col_integer() #&gt; ) Box-plots of altitude ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = altitude)) + geom_boxplot() + labs(y = &quot;Altitude (meter)&quot;, x = &quot;&quot;) Boxplots log-population values of sampled and non-sampled ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = log(population))) + geom_boxplot() + labs(y = &quot;log(population)&quot;, x = &quot;&quot;) You can also compare these distributions by plotting their densities: ggplot(afghan.village, aes(colour = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), x = log(population))) + geom_density() + geom_rug() + labs(x = &quot;log(population)&quot;, colour = &quot;&quot;) 4.5.1 Non-response and other sources of bias Calculate the rates of non-response by province to violent.exp.ISAF and violent.exp.taliban: afghan %&gt;% group_by(province) %&gt;% summarise(ISAF = mean(is.na(violent.exp.ISAF)), taliban = mean(is.na(violent.exp.taliban))) %&gt;% arrange(-ISAF) #&gt; # A tibble: 5 × 3 #&gt; province ISAF taliban #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 0.02067 0.06202 #&gt; 2 Helmand 0.01637 0.03041 #&gt; 3 Khost 0.00476 0.00635 #&gt; 4 Kunar 0.00000 0.00000 #&gt; 5 Logar 0.00000 0.00000 Calculat the proportion who support the ISAF using the difference in means between the ISAF and control groups: (mean(filter(afghan, list.group == &quot;ISAF&quot;)$list.response) - mean(filter(afghan, list.group == &quot;control&quot;)$list.response)) #&gt; [1] 0.049 To calculate the table responses to the list expriment in the control, ISAF, and taliban groups&gt; afghan %&gt;% group_by(list.response, list.group) %&gt;% count() %T&gt;% glimpse() %&gt;% spread(list.group, n, fill = 0) #&gt; Observations: 12 #&gt; Variables: 3 #&gt; $ list.response &lt;int&gt; 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4 #&gt; $ list.group &lt;chr&gt; &quot;control&quot;, &quot;ISAF&quot;, &quot;control&quot;, &quot;ISAF&quot;, &quot;taliban&quot;,... #&gt; $ n &lt;int&gt; 188, 174, 265, 278, 433, 265, 260, 287, 200, 182... #&gt; Source: local data frame [5 x 4] #&gt; Groups: list.response [5] #&gt; #&gt; list.response control ISAF taliban #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 188 174 0 #&gt; 2 1 265 278 433 #&gt; 3 2 265 260 287 #&gt; 4 3 200 182 198 #&gt; 5 4 0 24 0 4.6 Measuring Political Polarization congress_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/congress.csv&quot; congress &lt;- read_csv(congress_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; congress = col_integer(), #&gt; district = col_integer(), #&gt; state = col_character(), #&gt; party = col_character(), #&gt; name = col_character(), #&gt; dwnom1 = col_double(), #&gt; dwnom2 = col_double() #&gt; ) glimpse(congress) #&gt; Observations: 14,552 #&gt; Variables: 7 #&gt; $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8... #&gt; $ district &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 98, 98, 1, 2, 3, 4, 5, ... #&gt; $ state &lt;chr&gt; &quot;USA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;A... #&gt; $ party &lt;chr&gt; &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Demo... #&gt; $ name &lt;chr&gt; &quot;TRUMAN&quot;, &quot;BOYKIN F.&quot;, &quot;GRANT G.&quot;, &quot;ANDREWS G.&quot;, &quot;... #&gt; $ dwnom1 &lt;dbl&gt; -0.276, -0.026, -0.042, -0.008, -0.082, -0.170, -0.12... #&gt; $ dwnom2 &lt;dbl&gt; 0.016, 0.796, 0.999, 1.005, 1.066, 0.870, 0.990, 0.89... To create the scatterplot in 3.6, we can congress %&gt;% filter(congress %in% c(80, 112), party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) congress %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) #&gt; Warning: Removed 2 rows containing missing values (geom_point). congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = congress, y = dwnom1, colour = fct_reorder2(party, congress, dwnom1))) + geom_line() + labs(y = &quot;DW-NOMINATE score (1st Dimension)&quot;, x = &quot;Congress&quot;, colour = &quot;Party&quot;) 4.6.1 Correlation Let’s plot the Gini coefficient usgini_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/USGini.csv&quot; USGini &lt;- read_csv(usgini_url) #&gt; Warning: Missing column names filled in: &#39;X1&#39; [1] #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_integer(), #&gt; year = col_integer(), #&gt; gini = col_double() #&gt; ) ggplot(USGini, aes(x = year, y = gini)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Gini coefficient&quot;) + ggtitle(&quot;Income Inequality&quot;) To calculate a measure of party polarization take the code used in the plot of Republican and Democratic party median ideal points and adapt it to calculate the difference in the party medians: party_polarization &lt;- congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% spread(party, dwnom1) %&gt;% mutate(polarization = Republican - Democrat) party_polarization #&gt; Source: local data frame [33 x 4] #&gt; Groups: congress [33] #&gt; #&gt; congress Democrat Republican polarization #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 80 -0.146 0.276 0.421 #&gt; 2 81 -0.195 0.264 0.459 #&gt; 3 82 -0.180 0.265 0.445 #&gt; 4 83 -0.181 0.261 0.442 #&gt; 5 84 -0.209 0.261 0.471 #&gt; 6 85 -0.214 0.250 0.464 #&gt; # ... with 27 more rows ggplot(party_polarization, aes(x = congress, y = polarization)) + geom_point() + ggtitle(&quot;Political Polarization&quot;) + labs(x = &quot;Year&quot;, y = &quot;Republican median − Democratic median&quot;) 4.6.2 Quantile-Quantile Plot To create histogram plots similar congress %&gt;% filter(congress == 112, party %in% c(&quot;Republican&quot;, &quot;Democrat&quot;)) %&gt;% ggplot(aes(x = dwnom2, y = ..density..)) + geom_histogram() + facet_grid(. ~ party) + labs(x = &quot;racial liberalism/conservatism dimension&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot2 includes a stat_qq which can be used to create qq-plots but it is more suited to comparing a sample distribution with a theoretical distibution, usually the normal one. However, we can calculate one by hand, which may give more insight into exactly what the qq-plot is doing. party_qtiles &lt;- tibble( probs = seq(0, 1, by = 0.01), Democrat = quantile(filter(congress, congress == 112, party == &quot;Democrat&quot;)$dwnom2, probs = probs), Republican = quantile(filter(congress, congress == 112, party == &quot;Republican&quot;)$dwnom2, probs = probs) ) party_qtiles #&gt; # A tibble: 101 × 3 #&gt; probs Democrat Republican #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00 -0.925 -1.381 #&gt; 2 0.01 -0.672 -0.720 #&gt; 3 0.02 -0.619 -0.566 #&gt; 4 0.03 -0.593 -0.526 #&gt; 5 0.04 -0.567 -0.468 #&gt; 6 0.05 -0.560 -0.436 #&gt; # ... with 95 more rows The plot looks different than the one in the text since the x- and y-scales are in the original values instead of z-scores (see the next section). party_qtiles %&gt;% ggplot(aes(x = Democrat, y = Republican)) + geom_point() + geom_abline() + coord_fixed() 4.7 Clustering 4.7.1 Matrices While matrices are great for numerical computations, such as when you are implementing algorithms, generally keeping data in data frames is more convenient for data wrangling. 4.7.2 Lists See R4DS Chapter 20: Vectors, Chapter 21: Iteration and the purrr package for more powerful methods of computing on lists. 4.7.3 k-means algorithms TODO A good visualization of the k-means algorithm and a simple, naive implementation in R. Calculate the clusters by the 80th and 112th congresses, k80two.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 2, nstart = 5) Add the cluster ids to datasets congress80 &lt;- congress %&gt;% filter(congress == 80) %&gt;% mutate(cluster2 = factor(k80two.out$cluster)) We will also create a data sets with the cluster centroids. These are in the centers element of the cluster object. k80two.out$centers #&gt; dwnom1 dwnom2 #&gt; 1 -0.0484 0.783 #&gt; 2 0.1468 -0.339 To make it easier to use with ggplot, we need to convert this to a data frame. The tidy function from the broom package: k80two.clusters &lt;- tidy(k80two.out) k80two.clusters #&gt; x1 x2 size withinss cluster #&gt; 1 -0.0484 0.783 135 10.9 1 #&gt; 2 0.1468 -0.339 311 54.9 2 Plot the ideal points and clusters ggplot() + geom_point(data = congress80, aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80two.clusters, mapping = aes(x = x1, y = x2)) We can also plot, congress80 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [5 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 132 #&gt; 2 Democrat 2 62 #&gt; 3 Other 2 2 #&gt; 4 Republican 1 3 #&gt; 5 Republican 2 247 And now we can repeat these steps for the 112th congress: k112two.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 2, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112two.out$cluster)) k112two.clusters &lt;- tidy(k112two.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112two.clusters, mapping = aes(x = x1, y = x2)) congress112 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [3 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 2 200 #&gt; 2 Republican 1 242 #&gt; 3 Republican 2 1 Now repeat the same with four clusters on the 80th congress: k80four.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 4, nstart = 5) congress80 &lt;- filter(congress, congress == 80) %&gt;% mutate(cluster2 = factor(k80four.out$cluster)) k80four.clusters &lt;- tidy(k80four.out) ggplot() + geom_point(data = congress80, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80four.clusters, mapping = aes(x = x1, y = x2), size = 3) and on the 112th congress: k112four.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 4, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112four.out$cluster)) k112four.clusters &lt;- tidy(k112four.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112four.clusters, mapping = aes(x = x1, y = x2), size = 3) "],
["prediction.html", "5 Prediction 5.1 Prerequisites 5.2 Loops in R 5.3 General Conditional Statements in R 5.4 Poll Predictions 5.5 Linear Regression 5.6 Regression and Causation 5.7 Regression Discontinuity Design", " 5 Prediction 5.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) The packages modelr and broom are used to wrangle the results of linear regressions, library(&quot;broom&quot;) library(&quot;modelr&quot;) 5.2 Loops in R For more on looping and iteration in R see the R for Data Science chapter Iteration. RStudio provides many features to help debugging, which will be useful in for loops and function: see this article for an example. 5.3 General Conditional Statements in R TODO Is this in R4DS, find good cites. If you are using conditional statements to assign values for data frame, see the dplyr functions if_else, recode, and case_when 5.4 Poll Predictions Load the election polls and election results for 2008, polls08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;polls08.csv&quot;)) glimpse(polls08) #&gt; Observations: 1,332 #&gt; Variables: 5 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;,... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;SurveyUSA-2&quot;, &quot;Ca... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34, 37, 36, 36, 3... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47, 55, 51, 49, 5... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 2008-10-06, 2008... pres08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;pres08.csv&quot;)) glimpse(pres08) #&gt; Observations: 51 #&gt; Variables: 5 #&gt; $ state.name &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo... #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE... #&gt; $ Obama &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36,... #&gt; $ McCain &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, ... #&gt; $ EV &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, ... Compute Obama’s margin in polls and final election polls08 &lt;- polls08 %&gt;% mutate(margin = Obama - McCain) pres08 &lt;- pres08 %&gt;% mutate(margin = Obama - McCain) To work with dates, the R package lubridate makes wrangling them much easier. See the R for Data Science chapter Dates and Times. The function ymd will convert character strings like year-month-day and more into dates, as long as the order is (year, month, day). See dmy, ymd, and others for other ways to convert strings to dates. x &lt;- ymd(&quot;2008-11-04&quot;) y &lt;- ymd(&quot;2008/9/1&quot;) x - y #&gt; Time difference of 64 days However, note that in polls08, the date middate is already a date object, class(polls08$middate) #&gt; [1] &quot;Date&quot; The function read_csv by default will check character vectors to see if they have patterns that appear to be dates, and if so, will parse those columns as dates. We’ll create a variable for election day ELECTION_DAY &lt;- ymd(&quot;2008-11-04&quot;) and add a new column to poll08 with the days to the election polls08 &lt;- mutate(polls08, ELECTION_DAY - middate) Although the code in the chapter uses a for loop, there is no reason to do so. We can accomplish the same task by merging the election results data to the polling data by state. polls_w_results &lt;- left_join(polls08, select(pres08, state, elec_margin = margin), by = &quot;state&quot;) %&gt;% mutate(error = elec_margin - margin) glimpse(polls_w_results) #&gt; Observations: 1,332 #&gt; Variables: 9 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;Sur... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34,... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47,... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 20... #&gt; $ margin &lt;int&gt; -25, -20, -27, -20, -21, -30, -22, -27,... #&gt; $ ELECTION_DAY - middate &lt;time&gt; 8 days, 20 days, 27 days, 29 days, 43 ... #&gt; $ elec_margin &lt;int&gt; -21, -21, -21, -21, -21, -21, -21, -21,... #&gt; $ error &lt;int&gt; 4, -1, 6, -1, 0, 9, 1, 6, -1, -8, -3, -... To get the last poll in each state, arrange and filter on middate last_polls &lt;- polls_w_results %&gt;% arrange(state, desc(middate)) %&gt;% group_by(state) %&gt;% slice(1) last_polls #&gt; Source: local data frame [51 x 9] #&gt; Groups: state [51] #&gt; #&gt; state Pollster Obama McCain middate margin #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 AK Research 2000-3 39 58 2008-10-29 -19 #&gt; 2 AL SurveyUSA-2 36 61 2008-10-27 -25 #&gt; 3 AR ARG-4 44 51 2008-10-29 -7 #&gt; 4 AZ ARG-3 46 50 2008-10-29 -4 #&gt; 5 CA SurveyUSA-3 60 36 2008-10-30 24 #&gt; 6 CO ARG-3 52 45 2008-10-29 7 #&gt; # ... with 45 more rows, and 3 more variables: `ELECTION_DAY - #&gt; # middate` &lt;time&gt;, elec_margin &lt;int&gt;, error &lt;int&gt; Challenge: Instead of using the last poll, use the average of polls in the last week? Last month? How do the margins on the polls change over the election period? To simplify things for later, let’s define a function rmse which calculates the root mean squared error, as defined in the book. See the R for Data Science chapter Functions for more on writing functions. rmse &lt;- function(actual, pred) { sqrt(mean((actual - pred) ^ 2)) } Now we can use rmse() to calculate the RMSE for all the final polls: rmse(last_polls$margin, last_polls$elec_margin) #&gt; [1] 5.88 Or since we already have a variable error, sqrt(mean(last_polls$error ^ 2)) #&gt; [1] 5.88 The mean prediction error is mean(last_polls$error) #&gt; [1] 1.08 This is slightly different than what is in the book due to the difference in the poll used as the final poll; many states have many polls on the last day. I’ll choose binwidths of 1%, since that is fairly interpretable: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 1, boundary = 0) The text uses bindwidths of 5%: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 5, boundary = 0) Challenge: What other ways could you visualize the results? How would you show all states? What about plotting the absolute or squared errors instead of the errors? Challenge: What happens to prediction error if you average polls? Consider averaging back over time? What happens if you take the averages of the state poll average and average of all polls - does that improve prediction? To create a scatterplot using the state abbreviations instead of points use geom_text instead of geom_point. ggplot(last_polls, aes(x = margin, y = elec_margin, label = state)) + geom_text() + geom_abline(col = &quot;red&quot;) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + coord_fixed() + labs(x = &quot;Poll Results&quot;, y = &quot;Actual Election Results&quot;) We can create a confusion matrix as follow. Create a new columns classification which shows whether how the poll’s classification was related to the actual election outcome (“true positive”, “false positive”, “false negative”, “false positive”). This can be accomplished easily with the dplyr funtion case_when. Note: You need to use . to refer to the data frame when using case_when with a mutate function. last_polls &lt;- last_polls %&gt;% ungroup() %&gt;% mutate(classification = case_when( (.$margin &gt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;true positive&quot;, (.$margin &gt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;false positive&quot;, (.$margin &lt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;true negative&quot;, (.$margin &lt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;false negative&quot; )) No simply count the last_polls %&gt;% group_by(classification) %&gt;% count() #&gt; # A tibble: 4 × 2 #&gt; classification n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 false negative 2 #&gt; 2 false positive 1 #&gt; 3 true negative 21 #&gt; 4 true positive 27 Which states were incorrectly predicted? last_polls %&gt;% filter(classification %in% c(&quot;false positive&quot;, &quot;false negative&quot;)) %&gt;% select(state, margin, elec_margin, classification) %&gt;% arrange(desc(elec_margin)) #&gt; # A tibble: 3 × 4 #&gt; state margin elec_margin classification #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 IN -5 1 false negative #&gt; 2 NC -1 1 false negative #&gt; 3 MO 1 -1 false positive What was the difference in the poll prediction of electoral votes and actual electoral votes. We hadn’t included the variable EV when we first merged, but that’s no problem, we’ll just merge again in order to grab that variable: last_polls %&gt;% left_join(select(pres08, state, EV), by = &quot;state&quot;) %&gt;% summarise(EV_pred = sum((margin &gt; 0) * EV), EV_actual = sum((elec_margin &gt; 0) * EV)) #&gt; # A tibble: 1 × 2 #&gt; EV_pred EV_actual #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 349 364 To look at predictions of the # load the data pollsUS08 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pollsUS08.csv&quot;)) %&gt;% mutate(DaysToElection = ELECTION_DAY - middate) We’ll produce the seven-day averages slightly differently than the method used in the text. For all dates in the data, we’ll calculate the moving average. all_dates &lt;- seq(min(polls08$middate), ELECTION_DAY, by = &quot;days&quot;) # Number of poll days to use POLL_DAYS &lt;- 7 pop_vote_avg &lt;- vector(length(all_dates), mode = &quot;list&quot;) for (i in seq_along(all_dates)) { date &lt;- all_dates[i] # summarise the seven day week_data &lt;- pollsUS08 %&gt;% filter(as.integer(middate - date) &lt;= 0, as.integer(middate - date) &gt; -POLL_DAYS) %&gt;% summarise(Obama = mean(Obama, na.rm = TRUE), McCain = mean(McCain, na.rm = TRUE)) # add date for the observation week_data$date &lt;- date pop_vote_avg[[i]] &lt;- week_data } pop_vote_avg &lt;- bind_rows(pop_vote_avg) It is easier to plot this if the data are tidy, with Obama and McCain as categories of a column candidate. pop_vote_avg_tidy &lt;- pop_vote_avg %&gt;% gather(candidate, share, -date, na.rm = TRUE) pop_vote_avg_tidy #&gt; # A tibble: 598 × 3 #&gt; date candidate share #&gt; * &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2008-01-09 Obama 49 #&gt; 2 2008-01-10 Obama 46 #&gt; 3 2008-01-11 Obama 45 #&gt; 4 2008-01-12 Obama 45 #&gt; 5 2008-01-13 Obama 45 #&gt; 6 2008-01-14 Obama 45 #&gt; # ... with 592 more rows ggplot(pop_vote_avg_tidy, aes(x = date, y = share, colour = forcats::fct_reorder2(candidate, date, share))) + geom_point() + geom_line() Challenge read R for Data Science chapter Iteration and use the function map_df instead of a for loop. The 7-day average is similar to the simple method used by Real Clear Politics. The RCP average is simply the average of all polls in their data for the last seven days. Sites like 538 and the Huffington Post on the other hand, also use what amounts to averaging polls, but using more sophisticated statistical methods to assign different weights to different polls. Challenge Why do we need to use different polls for the popular vote data? Why not simply average all the state polls? What would you have to do? Would the overall popular vote be useful in predicting state-level polling, or vice-versa? How would you use them? 5.5 Linear Regression 5.5.1 Facial Appearance and Election Outcomes Load the face dataset: face &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;face.csv&quot;)) Add Democrat and Republican vote shares, and the difference in shares: face &lt;- face %&gt;% mutate(d.share = d.votes / (d.votes + r.votes), r.share = r.votes / (d.votes + r.votes), diff.share = d.share - r.share) Plot facial competence vs. vote share: ggplot(face, aes(x = d.comp, y = diff.share, colour = w.party)) + geom_point() + labs(x = &quot;Competence scores for Democrats&quot;, y = &quot;Democratic margin in vote share&quot;) 5.5.2 Correlation 5.5.3 Least Squares Run the linear regression fit &lt;- lm(diff.share ~ d.comp, data = face) summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = diff.share ~ d.comp, data = face) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.675 -0.166 0.014 0.177 0.743 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.312 0.066 -4.73 6.2e-06 *** #&gt; d.comp 0.660 0.127 5.19 8.9e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.266 on 117 degrees of freedom #&gt; Multiple R-squared: 0.187, Adjusted R-squared: 0.18 #&gt; F-statistic: 27 on 1 and 117 DF, p-value: 8.85e-07 There are many functions to get data out of the lm model. In addition to these, the broom package provides three functions: glance, tidy, and augment that always return data frames. The function glance returns a one-row data-frame summary of the model, glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.187 0.18 0.266 27 8.85e-07 2 -10.5 27 35.3 #&gt; deviance df.residual #&gt; 1 8.31 117 The function tidy returns a data frame in which each row is a coefficient, tidy(fit) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) -0.312 0.066 -4.73 6.24e-06 #&gt; 2 d.comp 0.660 0.127 5.19 8.85e-07 The function augment returns the original data with fitted values, residuals, and other observation level stats from the model appended to it. augment(fit) %&gt;% head() #&gt; diff.share d.comp .fitted .se.fit .resid .hat .sigma .cooksd #&gt; 1 0.2101 0.565 0.0606 0.0266 0.1495 0.00996 0.267 0.001600 #&gt; 2 0.1194 0.342 -0.0864 0.0302 0.2059 0.01286 0.267 0.003938 #&gt; 3 0.0499 0.612 0.0922 0.0295 -0.0423 0.01229 0.268 0.000158 #&gt; 4 0.1965 0.542 0.0454 0.0256 0.1511 0.00922 0.267 0.001510 #&gt; 5 0.4958 0.680 0.1370 0.0351 0.3588 0.01737 0.266 0.016307 #&gt; 6 -0.3495 0.321 -0.1006 0.0319 -0.2490 0.01433 0.267 0.006436 #&gt; .std.resid #&gt; 1 0.564 #&gt; 2 0.778 #&gt; 3 -0.160 #&gt; 4 0.570 #&gt; 5 1.358 #&gt; 6 -0.941 We can plot the results of the bivariate linear regression as follows: ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_abline(slope = coef(fit)[&quot;d.comp&quot;], intercept = coef(fit)[&quot;(Intercept)&quot;]) A more general way to plot the predictions of the model against the data is to use the methods described in Ch 23.3.3 of R4DS. Create an evenly spaced grid of values of d.comp, and add predictions of the model to it. grid &lt;- face %&gt;% data_grid(d.comp) %&gt;% add_predictions(fit) head(grid) #&gt; # A tibble: 6 × 2 #&gt; d.comp pred #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0640 -0.270 #&gt; 2 0.0847 -0.256 #&gt; 3 0.0893 -0.253 #&gt; 4 0.1145 -0.237 #&gt; 5 0.1148 -0.236 #&gt; 6 0.1639 -0.204 Now we can plot the regression line and the original data just like any other plot. ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_line(data = grid, mapping = aes(x = d.comp, y = pred)) This method is more complicated than the geom_abline method for a bivariate regerssion, but will work for more complicated models, while the geom_abline method won’t. Note that geom_smooth can be used to add a regression line to a data-set. ggplot(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The argument method = &quot;lm&quot; specifies that the function lm is to be used to generate fitted values. It is equivalent to running the regression lm(y ~ x) and plotting the regression line, where y and x are the aesthetics specified by the mappings. The argument se = FALSE tells the function not to plot the confidence interval of the regresion (discussed later). 5.5.4 Regresion towards the mean 5.5.5 Merging Data Sets in R See the R for Data Science chapter Relational data. Merge - or intter join the data frames by state: pres12 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pres12.csv&quot;)) full_join(pres08, pres12, by = &quot;state&quot;) #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama.x McCain EV.x margin Obama.y Romney EV.y #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows Note: this could be a question. How would you change the .x, .y suffixes to something more informative To avoid the duplicate names, or change them, you can rename before merging, or use the suffix argument: pres &lt;- full_join(pres08, pres12, by = &quot;state&quot;, suffix = c(&quot;_08&quot;, &quot;_12&quot;)) pres #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama_08 McCain EV_08 margin Obama_12 Romney EV_12 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows The dplyr equivalent functions for cbind is bind_cols. pres &lt;- pres %&gt;% mutate(Obama2008.z = scale(Obama_08), Obama2012.z = scale(Obama_12)) Scatterplot of states with vote shares in 2008 and 2012 ggplot(pres, aes(x = Obama2008.z, y = Obama2012.z, label = state)) + geom_text() + geom_abline() + coord_fixed() + scale_x_continuous(&quot;Obama&#39;s standardized vote share in 2008&quot;, limits = c(-4, 4)) + scale_y_continuous(&quot;Obama&#39;s standardized vote share in 2012&quot;, limits = c(-4, 4)) To calcualte the bottom and top quartiles pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.25)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.583 pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.75)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.5 Challenge: Why is it important to standardize the vote shares? 5.5.6 Model Fit florida &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;florida.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; county = col_character(), #&gt; Clinton96 = col_integer(), #&gt; Dole96 = col_integer(), #&gt; Perot96 = col_integer(), #&gt; Bush00 = col_integer(), #&gt; Gore00 = col_integer(), #&gt; Buchanan00 = col_integer() #&gt; ) fit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida) summary(fit2) #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -612.7 -66.0 1.9 32.9 2301.7 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.34575 49.75931 0.03 0.98 #&gt; Perot96 0.03592 0.00434 8.28 9.5e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 316 on 65 degrees of freedom #&gt; Multiple R-squared: 0.513, Adjusted R-squared: 0.506 #&gt; F-statistic: 68.5 on 1 and 65 DF, p-value: 9.47e-12 In addition to, summary(fit2)$r.squared #&gt; [1] 0.513 we can get the R2 squared value from the data frame glance returns: glance(fit2) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.513 0.506 316 68.5 9.47e-12 2 -480 966 972 #&gt; deviance df.residual #&gt; 1 6506118 65 We can get predictions and residuals on the original data frame using the modelr functions add_residuals and add_predictions florida &lt;- florida %&gt;% add_predictions(fit2) %&gt;% add_residuals(fit2) glimpse(florida) #&gt; Observations: 67 #&gt; Variables: 9 #&gt; $ county &lt;chr&gt; &quot;Alachua&quot;, &quot;Baker&quot;, &quot;Bay&quot;, &quot;Bradford&quot;, &quot;Brevard&quot;, &quot;... #&gt; $ Clinton96 &lt;int&gt; 40144, 2273, 17020, 3356, 80416, 320736, 1794, 2712... #&gt; $ Dole96 &lt;int&gt; 25303, 3684, 28290, 4038, 87980, 142834, 1717, 2783... #&gt; $ Perot96 &lt;int&gt; 8072, 667, 5922, 819, 25249, 38964, 630, 7783, 7244... #&gt; $ Bush00 &lt;int&gt; 34124, 5610, 38637, 5414, 115185, 177323, 2873, 354... #&gt; $ Gore00 &lt;int&gt; 47365, 2392, 18850, 3075, 97318, 386561, 2155, 2964... #&gt; $ Buchanan00 &lt;int&gt; 263, 73, 248, 65, 570, 788, 90, 182, 270, 186, 122,... #&gt; $ pred &lt;dbl&gt; 291.3, 25.3, 214.0, 30.8, 908.2, 1400.7, 24.0, 280.... #&gt; $ resid &lt;dbl&gt; -2.83e+01, 4.77e+01, 3.40e+01, 3.42e+01, -3.38e+02,... There are now two new columns in florida, pred with the fitted values (predictions), and resid with the residuals. Use fit2_augment to create a residual plot: fit2_resid_plot &lt;- ggplot(florida, aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) fit2_resid_plot Note, we use the function geom_refline to add a reference line at 0. Let’s add some labels to points, who is that outlier? fit2_resid_plot + geom_label(aes(label = county)) The outlier county is “Palm Beach” arrange(florida) %&gt;% arrange(desc(abs(resid))) %&gt;% select(county, resid) %&gt;% head() #&gt; # A tibble: 6 × 2 #&gt; county resid #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 PalmBeach 2302 #&gt; 2 Broward -613 #&gt; 3 Lee -357 #&gt; 4 Brevard -338 #&gt; 5 Miami-Dade -329 #&gt; 6 Pinellas -317 Ted Mosby dressed as a Hanging Chad in “How I Met Your Mother” Data without Palm Beach florida_pb &lt;- filter(florida, county != &quot;PalmBeach&quot;) fit3 &lt;- lm(Buchanan00 ~ Perot96, data = florida_pb) summary(fit3) #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida_pb) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -206.7 -43.5 -16.0 26.9 269.0 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 45.84193 13.89275 3.3 0.0016 ** #&gt; Perot96 0.02435 0.00127 19.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 87.7 on 64 degrees of freedom #&gt; Multiple R-squared: 0.851, Adjusted R-squared: 0.849 #&gt; F-statistic: 366 on 1 and 64 DF, p-value: &lt;2e-16 \\(R^2\\) or coefficient of determination glance(fit3) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.851 0.849 87.7 366 3.61e-28 2 -388 782 788 #&gt; deviance df.residual #&gt; 1 492803 64 florida_pb %&gt;% add_residuals(fit3) %&gt;% add_predictions(fit3) %&gt;% ggplot(aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + ylim(-750, 2500) + xlim(0, 1500) + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) Create predictions for both models using data_grid and gather_predictions: florida_grid &lt;- florida %&gt;% data_grid(Perot96) %&gt;% gather_predictions(fit2, fit3) %&gt;% mutate(model = fct_recode(model, &quot;Regression\\n with Palm Beach&quot; = &quot;fit2&quot;, &quot;Regression\\n without Palm Beach&quot; = &quot;fit3&quot;)) Note this is an example of using non-syntactic column names in a tibble, as discussed in Chapter 10 of R for data science. ggplot() + geom_point(data = florida, mapping = aes(x = Perot96, y = Buchanan00)) + geom_line(data = florida_grid, mapping = aes(x = Perot96, y = pred, colour = model)) + geom_label(data = filter(florida, county == &quot;PalmBeach&quot;), mapping = aes(x = Perot96, y = Buchanan00, label = county), vjust = &quot;top&quot;, hjust = &quot;right&quot;) + geom_text(data = tibble(label = unique(florida_grid$model), x = c(20000, 31000), y = c(1000, 300)), mapping = aes(x = x, y = y, label = label, colour = label)) + labs(x = &quot;Perot&#39;s Vote in 1996&quot;, y = &quot;Buchanan&#39;s Votes in 1996&quot;) + theme(legend.position = &quot;none&quot;) See Graphics for communication in R for Data Science on labels and annotations in plots. 5.6 Regression and Causation Load data women &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;women.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; GP = col_integer(), #&gt; village = col_integer(), #&gt; reserved = col_integer(), #&gt; female = col_integer(), #&gt; irrigation = col_integer(), #&gt; water = col_integer() #&gt; ) proportion of female politicians in reserved GP vs. unreserved GP women %&gt;% group_by(reserved) %&gt;% summarise(prop_female = mean(female)) #&gt; # A tibble: 2 × 2 #&gt; reserved prop_female #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0.0748 #&gt; 2 1 1.0000 The diff in diff estimator ## drinking-water facilities ## irrigation facilities mean(women$irrigation[women$reserved == 1]) - mean(women$irrigation[women$reserved == 0]) #&gt; [1] -0.369 Mean values of irrigation and water in reserved and non-reserved districts. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) #&gt; # A tibble: 2 × 3 #&gt; reserved irrigation water #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 3.39 14.7 #&gt; 2 1 3.02 24.0 The difference between the two groups can be calculated with the function diff, which calculates the difference between subsequent observations. This works as long as we are careful about which group is first or second. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% summarise(diff_irrigation = diff(irrigation), diff_water = diff(water)) #&gt; # A tibble: 1 × 2 #&gt; diff_irrigation diff_water #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.369 9.25 The other way uses tidyr spread and gather, women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% gather(variable, value, -reserved) %&gt;% spread(reserved, value) %&gt;% mutate(diff = `1` - `0`) #&gt; # A tibble: 2 × 4 #&gt; variable `0` `1` diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 irrigation 3.39 3.02 -0.369 #&gt; 2 water 14.74 23.99 9.252 Now each row is an outcome variable of interest, and there are columns for the treatment (1) and control (0) groups, and the difference (diff). lm(water ~ reserved, data = women) %&gt;% summary() #&gt; #&gt; Call: #&gt; lm(formula = water ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -23.99 -14.74 -7.86 2.26 316.01 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 14.74 2.29 6.45 4.2e-10 *** #&gt; reserved 9.25 3.95 2.34 0.02 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 33.4 on 320 degrees of freedom #&gt; Multiple R-squared: 0.0169, Adjusted R-squared: 0.0138 #&gt; F-statistic: 5.49 on 1 and 320 DF, p-value: 0.0197 lm(irrigation ~ reserved, data = women) %&gt;% summary() #&gt; #&gt; Call: #&gt; lm(formula = irrigation ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.39 -3.39 -3.02 -1.02 86.61 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.388 0.650 5.21 3.3e-07 *** #&gt; reserved -0.369 1.122 -0.33 0.74 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 9.51 on 320 degrees of freedom #&gt; Multiple R-squared: 0.000338, Adjusted R-squared: -0.00279 #&gt; F-statistic: 0.108 on 1 and 320 DF, p-value: 0.742 5.6.1 Regression with multiple predictors social &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;social.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2008 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) glimpse(social) #&gt; Observations: 305,866 #&gt; Variables: 6 #&gt; $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;mal... #&gt; $ yearofbirth &lt;int&gt; 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 19... #&gt; $ primary2004 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,... #&gt; $ messages &lt;chr&gt; &quot;Civic Duty&quot;, &quot;Civic Duty&quot;, &quot;Hawthorne&quot;, &quot;Hawthorn... #&gt; $ primary2008 &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,... #&gt; $ hhsize &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 1,... levels(social$messages) #&gt; NULL fit &lt;- lm(primary2008 ~ messages, data = social) fit #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ messages, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesControl messagesHawthorne #&gt; 0.31454 -0.01790 0.00784 #&gt; messagesNeighbors #&gt; 0.06341 Create indicator variables social &lt;- social %&gt;% mutate(Control = as.integer(messages == &quot;Control&quot;), Hawthorne = as.integer(messages == &quot;Hawthorne&quot;), Neighbors = as.integer(messages == &quot;Neighbors&quot;)) alternatively, create these using a for loop. This is easier to understand and less prone to typo. for (i in unique(social$messages)) { social[[i]] &lt;- as.integer(social[[&quot;messages&quot;]] == i) } We created a variable for each level of messages even though we will exclude one of them. lm(primary2008 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) Control Hawthorne Neighbors #&gt; 0.31454 -0.01790 0.00784 0.06341 Create predictions for each unique value of messages unique_messages &lt;- data_grid(social, messages) %&gt;% add_predictions(fit) unique_messages #&gt; # A tibble: 4 × 2 #&gt; messages pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Compare to the sample averages social %&gt;% group_by(messages) %&gt;% summarise(mean(primary2008)) #&gt; # A tibble: 4 × 2 #&gt; messages `mean(primary2008)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Linear regression without intercept. fit.noint &lt;- lm(primary2008 ~ -1 + messages, data = social) fit.noint #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ -1 + messages, data = social) #&gt; #&gt; Coefficients: #&gt; messagesCivic Duty messagesControl messagesHawthorne #&gt; 0.315 0.297 0.322 #&gt; messagesNeighbors #&gt; 0.378 Calculating the regression average effect is also easier if we make the control group the first level so all regression coefficients are comparisons to it. Use fct_relevel to make “Control” fit.control &lt;- mutate(social, messages = fct_relevel(messages, &quot;Control&quot;)) %&gt;% lm(primary2008 ~ messages, data = .) fit.control #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ messages, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesCivic Duty messagesHawthorne #&gt; 0.2966 0.0179 0.0257 #&gt; messagesNeighbors #&gt; 0.0813 Difference in means social %&gt;% group_by(messages) %&gt;% summarise(primary2008 = mean(primary2008)) %&gt;% mutate(Control = primary2008[messages == &quot;Control&quot;], diff = primary2008 - Control) #&gt; # A tibble: 4 × 4 #&gt; messages primary2008 Control diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Adjusted R-squared is included in the output of broom::glance() glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC #&gt; 1 0.00328 0.00327 0.463 336 1.06e-217 4 -198247 396504 #&gt; BIC deviance df.residual #&gt; 1 396557 65468 305862 glance(fit)$adj.r.squared #&gt; [1] 0.00327 5.6.2 Heterogenous Treatment Effects Average treatment effect (ate) among those who voted in 2004 primary ate &lt;- social %&gt;% group_by(primary2004, messages) %&gt;% summarise(primary2008 = mean(primary2008)) %&gt;% spread(messages, primary2008) %&gt;% mutate(ate_Neighbors = Neighbors - Control) %&gt;% select(primary2004, Neighbors, Control, ate_Neighbors) ate #&gt; Source: local data frame [2 x 4] #&gt; Groups: primary2004 [2] #&gt; #&gt; primary2004 Neighbors Control ate_Neighbors #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.306 0.237 0.0693 #&gt; 2 1 0.482 0.386 0.0965 Difference in ATE in 2004 voters and non-voters diff(ate$ate_Neighbors) #&gt; [1] 0.0272 social.neighbor &lt;- social %&gt;% filter((messages == &quot;Control&quot;) | (messages == &quot;Neighbors&quot;)) fit.int &lt;- lm(primary2008 ~ primary2004 + messages + primary2004:messages, data = social.neighbor) fit.int #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ primary2004 + messages + primary2004:messages, #&gt; data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 lm(primary2008 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 social.neighbor &lt;- social.neighbor %&gt;% mutate(age = 2008 - yearofbirth) summary(social.neighbor$age) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 22.0 43.0 52.0 51.8 61.0 108.0 fit.age &lt;- lm(primary2008 ~ age * messages, data = social.neighbor) fit.age #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ age * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) age messagesNeighbors #&gt; 0.089477 0.003998 0.048573 #&gt; age:messagesNeighbors #&gt; 0.000628 Calculate average treatment effects ate.age &lt;- crossing(age = seq(from = 25, to = 85, by = 20), messages = c(&quot;Neighbors&quot;, &quot;Control&quot;)) %&gt;% add_predictions(fit.age) %&gt;% spread(messages, pred) %&gt;% mutate(diff = Neighbors - Control) ate.age #&gt; # A tibble: 4 × 4 #&gt; age Control Neighbors diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 25 0.189 0.254 0.0643 #&gt; 2 45 0.269 0.346 0.0768 #&gt; 3 65 0.349 0.439 0.0894 #&gt; 4 85 0.429 0.531 0.1020 You can use poly for polynomials instead of age + I(age ^ 2). Though note that the coefficients will be be different since by default poly calculates orthogonal polynomials instead of the natural (raw) polynomials. However, you really shouldn’t interpet the coefficients directly anyways, so this should matter. fit.age2 &lt;- lm(primary2008 ~ poly(age, 2) * messages, data = social.neighbor) fit.age2 #&gt; #&gt; Call: #&gt; lm(formula = primary2008 ~ poly(age, 2) * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) poly(age, 2)1 #&gt; 0.2966 27.6665 #&gt; poly(age, 2)2 messagesNeighbors #&gt; -10.2832 0.0816 #&gt; poly(age, 2)1:messagesNeighbors poly(age, 2)2:messagesNeighbors #&gt; 4.5820 -5.5124 Create a data frame of combinations of ages and messages using data_grid, which means that we only need to specify the variables, and not the specific values, y.hat &lt;- data_grid(social.neighbor, age, messages) %&gt;% add_predictions(fit.age2) ggplot(y.hat, aes(x = age, y = pred, colour = str_c(messages, &quot; condition&quot;))) + geom_line() + labs(colour = &quot;&quot;, y = &quot;predicted turnout rates&quot;) + theme(legend.position = &quot;bottom&quot;) y.hat %&gt;% spread(messages, pred) %&gt;% mutate(ate = Neighbors - Control) %&gt;% filter(age &gt; 20, age &lt; 90) %&gt;% ggplot(aes(x = age, y = ate)) + geom_line() + labs(y = &quot;estimated average treatment effect&quot;) + ylim(0, 0.1) 5.7 Regression Discontinuity Design Original code MPs &lt;- read.csv(&quot;MPs.csv&quot;) MPs.labour &lt;- subset(MPs, subset = (party == &quot;labour&quot;)) MPs.tory &lt;- subset(MPs, subset = (party == &quot;tory&quot;)) ## two regressions for Labour: negative and positive margin labour.fit1 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &lt; 0, ]) labour.fit2 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &gt; 0, ]) ## two regressions for Tory: negative and positive margin tory.fit1 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &lt; 0, ]) tory.fit2 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &gt; 0, ]) Tidyverse MPs &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;MPs.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; firstname = col_character(), #&gt; party = col_character(), #&gt; ln.gross = col_double(), #&gt; ln.net = col_double(), #&gt; yob = col_integer(), #&gt; yod = col_integer(), #&gt; margin.pre = col_double(), #&gt; region = col_character(), #&gt; margin = col_double() #&gt; ) MPs_labour &lt;- filter(MPs, party == &quot;labour&quot;) MPs_tory &lt;- filter(MPs, party == &quot;tory&quot;) labour_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &lt; 0)) labour_fit2 &lt;- lm(ln.net ~ margin, MPs_labour, margin &gt; 0) tory_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &gt; 0)) Original code ## Labour: range of predictions y1l.range &lt;- c(min(MPs.labour$margin), 0) # min to 0 y2l.range &lt;- c(0, max(MPs.labour$margin)) # 0 to max ## prediction y1.labour &lt;- predict(labour.fit1, newdata = data.frame(margin = y1l.range)) y2.labour &lt;- predict(labour.fit2, newdata = data.frame(margin = y2l.range)) ## Tory: range of predictions y1t.range &lt;- c(min(MPs.tory$margin), 0) # min to 0 y2t.range &lt;- c(0, max(MPs.tory$margin)) # 0 to max ## predict outcome y1.tory &lt;- predict(tory.fit1, newdata = data.frame(margin = y1t.range)) y2.tory &lt;- predict(tory.fit2, newdata = data.frame(margin = y2t.range)) Tidyverse code Use data_grid to generate a grid for predictions. y1_labour &lt;- filter(MPs_labour, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit1) y2_labour &lt;- filter(MPs_labour, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit2) y1_tory &lt;- filter(MPs_tory, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit1) y2_tory &lt;- filter(MPs_tory, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit2) Original code ## scatterplot with regression lines for labour plot(MPs.labour$margin, MPs.labour$ln.net, main = &quot;Labour&quot;, xlim = c(-0.5, 0.5), ylim = c(6, 18), xlab = &quot;margin of victory&quot;, ylab = &quot;log net wealth at death&quot;) abline(v = 0, lty = &quot;dashed&quot;) ## add regression lines lines(y1l.range, y1.labour, col = &quot;red&quot;) lines(y2l.range, y2.labour, col = &quot;red&quot;) ## scatterplot with regression lines for tory plot(MPs.tory$margin, MPs.tory$ln.net, main = &quot;Tory&quot;, xlim = c(-0.5, 0.5), ylim = c(6, 18), xlab = &quot;margin of victory&quot;, ylab = &quot;log net wealth at death&quot;) abline(v = 0, lty = &quot;dashed&quot;) ## add regression lines lines(y1t.range, y1.tory, col = &quot;red&quot;) lines(y2t.range, y2.tory, col = &quot;red&quot;) Tidyverse code Labour politicians ggplot() + geom_ref_line(v = 0) + geom_point(data = MPs_labour, mapping = aes(x = margin, y = ln.net)) + geom_line(data = y1_labour, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + geom_line(data = y2_labour, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;, title = &quot;Labour&quot;) Tory politicians ggplot() + geom_ref_line(v = 0) + geom_point(data = MPs_tory, mapping = aes(x = margin, y = ln.net)) + geom_line(data = y1_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + geom_line(data = y2_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;, title = &quot;labour&quot;) We can actually produce this plot easily without running the regressions, by using geom_smooth: ggplot(mutate(MPs, winner = (margin &gt; 0)), aes(x = margin, y = ln.net)) + geom_ref_line(v = 0) + geom_point() + geom_smooth(method = lm, se = FALSE, mapping = aes(group = winner)) + facet_grid(party ~ .) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;) Original code ## average net wealth for Tory MP tory.MP &lt;- exp(y2.tory[1]) tory.MP ## 1 ## 533813.5 ## average net wealth for Tory non-MP tory.nonMP &lt;- exp(y1.tory[2]) tory.nonMP ## 2 ## 278762.5 ## causal effect in pounds tory.MP - tory.nonMP ## 1 ## 255050.9 Tidyverse code In the previous code, I didn’t directly compute the the average net wealth at 0, so I’ll need to do that here. I’ll use gather_predictions to add predictions for multiple models: spread_predictions(data_frame(margin = 0), tory_fit1, tory_fit2) %&gt;% mutate(rd_est = tory_fit2 - tory_fit1) #&gt; # A tibble: 1 × 4 #&gt; margin tory_fit1 tory_fit2 rd_est #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 12.5 13.2 0.65 Original code ## two regressions for Tory: negative and positive margin tory.fit3 &lt;- lm(margin.pre ~ margin, data = MPs.tory[MPs.tory$margin &lt; 0, ]) tory.fit4 &lt;- lm(margin.pre ~ margin, data = MPs.tory[MPs.tory$margin &gt; 0, ]) ## the difference between two intercepts is the estimated effect coef(tory.fit4)[1] - coef(tory.fit3)[1] ## (Intercept) ## -0.01725578 Tidyverse code tory_fit3 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit4 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &gt; 0)) (filter(tidy(tory_fit3), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]] - filter(tidy(tory_fit4), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]]) #&gt; [1] 0.0173 "],
["discovery.html", "6 Discovery 6.1 Prerequisites 6.2 Textual data 6.3 Network Data 6.4 Spatial Data in R", " 6 Discovery The idea of tidy data and the common feature of tidyverse packages is that data should be stored in data frames with certain conventions. This works well with naturally tabular data, the type which has been common in social science applications. But there are other domains in which other data structures are more appropriate because they more naturally model the data or processes, or for computational reasons. The three applications in this chapter: text, networks, and spatial data are examples where the tidy data structure is less of an advantage. I will still rely on ggplot2 for plotting, and use tidyverse compatible packages where appropriate. Textual data: tidytext Network data: igraph for network computation, as in the chapter. But several ggplot2 extension packages for plotting the networks. Spatial data: ggplot2 has some built-in support for maps. The map package provides map data. See the R for Data Science section 12.7 Non-tidy data and this post on Non-tidy data by Jeff Leek for more on non-tidy data. 6.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) library(&quot;modelr&quot;) 6.2 Textual data library(&quot;tm&quot;) #&gt; Loading required package: NLP #&gt; #&gt; Attaching package: &#39;NLP&#39; #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; annotate library(&quot;SnowballC&quot;) library(&quot;tidytext&quot;) This section will primarily use the tidytext package. It is a relatively new package. The tm and quanteda (by Ken Benoit) packages are more established and use the document-term matrix format as described in the QSS chapter. The tidytext package stores everything in a data frame; this may be less efficient than the other packages, but has the benefit of being able to easily take advantage of the tidyverse ecosystem. If your corpus is not too large, this shouldn’t be an issue. See Tidy Text Mining with R for a full introduction to using tidytext. In tidy data, each row is an observation and each column is a variable. In the tidytext package, documents are stored as data frames with one-term-per-row. Original ## load the raw corpus corpus.raw &lt;- Corpus(DirSource(directory = &quot;federalist&quot;, pattern = &quot;fp&quot;)) corpus.raw ## make lower case corpus.prep &lt;- tm_map(corpus.raw, content_transformer(tolower)) ## remove white space corpus.prep &lt;- tm_map(corpus.prep, stripWhitespace) ## remove punctuation corpus.prep &lt;- tm_map(corpus.prep, removePunctuation) ## remove numbers corpus.prep &lt;- tm_map(corpus.prep, removeNumbers) head(stopwords(&quot;english&quot;)) ## remove stop words corpus &lt;- tm_map(corpus.prep, removeWords, stopwords(&quot;english&quot;)) ## finally stem remaining words corpus &lt;- tm_map(corpus, stemDocument) ## the output is truncated here to save space content(corpus[[10]]) # Essay No. 10 We can cast data into the tidytext format either from the Corpus object, or, after processing, from the document-term matrix object. DIR_SOURCE &lt;- file.path(&quot;qss&quot;, &quot;DISCOVERY&quot;, &quot;federalist&quot;) corpus_raw &lt;- Corpus(DirSource(directory = DIR_SOURCE, pattern = &quot;fp&quot;)) corpus_raw #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 85 Use the tidy function to convert it to a data frame with one row per document. corpus_tidy &lt;- tidy(corpus_raw) corpus_tidy #&gt; # A tibble: 85 × 8 #&gt; author datetimestamp description heading id language origin #&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 NA 2017-01-17 01:14:10 NA NA fp01.txt en NA #&gt; 2 NA 2017-01-17 01:14:10 NA NA fp02.txt en NA #&gt; 3 NA 2017-01-17 01:14:10 NA NA fp03.txt en NA #&gt; 4 NA 2017-01-17 01:14:10 NA NA fp04.txt en NA #&gt; 5 NA 2017-01-17 01:14:10 NA NA fp05.txt en NA #&gt; 6 NA 2017-01-17 01:14:10 NA NA fp06.txt en NA #&gt; # ... with 79 more rows, and 1 more variables: text &lt;chr&gt; The text column contains the text of the documents themselves. Since most of the metadat is irrelevant, we’ll delete those columns, keepin only the document (id) and text columns. corpus_tidy &lt;- select(corpus_tidy, id, text) Also, we want to extract the essay number and use that as the document id rather than its filename. corpus_tidy &lt;- mutate(corpus_tidy, document = as.integer(str_extract(id, &quot;\\\\d+&quot;))) %&gt;% select(-id) The unnest_tokens tokenizes the document texts. tokens &lt;- corpus_tidy %&gt;% # tokenizes into words and stems them unnest_tokens(word, text, token = &quot;word_stems&quot;) %&gt;% # remove any numbers in the strings mutate(word = str_replace_all(word, &quot;\\\\d+&quot;, &quot;&quot;)) %&gt;% # drop any empty strings filter(word != &quot;&quot;) tokens #&gt; # A tibble: 187,412 × 2 #&gt; document word #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 after #&gt; 2 1 an #&gt; 3 1 unequivoc #&gt; 4 1 experi #&gt; 5 1 of #&gt; 6 1 the #&gt; # ... with 1.874e+05 more rows The unnest_tokens function uses the tokenizers package to tokenize the text. By default, it uses the function which removes punctuation, and lowercases the words. I set the tokenizer to to stem the word, using the SnowballC package. We can remove stopwords with an anti_join on the dataset stop_words data(&quot;stop_words&quot;, package = &quot;tidytext&quot;) tokens &lt;- anti_join(tokens, stop_words, by = &quot;word&quot;) 6.2.1 Document-Term Matrix Original: dtm &lt;- DocumentTermMatrix(corpus) dtm inspect(dtm[1:5, 1:8]) dtm.mat &lt;- as.matrix(dtm) In tokens there is one observation for each token (word) in the each document. This is almost equivalent to a document-term matrix. For a document-term matrix we need documents, and terms as the keys for the data and a column with the number of times the term appeared in the document. dtm &lt;- count(tokens, document, word) head(dtm) #&gt; Source: local data frame [6 x 3] #&gt; Groups: document [1] #&gt; #&gt; document word n #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 abl 1 #&gt; 2 1 absurd 1 #&gt; 3 1 accid 1 #&gt; 4 1 accord 1 #&gt; 5 1 acknowledg 1 #&gt; 6 1 act 1 6.2.2 Topic Discovery Original: library(wordcloud) wordcloud(colnames(dtm.mat), dtm.mat[12, ], max.words = 20) # essay No. 12 wordcloud(colnames(dtm.mat), dtm.mat[24, ], max.words = 20) # essay No. 24 Plot the wordclouds for essays 12 and 24: library(&quot;wordcloud&quot;) #&gt; Loading required package: RColorBrewer filter(dtm, document == 12) %&gt;% {wordcloud(.$word, .$n, max.words = 20)} filter(dtm, document == 24) %&gt;% {wordcloud(.$word, .$n, max.words = 20)} Original: stemCompletion(c(&quot;revenu&quot;, &quot;commerc&quot;, &quot;peac&quot;, &quot;army&quot;), corpus.prep) Original: dtm.tfidf &lt;- weightTfIdf(dtm) # tf-idf calculation dtm.tfidf.mat &lt;- as.matrix(dtm.tfidf) # convert to matrix ## 10 most important words for Paper No. 12 head(sort(dtm.tfidf.mat[12, ], decreasing = TRUE), n = 10) ## 10 most important words for Paper No. 24 head(sort(dtm.tfidf.mat[24, ], decreasing = TRUE), n = 10) tidyverse: Use the function bind_tf_idf to add a column with the tf-idf to the data frame. dtm &lt;- bind_tf_idf(dtm, word, document, n) dtm #&gt; Source: local data frame [38,764 x 6] #&gt; Groups: document [85] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 abl 1 0.00176 0.705 0.001241 #&gt; 2 1 absurd 1 0.00176 1.735 0.003054 #&gt; 3 1 accid 1 0.00176 3.750 0.006601 #&gt; 4 1 accord 1 0.00176 0.754 0.001327 #&gt; 5 1 acknowledg 1 0.00176 1.552 0.002733 #&gt; 6 1 act 1 0.00176 0.400 0.000704 #&gt; # ... with 3.876e+04 more rows The 10 most important words for Paper No. 12 are dtm %&gt;% filter(document == 12) %&gt;% top_n(10, tf_idf) #&gt; Source: local data frame [10 x 6] #&gt; Groups: document [1] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 12 cent 2 0.00240 4.44 0.0107 #&gt; 2 12 coast 3 0.00360 3.75 0.0135 #&gt; 3 12 commerc 8 0.00959 1.11 0.0107 #&gt; 4 12 contraband 3 0.00360 4.44 0.0160 #&gt; 5 12 excis 5 0.00600 2.65 0.0159 #&gt; 6 12 gallon 2 0.00240 4.44 0.0107 #&gt; # ... with 4 more rows and for Paper No. 24, dtm %&gt;% filter(document == 24) %&gt;% top_n(10, tf_idf) #&gt; Source: local data frame [10 x 6] #&gt; Groups: document [1] #&gt; #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 24 armi 7 0.01034 1.26 0.01308 #&gt; 2 24 arsenal 2 0.00295 3.75 0.01108 #&gt; 3 24 dock 3 0.00443 4.44 0.01969 #&gt; 4 24 frontier 3 0.00443 2.83 0.01255 #&gt; 5 24 garrison 6 0.00886 2.83 0.02511 #&gt; 6 24 nearer 2 0.00295 3.34 0.00988 #&gt; # ... with 4 more rows The slightly different results from the book are due to tokenization differences. Original: k &lt;- 4 # number of clusters ## subset The Federalist papers written by Hamilton hamilton &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) dtm.tfidf.hamilton &lt;- dtm.tfidf.mat[hamilton, ] ## run k-means km.out &lt;- kmeans(dtm.tfidf.hamilton, centers = k) km.out$iter # check the convergence; number of iterations may vary ## label each centroid with the corresponding term colnames(km.out$centers) &lt;- colnames(dtm.tfidf.hamilton) for (i in 1:k) { # loop for each cluster cat(&quot;CLUSTER&quot;, i, &quot;\\n&quot;) cat(&quot;Top 10 words:\\n&quot;) # 10 most important terms at the centroid print(head(sort(km.out$centers[i, ], decreasing = TRUE), n = 10)) cat(&quot;\\n&quot;) cat(&quot;Federalist Papers classified: \\n&quot;) # extract essays classified print(rownames(dtm.tfidf.hamilton)[km.out$cluster == i]) cat(&quot;\\n&quot;) } tidyverse: Subset those documents known to have been written by Hamilton. HAMILTON_ESSAYS &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) dtm_hamilton &lt;- filter(dtm, document %in% HAMILTON_ESSAYS) The kmeans function expects the input to be rows for observations and columns for each variable: in our case that would be documents as rows, and words as columns, with the tf-idf as the cell values. We could use spread to do this, but that would be a large matrix. CLUSTERS &lt;- 4 km_out &lt;- kmeans(cast_dtm(dtm_hamilton, document, word, tf_idf), centers = CLUSTERS, nstart = 10) km_out$iter #&gt; [1] 3 Dataframe with the unique terms used by Hamilton. I extract these from the colnames of the DTM after cast_dtm to ensure that the order is the same as the k-means results. hamilton_words &lt;- tibble(word = colnames(cast_dtm(dtm_hamilton, document, word, tf_idf))) The centers of the clusters is a cluster x word matrix. We want to transpose it and then append columns to hamilton_words so the location of each word in the cluster is listed. dim(km_out$centers) #&gt; [1] 4 3850 hamilton_words &lt;- bind_cols(hamilton_words, as_tibble(t(km_out$centers))) hamilton_words #&gt; # A tibble: 3,850 × 5 #&gt; word `1` `2` `3` `4` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 abl 0.000262 0.000000 0.001004 0.00113 #&gt; 2 absurd 0.000272 0.001430 0.000678 0.00000 #&gt; 3 accid 0.000000 0.000000 0.000292 0.00000 #&gt; 4 accord 0.000319 0.001381 0.000499 0.00000 #&gt; 5 acknowledg 0.000000 0.000767 0.000556 0.00000 #&gt; 6 act 0.000927 0.000838 0.000657 0.00000 #&gt; # ... with 3,844 more rows To find the top 10 words in each centroid, we use top_n with group_by: top_words_cluster &lt;- gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) We can print them out using a for loop for (i in 1:CLUSTERS) { cat(&quot;CLUSTER &quot;, i, &quot;: &quot;, str_c(filter(top_words_cluster, cluster == i)$word, collapse = &quot;, &quot;), &quot;\\n\\n&quot;) } #&gt; CLUSTER 1 : offic, accus, presid, treati, appoint, senat, nomin, governor, impeach, pardon #&gt; #&gt; CLUSTER 2 : court, appeal, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, appel #&gt; #&gt; CLUSTER 3 : confederaci, tax, war, land, revenu, armi, militari, militia, taxat, claus #&gt; #&gt; CLUSTER 4 : presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc This is alternative code that prints out a table: gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) %&gt;% summarise(top_words = str_c(word, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster top_words 1 offic, accus, presid, treati, appoint, senat, nomin, governor, impeach, pardon 2 court, appeal, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, appel 3 confederaci, tax, war, land, revenu, armi, militari, militia, taxat, claus 4 presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc Or to print out the documents in each cluster, enframe(km_out$cluster, &quot;document&quot;, &quot;cluster&quot;) %&gt;% group_by(cluster) %&gt;% summarise(documents = str_c(document, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster documents 1 65, 66, 68, 69, 74, 75, 76, 77, 79 2 81, 82, 83 3 1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60, 61, 70, 71, 72, 73, 78, 80, 84, 85 4 67 6.2.3 Authorship Prediction Original: ## document-term matrix converted to matrix for manipulation dtm1 &lt;- as.matrix(DocumentTermMatrix(corpus.prep)) tfm &lt;- dtm1 / rowSums(dtm1) * 1000 # term frequency per 1000 words ## words of interest words &lt;- c(&quot;although&quot;, &quot;always&quot;, &quot;commonly&quot;, &quot;consequently&quot;, &quot;considerable&quot;, &quot;enough&quot;, &quot;there&quot;, &quot;upon&quot;, &quot;while&quot;, &quot;whilst&quot;) ## select only these words tfm &lt;- tfm[, words] ## essays written by Madison: `hamilton&#39; defined earlier madison &lt;- c(10, 14, 37:48, 58) ## average among Hamilton/Madison essays tfm.ave &lt;- rbind(colSums(tfm[hamilton, ]) / length(hamilton), colSums(tfm[madison, ]) / length(madison)) tfm.ave tidyverse: We’ll create a data-frame with the known MADISON_ESSAYS &lt;- c(10, 14, 37:48, 58) JAY_ESSAYS &lt;- c(2:5, 64) known_essays &lt;- bind_rows(tibble(document = MADISON_ESSAYS, author = &quot;Madison&quot;), tibble(document = HAMILTON_ESSAYS, author = &quot;Hamilton&quot;), tibble(document = JAY_ESSAYS, author = &quot;Jay&quot;)) STYLE_WORDS &lt;- tibble(word = c(&quot;although&quot;, &quot;always&quot;, &quot;commonly&quot;, &quot;consequently&quot;, &quot;considerable&quot;, &quot;enough&quot;, &quot;there&quot;, &quot;upon&quot;, &quot;while&quot;, &quot;whilst&quot;)) hm_tfm &lt;- unnest_tokens(corpus_tidy, word, text) %&gt;% count(document, word) %&gt;% # term freq per 1000 words group_by(document) %&gt;% mutate(count = n / sum(n) * 1000) %&gt;% select(-n) %&gt;% inner_join(STYLE_WORDS, by = &quot;word&quot;) %&gt;% # merge known essays left_join(known_essays, by = &quot;document&quot;) %&gt;% # make wide with each word a column # fill empty values with 0 spread(word, count, fill = 0) Calculate average usage by each author of each word hm_tfm %&gt;% # remove docs with no author filter(!is.na(author)) %&gt;% # convert back to long (tidy) format to make it easier to summarize gather(word, count, -document, -author) %&gt;% # calculate averge document word usage by author group_by(author, word) %&gt;% summarise(avg_count = mean(count)) %&gt;% spread(author, avg_count) %&gt;% knitr::kable() word Hamilton Jay Madison although 0.013 0.584 0.222 always 0.563 0.999 0.166 commonly 0.198 0.139 0.000 consequently 0.019 0.503 0.371 considerable 0.406 0.087 0.133 enough 0.295 0.000 0.000 there 3.303 1.026 0.917 upon 3.291 0.120 0.164 while 0.274 0.207 0.000 whilst 0.005 0.000 0.315 Original: author &lt;- rep(NA, nrow(dtm1)) # a vector with missing values author[hamilton] &lt;- 1 # 1 if Hamilton author[madison] &lt;- -1 # -1 if Madison ## data frame for regression author.data &lt;- data.frame(author = author[c(hamilton, madison)], tfm[c(hamilton, madison), ]) hm.fit &lt;- lm(author ~ upon + there + consequently + whilst, data = author.data) hm.fit hm.fitted &lt;- fitted(hm.fit) # fitted values sd(hm.fitted) tidyverse: library(&quot;modelr&quot;) author_data &lt;- hm_tfm %&gt;% ungroup() %&gt;% filter(is.na(author) | author != &quot;Jay&quot;) %&gt;% mutate(author2 = case_when(.$author == &quot;Hamilton&quot; ~ 1, .$author == &quot;Madison&quot; ~ -1, TRUE ~ NA_real_)) hm_fit &lt;- lm(author2 ~ upon + there + consequently + whilst, data = author_data) hm_fit #&gt; #&gt; Call: #&gt; lm(formula = author2 ~ upon + there + consequently + whilst, #&gt; data = author_data) #&gt; #&gt; Coefficients: #&gt; (Intercept) upon there consequently whilst #&gt; -0.195 0.213 0.118 -0.596 -0.910 author_data &lt;- author_data %&gt;% add_predictions(hm_fit) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;)) sd(author_data$pred) #&gt; [1] 0.79 These coefficients are a little different, probably due to differences in the tokenization procedure, and in particular, the document size normalization. 6.2.4 Cross-Validation Original: ## proportion of correctly classified essays by Hamilton mean(hm.fitted[author.data$author == 1] &gt; 0) ## proportion of correctly classified essays by Madison mean(hm.fitted[author.data$author == -1] &lt; 0) tidyverse: For cross-validation, I rely on the modelr package function RDoc(&quot;modelr::crossv_kfold&quot;). See the tutorial Cross validation of linear regression with modelr for more on using modelr for cross validation or k-fold cross-validation with modelr and broom. In sample, this regression perfectly predicts the authorship of the documents with known authors. author_data %&gt;% filter(!is.na(author)) %&gt;% group_by(author) %&gt;% summarise(`Proportion Correct` = mean(author == pred_author)) #&gt; # A tibble: 2 × 2 #&gt; author `Proportion Correct` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1 #&gt; 2 Madison 1 Create the cross-validation data-sets using . As in the chapter, I will use a leave-one-out cross-validation, which is a k-fold crossvalidation where k is the number of observations. To simplify this, I define the crossv_loo function that runs crossv_kfold with k = nrow(data). crossv_loo &lt;- function(data, id = &quot;.id&quot;) { crossv_kfold(data, k = nrow(data), id = id) } # leave one out cross-validation object cv &lt;- author_data %&gt;% filter(!is.na(author)) %&gt;% crossv_loo() Now estimate the model for each training dataset models &lt;- purrr::map(cv$train, ~ lm(author2 ~ upon + there + consequently + whilst, data = ., model = FALSE)) Note that I use purrr::map to ensure that the correct map() function is used since the maps package also defines a map. Now calculate the test performance on the held out observation, test &lt;- map2_df(models, cv$test, function(mod, test) { add_predictions(as.data.frame(test), mod) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;), correct = (pred_author == author)) }) test %&gt;% group_by(author) %&gt;% summarise(mean(correct)) #&gt; # A tibble: 2 × 2 #&gt; author `mean(correct)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1.000 #&gt; 2 Madison 0.786 Original: n &lt;- nrow(author.data) hm.classify &lt;- rep(NA, n) # a container vector with missing values for (i in 1:n) { ## fit the model to the data after removing the ith observation sub.fit &lt;- lm(author ~ upon + there + consequently + whilst, data = author.data[-i, ]) # exclude ith row ## predict the authorship for the ith observation hm.classify[i] &lt;- predict(sub.fit, newdata = author.data[i, ]) } ## proportion of correctly classified essays by Hamilton mean(hm.classify[author.data$author == 1] &gt; 0) ## proportion of correctly classified essays by Madison mean(hm.classify[author.data$author == -1] &lt; 0) Original: disputed &lt;- c(49, 50:57, 62, 63) # 11 essays with disputed authorship tf.disputed &lt;- as.data.frame(tfm[disputed, ]) ## prediction of disputed authorship pred &lt;- predict(hm.fit, newdata = tf.disputed) pred # predicted values tidyverse: When adding prediction with add_predictions it added predictions for missing values as well. Table of authorship of disputed papers author_data %&gt;% filter(is.na(author)) %&gt;% select(document, pred, pred_author) %&gt;% knitr::kable() document pred pred_author 18 -0.359 Madison 19 -0.586 Madison 20 -0.055 Madison 49 -0.966 Madison 50 -0.002 Madison 51 -1.522 Madison 52 -0.195 Madison 53 -0.506 Madison 54 -0.520 Madison 55 0.094 Hamilton 56 -0.550 Madison 57 -1.219 Madison 62 -0.944 Madison 63 -0.184 Madison par(cex = 1.25) ## fitted values for essays authored by Hamilton; red squares plot(hamilton, hm.fitted[author.data$author == 1], pch = 15, xlim = c(1, 85), ylim = c(-2, 2), col = &quot;red&quot;, xlab = &quot;Federalist Papers&quot;, ylab = &quot;Predicted values&quot;) abline(h = 0, lty = &quot;dashed&quot;) ## essays authored by Madison; blue circles points(madison, hm.fitted[author.data$author == -1], pch = 16, col = &quot;blue&quot;) ## disputed authorship; black triangles points(disputed, pred, pch = 17) disputed_essays &lt;- filter(author_data, is.na(author))$document ggplot(mutate(author_data, author = fct_explicit_na(factor(author), &quot;Disputed&quot;)), aes(y = document, x = pred, colour = author, shape = author)) + geom_ref_line(v = 0) + geom_point() + scale_y_continuous(breaks = seq(10, 80, by = 10), minor_breaks = seq(5, 80, by = 5)) + scale_color_manual(values = c(&quot;Madison&quot; = &quot;blue&quot;, &quot;Hamilton&quot; = &quot;red&quot;, &quot;Disputed&quot; = &quot;black&quot;)) + scale_shape_manual(values = c(&quot;Madison&quot; = 16, &quot;Hamilton&quot; = 15, &quot;Disputed&quot; = 17)) + labs(colour = &quot;Author&quot;, shape = &quot;Author&quot;, x = &quot;Federalist Papers&quot;, y = &quot;Predicted values&quot;) 6.3 Network Data Network data is area for which the tidyverse is not well suited. The igraph, sna, and network packages are the best in class. See the Social Network Analysis section of the Social Sciences Task View. See this tutorial by Katherin Ognyanova, Static and dynamic nework visualization with R, for a good overview of network visualization with those packages in R. There are several packages that plot networks in ggplot2. ggnetwork ggraph geomnet GGally functions ggnet, ggnet2, and ggnetworkmap. ggCompNet compares the speed of various network plotting packages in R. See this presentation for an overview of some of those packages for data visualization. Examples: Network Visualization Examples with the ggplot2 Package library(&quot;igraph&quot;) 6.3.1 Twitter Following Network Original: twitter &lt;- read.csv(&quot;twitter-following.csv&quot;) senator &lt;- read.csv(&quot;twitter-senator.csv&quot;) tidyverse: twitter &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;twitter-following.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; following = col_character(), #&gt; followed = col_character() #&gt; ) senator &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;twitter-senator.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; screen_name = col_character(), #&gt; name = col_character(), #&gt; party = col_character(), #&gt; state = col_character() #&gt; ) Original: n &lt;- nrow(senator) # number of senators ## initialize adjacency matrix twitter.adj &lt;- matrix(0, nrow = n, ncol = n) ## assign screen names to rows and columns colnames(twitter.adj) &lt;- rownames(twitter.adj) &lt;- senator$screen_name ## change `0&#39; to `1&#39; when edge goes from node `i&#39; to node `j&#39; for (i in 1:nrow(twitter)) { twitter.adj[twitter$following[i], twitter$followed[i]] &lt;- 1 } twitter.adj &lt;- graph.adjacency(twitter.adj, mode = &quot;directed&quot;, diag = FALSE) tidyverse: Simply use the function since twitter consists of edges (a link from a senator to another). SInce graph_from_edgelist expects a matrix, convert the data frame to a matrix using . twitter_adj &lt;- graph_from_edgelist(as.matrix(twitter)) original: senator$indegree &lt;- degree(twitter.adj, mode = &quot;in&quot;) senator$outdegree &lt;- degree(twitter.adj, mode = &quot;out&quot;) in.order &lt;- order(senator$indegree, decreasing = TRUE) out.order &lt;- order(senator$outdegree, decreasing = TRUE) ## 3 greatest indegree senator[in.order[1:3], ] ## 3 greatest outdegree senator[out.order[1:3], ] tidyverse: Add in- and out-degree varibles to the senator data frame: senator &lt;- mutate(senator, indegree = degree(twitter_adj, mode = &quot;in&quot;), outdegree = degree(twitter_adj, mode = &quot;out&quot;)) Now find the senators with the 3 greatest in-degrees arrange(senator, desc(indegree)) %&gt;% slice(1:3) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 3 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tom Cotton R AR 64 15 #&gt; 2 Richard J. Durbin D IL 60 87 #&gt; 3 John Barrasso R WY 58 79 or using the function: top_n(senator, 3, indegree) %&gt;% arrange(desc(indegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 5 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tom Cotton R AR 64 15 #&gt; 2 Richard J. Durbin D IL 60 87 #&gt; 3 John Barrasso R WY 58 79 #&gt; 4 Joe Donnelly D IN 58 9 #&gt; 5 Orrin G. Hatch R UT 58 50 The top_n function catches that three senators are tied for 3rd highest outdegree, whereas the simply sorting and slicing cannot. And we can find the senators with the three highest out-degrees similarly, top_n(senator, 3, outdegree) %&gt;% arrange(desc(outdegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 4 × 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Thad Cochran R MS 55 89 #&gt; 2 Steve Daines R MT 30 88 #&gt; 3 John McCain R AZ 41 88 #&gt; 4 Joe Manchin, III D WV 43 88 original: n &lt;- nrow(senator) ## color: Democrats = `blue&#39;, Republicans = `red&#39;, Independent = `black&#39; col &lt;- rep(&quot;red&quot;, n) col[senator$party == &quot;D&quot;] &lt;- &quot;blue&quot; col[senator$party == &quot;I&quot;] &lt;- &quot;black&quot; ## pch: Democrats = circle, Republicans = diamond, Independent = cross pch &lt;- rep(16, n) pch[senator$party == &quot;D&quot;] &lt;- 17 pch[senator$party == &quot;I&quot;] &lt;- 4 par(cex = 1.25) ## plot for comparing two closeness measures (incoming vs. outgoing) plot(closeness(twitter.adj, mode = &quot;in&quot;), closeness(twitter.adj, mode = &quot;out&quot;), pch = pch, col = col, main = &quot;Closeness&quot;, xlab = &quot;Incoming path&quot;, ylab = &quot;Outgoing path&quot;) ## plot for comparing directed and undirected betweenness plot(betweenness(twitter.adj, directed = TRUE), betweenness(twitter.adj, directed = FALSE), pch = pch, col = col, main = &quot;Betweenness&quot;, xlab = &quot;Directed&quot;, ylab = &quot;Undirected&quot;) tidyverse # Define scales to reuse for the plots scale_colour_parties &lt;- scale_colour_manual(&quot;Party&quot;, values = c(R = &quot;red&quot;, D = &quot;blue&quot;, I = &quot;green&quot;)) scale_shape_parties &lt;- scale_shape_manual(&quot;Party&quot;, values = c(R = 16, D = 17, I = 4)) senator %&gt;% mutate(closeness_in = closeness(twitter_adj, mode = &quot;in&quot;), closeness_out = closeness(twitter_adj, mode = &quot;out&quot;)) %&gt;% ggplot(aes(x = closeness_in, y = closeness_out, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + labs(main = &quot;Closeness&quot;, x = &quot;Incoming path&quot;, y = &quot;Outgoing path&quot;) What does the reference line indicate? What does that say about senators twitter networks? senator %&gt;% mutate(betweenness_dir = betweenness(twitter_adj, directed = TRUE), betweenness_undir = betweenness(twitter_adj, directed = FALSE)) %&gt;% ggplot(aes(x = betweenness_dir, y = betweenness_undir, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + coord_fixed() + labs(main = &quot;Betweenness&quot;, x = &quot;Directed&quot;, y = &quot;Undirected&quot;) We’ve covered three different methods of calculating the importance of a node in a network: degree, closeness, and centrality. But what do they mean? What’s the “best” measure of importance? The answer to the the former is “it depends on the question”. There are probably other papers out there on this, but Borgatti (2005) is a good discussion: Borgatti, Stephen. 2005. “Centrality and Network Flow”. Social Networks. DOI Original: senator$pagerank &lt;- page.rank(twitter.adj)$vector par(cex = 1.25) ## `col&#39; parameter is defined earlier plot(twitter.adj, vertex.size = senator$pagerank * 1000, vertex.color = col, vertex.label = NA, edge.arrow.size = 0.1, edge.width = 0.5) Add and plot page-rank: senator &lt;- mutate(senator, page_rank = page_rank(twitter_adj)[[&quot;vector&quot;]]) library(&quot;GGally&quot;) #&gt; #&gt; Attaching package: &#39;GGally&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; nasa library(&quot;intergraph&quot;) ggnet(twitter_adj, mode = &quot;target&quot;) #&gt; Loading required package: network #&gt; network: Classes for Relational Data #&gt; Version 1.13.0 created on 2015-08-31. #&gt; copyright (c) 2005, Carter T. Butts, University of California-Irvine #&gt; Mark S. Handcock, University of California -- Los Angeles #&gt; David R. Hunter, Penn State University #&gt; Martina Morris, University of Washington #&gt; Skye Bender-deMoll, University of Washington #&gt; For citation information, type citation(&quot;network&quot;). #&gt; Type help(&quot;network-package&quot;) to get started. #&gt; #&gt; Attaching package: &#39;network&#39; #&gt; The following objects are masked from &#39;package:igraph&#39;: #&gt; #&gt; %c%, %s%, add.edges, add.vertices, delete.edges, #&gt; delete.vertices, get.edge.attribute, get.edges, #&gt; get.vertex.attribute, is.bipartite, is.directed, #&gt; list.edge.attributes, list.vertex.attributes, #&gt; set.edge.attribute, set.vertex.attribute #&gt; Loading required package: sna #&gt; Loading required package: statnet.common #&gt; sna: Tools for Social Network Analysis #&gt; Version 2.4 created on 2016-07-23. #&gt; copyright (c) 2005, Carter T. Butts, University of California-Irvine #&gt; For citation information, type citation(&quot;sna&quot;). #&gt; Type help(package=&quot;sna&quot;) to get started. #&gt; #&gt; Attaching package: &#39;sna&#39; #&gt; The following objects are masked from &#39;package:igraph&#39;: #&gt; #&gt; betweenness, bonpow, closeness, components, degree, #&gt; dyad.census, evcent, hierarchy, is.connected, neighborhood, #&gt; triad.census #&gt; Loading required package: scales #&gt; #&gt; Attaching package: &#39;scales&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; discard #&gt; The following objects are masked from &#39;package:readr&#39;: #&gt; #&gt; col_factor, col_numeric 6.4 Spatial Data in R Sources: ggplot2 has several map-related functions borders fortify.map map_data ggmap allows ggplot to us a map from Google Maps, OpenStreet Maps or similar as a background for the plot. David Kahle and Hadley Wickham. 2013. ggmap: Spatial Visualization with ggplot2. Journal of Statistical Software Github dkahle/ggmamp tmap is not built on ggplot2 but uses a ggplot2-like API for network data. leaflet is an R interface to a popular javascript mapping library. Here are few tutorials on plotting spatial data in ggplot2: http://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html https://www.r-bloggers.com/r-beginners-plotting-locations-on-to-a-world-map/ https://rpubs.com/m_dev/Intro-to-Spatial-Data-and-ggplot2 library(&quot;ggrepel&quot;) data(&quot;us.cities&quot;, package = &quot;maps&quot;) glimpse(us.cities) #&gt; Observations: 1,005 #&gt; Variables: 6 #&gt; $ name &lt;chr&gt; &quot;Abilene TX&quot;, &quot;Akron OH&quot;, &quot;Alameda CA&quot;, &quot;Albany GA... #&gt; $ country.etc &lt;chr&gt; &quot;TX&quot;, &quot;OH&quot;, &quot;CA&quot;, &quot;GA&quot;, &quot;NY&quot;, &quot;OR&quot;, &quot;NM&quot;, &quot;LA&quot;, &quot;V... #&gt; $ pop &lt;int&gt; 113888, 206634, 70069, 75510, 93576, 45535, 494962... #&gt; $ lat &lt;dbl&gt; 32.5, 41.1, 37.8, 31.6, 42.7, 44.6, 35.1, 31.3, 38... #&gt; $ long &lt;dbl&gt; -99.7, -81.5, -122.3, -84.2, -73.8, -123.1, -106.6... #&gt; $ capital &lt;int&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... usa_map &lt;- map_data(&quot;usa&quot;) capitals &lt;- filter(us.cities, capital == 2, !country.etc %in% c(&quot;HI&quot;, &quot;AK&quot;)) ggplot() + geom_map(map = usa_map) + borders(database = &quot;usa&quot;) + geom_point(aes(x = long, y = lat, size = pop), data = capitals) + # scale size area ensures: 0 = no area scale_size_area() + coord_quickmap() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;US State Capitals&quot;, size = &quot;Population&quot;) cal_cities &lt;- filter(us.cities, country.etc == &quot;CA&quot;) %&gt;% top_n(7, pop) ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;) + geom_point(aes(x = long, y = lat), data = cal_cities) + geom_text_repel(aes(x = long, y = lat, label = name), data = cal_cities) + coord_quickmap() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) 6.4.1 Colors in R For more resources on using colors in R R4DS chapter Graphics for Communication ggplot2 book Chapter “Scales” Jenny Bryan Using colors in R Achim Zeileis, Kurt Hornik, Paul Murrell (2009). Escaping RGBland: Selecting Colors for Statistical Graphics. Computational Statistics &amp; Data Analysis DOI colorspace vignette Maureen Stone Choosing Colors for Data Visualization ColorBrewer A website with a variety of palettes, primarily designed for maps, but also useful in data viz. Stephen Few Practical Rules for Using Color in Charts Why Should Engineers and Scientists by Worried About Color? A Better Default Colormap for Matplotlib A SciPy 2015 talk that describes how the viridis was created. Evaluation of Artery Visualizations for Heart Disease Diagnosis Using the wrong color scale can be deadly … literally. The python package matplotlib has a good discussion of colormaps. Use scale_identity for the color and alpha scales since the values of the variables are the values of the scale itself (the color names, and the alpha values). ggplot(tibble(x = rep(1:4, each = 2), y = x + rep(c(0, 0.2), times = 2), colour = rep(c(&quot;black&quot;, &quot;red&quot;), each = 4), alpha = c(1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5)), aes(x = x, y = y, colour = colour, alpha = alpha)) + geom_point(size = 15) + scale_color_identity() + scale_alpha_identity() + theme_bw() + theme(panel.grid = element_blank()) 6.4.2 United States Presidential Elections Original: pres08 &lt;- read.csv(&quot;pres08.csv&quot;) ## two-party vote share pres08$Dem &lt;- pres08$Obama / (pres08$Obama + pres08$McCain) pres08$Rep &lt;- pres08$McCain / (pres08$Obama + pres08$McCain) ## color for California cal.color &lt;- rgb(red = pres08$Rep[pres08$state == &quot;CA&quot;], blue = pres08$Dem[pres08$state == &quot;CA&quot;], green = 0) tidyverse: pres08 &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;pres08.csv&quot;)) %&gt;% mutate(Dem = Obama / (Obama + McCain), Rep = McCain / (Obama + McCain)) #&gt; Parsed with column specification: #&gt; cols( #&gt; state.name = col_character(), #&gt; state = col_character(), #&gt; Obama = col_integer(), #&gt; McCain = col_integer(), #&gt; EV = col_integer() #&gt; ) Original: ## California as a blue state map(database = &quot;state&quot;, regions = &quot;California&quot;, col = &quot;blue&quot;, fill = TRUE) ## California as a purple state map(database = &quot;state&quot;, regions = &quot;California&quot;, col = cal.color, fill = TRUE) tidyverse: ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = &quot;blue&quot;) + coord_quickmap() + theme_minimal() cal_color &lt;- filter(pres08, state == &quot;CA&quot;) %&gt;% {rgb(red = .$Rep, green = 0, blue = .$Dem)} ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = cal_color) + coord_quickmap() + theme_minimal() # America as red and blue states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ((pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = ifelse(pres08$Rep[i] &gt; pres08$Dem[i], &quot;red&quot;, &quot;blue&quot;), fill = TRUE, add = TRUE) } } ## America as purple states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ((pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = rgb(red = pres08$Rep[i], blue = pres08$Dem[i], green = 0), fill = TRUE, add = TRUE) } } states &lt;- map_data(&quot;state&quot;) %&gt;% left_join(mutate(pres08, state.name = str_to_lower(state.name)), by = c(&quot;region&quot; = &quot;state.name&quot;)) %&gt;% # drops DC filter(!is.na(EV)) %&gt;% mutate(party = if_else(Dem &gt; Rep, &quot;Dem&quot;, &quot;Rep&quot;), color = map2_chr(Dem, Rep, ~ rgb(blue = .x, red = .y, green = 0))) ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = party)) + coord_quickmap() + scale_fill_manual(values = c(&quot;Rep&quot; = &quot;red&quot;, &quot;Dem&quot; = &quot;blue&quot;)) + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) For plotting the purple states, I use scale_fill_identity since the color column contains the RGB values to use in the plot: ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = color)) + coord_quickmap() + scale_fill_identity() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) 6.4.3 Expansion of Walmart Original: walmart &lt;- read.csv(&quot;walmart.csv&quot;) ## red = WalMartStore, blue = SuperCenter, green = DistributionCenter walmart$storecolors &lt;- NA # create an empty vector walmart$storecolors[walmart$type == &quot;Wal-MartStore&quot;] &lt;- rgb(red = 1, green = 0, blue = 0, alpha = 1/3) walmart$storecolors[walmart$type == &quot;SuperCenter&quot;] &lt;- rgb(red = 0, green = 0, blue = 1, alpha = 1/3) walmart$storecolors[walmart$type == &quot;DistributionCenter&quot;] &lt;- rgb(red = 0, green = 1, blue = 0, alpha = 1/3) ## larger circles for DistributionCenter walmart$storesize &lt;- ifelse(walmart$type == &quot;DistributionCenter&quot;, 1, 0.5) tidyverse We don’t need to do the direct mapping since walmart &lt;- read_csv(qss_data_url(&quot;discovery&quot;, &quot;walmart.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; opendate = col_date(format = &quot;&quot;), #&gt; st.address = col_character(), #&gt; city = col_character(), #&gt; state = col_character(), #&gt; long = col_double(), #&gt; lat = col_double(), #&gt; type = col_character() #&gt; ) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = mutate(walmart, size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)), alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + theme_minimal() We don’t need to worry about colors since ggplot handles that. To make a plot showing all Walmart stores opened up through that year, I write a function, that takes the year and dataset as parameters. Since I am calling the function for its side effect (printing the plot) rather than the value it returns, I use the walk function rather than map. See R for Data Science, Chapter 21.8: Walk for more information. map_walmart &lt;- function(year, .data) { .data &lt;- filter(.data, opendate &lt; make_date(year, 1, 1)) %&gt;% mutate(size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = .data, alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + theme_minimal() + ggtitle(year) } years &lt;- c(1975, 1985, 1995, 2005) walk(years, ~ print(map_walmart(.x, walmart))) 6.4.4 Animation in R For easy annimation with ggplot, use the gganimate package. Note that the gganimate package is not on CRAN, so you have to install it with the devtools package: install.packages(&quot;cowplot&quot;) devtools::install_github(&quot;dgrtwo/animate&quot;) The gganimate works by simply adding a new frame aesthetic. And then the function gg_animate will animate the plot. This creates a gif with store openings by year. I use frame = year(opendate) to have the animation use each year as a frame, and cumulative = TRUE so that the previous years are shown. library(&quot;gganimate&quot;) walmart_animated &lt;- ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, fill = type, frame = year(opendate), cumulative = TRUE), data = walmart) + coord_quickmap() + theme_minimal() gganimate(walmart_animated) "],
["probability.html", "7 Probability 7.1 Prerequisites 7.2 Probability 7.3 Conditional Probability 7.4 Large Sample Theorems", " 7 Probability 7.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;stringr&quot;) 7.2 Probability Original code k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse birthday &lt;- function(k) { logdenom &lt;- k * log(365) + lfactorial(365 - k) lognumer &lt;- lfactorial(365) pr &lt;- 1 - exp(lognumer - logdenom) pr } bday &lt;- tibble(k = 1:50, pr = birthday(k)) ggplot(bday, aes(x = k , y = pr)) + geom_hline(yintercept = 0.5, colour = &quot;white&quot;, size = 2) + geom_point() + scale_y_continuous(&quot;Probability that at least two\\n people have the same birthday&quot;, limits = c(0, 1)) + labs(x = &quot;Number of people&quot;) Note: The logarithm is used for numerical stability. Basically, “floating-point” numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues. See “Falling Into the Floating Point Trap” in The R Inforno for a summary of floating point numbers. See these John Fox posts 1 2 for an example of numerical stability gone wrong. Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/. 7.2.1 Sampling without replacement Original code: k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse code: Instead of using a for loop, we could do the simulations using a functional as described in R for Data Science. Define the function sim_bdays to randomly sample k birthdays, and returns TRUE if there are any duplicates. sim_bdays &lt;- function(k) { days &lt;- sample(1:365, k, replace = TRUE) length(unique(days)) &lt; k } Set the parameters for 1,000 simulations, and 23 individuals. We use map_lgl since sim_bdays returns a logical value (TRUE, FALSE): sims &lt;- 1000 k &lt;- 23 map_lgl(seq_len(sims), ~ sim_bdays(k)) %&gt;% mean() #&gt; [1] 0.486 7.2.2 Combinations Original code choose(84, 6) #&gt; [1] 4.06e+08 7.3 Conditional Probability 7.3.1 Conditional, Marginal, and Joint Probabilities Original: FLVoters &lt;- read.csv(&quot;FLVoters.csv&quot;) dim(FLVoters) # before removal of missing data ## [1] 10000 6 FLVoters &lt;- na.omit(FLVoters) dim(FLVoters) # after removal tidyverse FLVoters &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLVoters.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; county = col_integer(), #&gt; VTD = col_integer(), #&gt; age = col_integer(), #&gt; gender = col_character(), #&gt; race = col_character() #&gt; ) dim(FLVoters) #&gt; [1] 10000 6 FLVoters &lt;- FLVoters %&gt;% na.omit() original: margin.race &lt;- prop.table(table(FLVoters$race)) margin.race margin.gender &lt;- prop.table(table(FLVoters$gender)) margin.gender tidyverse: Instead of using prop.base, we calculate the probabilities with a data frame. Marginal probabilities of race, margin_race &lt;- FLVoters %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) margin_race #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 175 0.01920 #&gt; 2 black 1194 0.13102 #&gt; 3 hispanic 1192 0.13080 #&gt; 4 native 29 0.00318 #&gt; 5 other 310 0.03402 #&gt; 6 white 6213 0.68177 Marginal probabilities of gender: margin_gender &lt;- FLVoters %&gt;% count(gender) %&gt;% mutate(prop = n / sum(n)) margin_gender #&gt; # A tibble: 2 × 3 #&gt; gender n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f 4883 0.536 #&gt; 2 m 4230 0.464 Original: prop.table(table(FLVoters$race[FLVoters$gender == &quot;f&quot;])) tidyverse: FLVoters %&gt;% filter(gender == &quot;f&quot;) %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 83 0.01700 #&gt; 2 black 678 0.13885 #&gt; 3 hispanic 666 0.13639 #&gt; 4 native 17 0.00348 #&gt; 5 other 158 0.03236 #&gt; 6 white 3281 0.67192 Original: joint.p &lt;- prop.table(table(race = FLVoters$race, gender = FLVoters$gender)) joint.p #&gt; gender #&gt; race f m #&gt; asian 0.00911 0.01010 #&gt; black 0.07440 0.05662 #&gt; hispanic 0.07308 0.05772 #&gt; native 0.00187 0.00132 #&gt; other 0.01734 0.01668 #&gt; white 0.36004 0.32174 tidyverse: joint_p &lt;- FLVoters %&gt;% count(gender, race) %&gt;% # needed because it is still grouped by gender ungroup() %&gt;% mutate(prop = n / sum(n)) joint_p #&gt; # A tibble: 12 × 4 #&gt; gender race n prop #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f asian 83 0.00911 #&gt; 2 f black 678 0.07440 #&gt; 3 f hispanic 666 0.07308 #&gt; 4 f native 17 0.00187 #&gt; 5 f other 158 0.01734 #&gt; 6 f white 3281 0.36004 #&gt; # ... with 6 more rows We can convert the data frame to have gender as columns: joint_p %&gt;% ungroup() %&gt;% select(-n) %&gt;% spread(gender, prop) #&gt; # A tibble: 6 × 3 #&gt; race f m #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian 0.00911 0.01010 #&gt; 2 black 0.07440 0.05662 #&gt; 3 hispanic 0.07308 0.05772 #&gt; 4 native 0.00187 0.00132 #&gt; 5 other 0.01734 0.01668 #&gt; 6 white 0.36004 0.32174 Original: rowSums(joint.p) tidyverse: Sum over race: joint_p %&gt;% group_by(race) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 6 × 2 #&gt; race prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 asian 0.01920 #&gt; 2 black 0.13102 #&gt; 3 hispanic 0.13080 #&gt; 4 native 0.00318 #&gt; 5 other 0.03402 #&gt; 6 white 0.68177 Original: colSums(joint.p) tidyverse: Sum over gender joint_p %&gt;% group_by(gender) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 2 × 2 #&gt; gender prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 f 0.536 #&gt; 2 m 0.464 Original: FLVoters$age.group &lt;- NA # initialize a variable FLVoters$age.group[FLVoters$age &lt;= 20] &lt;- 1 FLVoters$age.group[FLVoters$age &gt; 20 &amp; FLVoters$age &lt;= 40] &lt;- 2 FLVoters$age.group[FLVoters$age &gt; 40 &amp; FLVoters$age &lt;= 60] &lt;- 3 FLVoters$age.group[FLVoters$age &gt; 60] &lt;- 4 tidyverse: Use the cut FLVoters &lt;- FLVoters %&gt;% mutate(age_group = cut(age, c(0, 20, 40, 60, Inf), right = TRUE, labels = c(&quot;&lt;= 20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;&gt; 60&quot;))) Original: joint3 &lt;- prop.table(table(race = FLVoters$race, age.group = FLVoters$age.group, gender = FLVoters$gender)) joint3 tidyverse: joint3 &lt;- FLVoters %&gt;% count(race, age_group, gender) %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) joint3 #&gt; # A tibble: 47 × 5 #&gt; race age_group gender n prop #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian &lt;= 20 f 1 0.000110 #&gt; 2 asian &lt;= 20 m 2 0.000219 #&gt; 3 asian 20-40 f 24 0.002634 #&gt; 4 asian 20-40 m 26 0.002853 #&gt; 5 asian 40-60 f 38 0.004170 #&gt; 6 asian 40-60 m 47 0.005157 #&gt; # ... with 41 more rows original: margin.age &lt;- prop.table(table(FLVoters$age.group)) margin.age joint3[&quot;black&quot;, 4, &quot;f&quot;] / margin.age[4] tidyverse: Marginal probabilities by age groups margin_age &lt;- FLVoters %&gt;% count(age_group) %&gt;% mutate(prop = n / sum(n)) margin_age #&gt; # A tibble: 4 × 3 #&gt; age_group n prop #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 161 0.0177 #&gt; 2 20-40 2469 0.2709 #&gt; 3 40-60 3285 0.3605 #&gt; 4 &gt; 60 3198 0.3509 Calculate the probabilities that each group is in a given age group, and show $P( | ) left_join(joint3, select(margin_age, age_group, margin_age = prop), by = &quot;age_group&quot;) %&gt;% mutate(prob_age_group = prop / margin_age) %&gt;% filter(race == &quot;black&quot;, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;) %&gt;% select(race, age_group, gender, prob_age_group) #&gt; # A tibble: 1 × 4 #&gt; race age_group gender prob_age_group #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black &gt; 60 f 0.0538 Original: joint2 &lt;- prop.table(table(age.group = FLVoters$age.group, gender = FLVoters$gender)) joint2 joint2[4, &quot;f&quot;] joint3[&quot;black&quot;, 4, &quot;f&quot;] / joint2[4, &quot;f&quot;] tidyverse: Two-way joint probability table for age group and gender joint2 &lt;- FLVoters %&gt;% count(age_group, gender) %&gt;% ungroup() %&gt;% mutate(prob_age_gender = n / sum(n)) joint2 #&gt; # A tibble: 8 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 f 88 0.00966 #&gt; 2 &lt;= 20 m 73 0.00801 #&gt; 3 20-40 f 1304 0.14309 #&gt; 4 20-40 m 1165 0.12784 #&gt; 5 40-60 f 1730 0.18984 #&gt; 6 40-60 m 1555 0.17064 #&gt; # ... with 2 more rows The joint probability \\(P(\\text{age} &gt; 60 \\land \\text{female})\\), filter(joint2, age_group == &quot;&gt; 60&quot;, gender == &quot;f&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f 1761 0.193 The onditional probablities \\(P(race | gender, age)\\), condprob_race &lt;- left_join(joint3, select(joint2, -n), by = c(&quot;age_group&quot;, &quot;gender&quot;)) %&gt;% mutate(prob_race = prop / prob_age_gender) %&gt;% arrange(age_group, gender) %&gt;% select(age_group, gender, race, prob_race) Each row is the \\(P(race | age_group, gender)\\), so \\(P(\\text{black} | \\text{female} \\land \\text{age} &gt; 60)\\), filter(condprob_race, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;, race == &quot;black&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender race prob_race #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f black 0.0977 7.3.2 Independence Original code plot(c(margin.race * margin.gender[&quot;f&quot;]), # product of marginal probs. c(joint.p[, &quot;f&quot;]), # joint probabilities xlim = c(0, 0.4), ylim = c(0, 0.4), xlab = &quot;P(race) * P(female)&quot;, ylab = &quot;P(race and female)&quot;) abline(0, 1) # 45 degree line Tidyverse Create a table with the products of margins of race and age. Using the function crossing to create a tibble with all combinations of race and gender and the indep prob race_gender_indep &lt;- crossing(select(margin_race, race, prob_race = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(prob_indep = prob_race * prob_gender) %&gt;% left_join(select(joint_p, gender, race, prob = prop), by = c(&quot;gender&quot;, &quot;race&quot;)) %&gt;% select(race, gender, everything()) race_gender_indep #&gt; # A tibble: 12 × 6 #&gt; race gender prob_race prob_gender prob_indep prob #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian f 0.0192 0.536 0.01029 0.00911 #&gt; 2 asian m 0.0192 0.464 0.00891 0.01010 #&gt; 3 black f 0.1310 0.536 0.07021 0.07440 #&gt; 4 black m 0.1310 0.464 0.06082 0.05662 #&gt; 5 hispanic f 0.1308 0.536 0.07009 0.07308 #&gt; 6 hispanic m 0.1308 0.464 0.06071 0.05772 #&gt; # ... with 6 more rows ggplot(race_gender_indep, aes(x = prob_indep, y = prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(. ~ gender) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(x = expression(P(&quot;race&quot;) * P(&quot;gender&quot;)), y = expression(P(&quot;race and gender&quot;))) Original code ## joint independence plot(c(joint3[, 4, &quot;f&quot;]), # joint probability margin.race * margin.age[4] * margin.gender[&quot;f&quot;], # product of marginals xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;joint independence&quot;, xlab = &quot;P(race and above 60 and female)&quot;, ylab = &quot;P(race) * P(above 60) * P(female)&quot;) abline(0, 1) ## conditional independence given female plot(c(joint3[, 4, &quot;f&quot;]) / margin.gender[&quot;f&quot;], # joint prob. given female ## product of marginals (joint.p[, &quot;f&quot;] / margin.gender[&quot;f&quot;]) * (joint2[4, &quot;f&quot;] / margin.gender[&quot;f&quot;]), xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;marginal independence&quot;, xlab = &quot;P(race and above 60 | female)&quot;, ylab = &quot;P(race | female) * P(above 60 | female)&quot;) abline(0, 1) While the original code only calculates joint-independence value for values of age &gt; 60, and female, this calculates the joint probabilities for all combinations of the three variables, and facets by age and gender. joint_indep &lt;- # all combinations of race, age, gender crossing(select(margin_race, race, prob_race = prop), select(margin_age, age_group, prob_age = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(indep_prob = prob_race * prob_age * prob_gender) %&gt;% left_join(select(joint3, race, age_group, gender, prob = prop), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% replace_na(list(prob = 0)) ggplot(joint_indep, aes(x = prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age and gender)&quot;, y = &quot;P(race) * P(age) * P(gender)&quot;, title = &quot;Joint Independence&quot;) The original code only calculates the conditional independence given female; this calculates conditional independence for all values of gender: cond_gender &lt;- left_join(select(joint3, race, age_group, gender, joint_prob = prop), select(margin_gender, gender, prob_gender = prop), by = c(&quot;gender&quot;)) %&gt;% mutate(cond_prob = joint_prob / prob_gender) # P(race | gender) prob_race_gender &lt;- left_join(select(joint_p, race, gender, prob_race_gender = prop), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_race = prob_race_gender / prob_gender) # P(age | gender) prob_age_gender &lt;- left_join(select(joint2, age_group, gender, prob_age_gender), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_age = prob_age_gender / prob_gender) # indep prob of race and age indep_cond_gender &lt;- full_join(select(prob_race_gender, race, gender, prob_race), select(prob_age_gender, age_group, gender, prob_age), by = &quot;gender&quot;) %&gt;% mutate(indep_prob = prob_race * prob_age) inner_join(select(indep_cond_gender, race, age_group, gender, indep_prob), select(cond_gender, race, age_group, gender, cond_prob), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% ggplot(aes(x = cond_prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age | gender)&quot;, y = &quot;P(race | gender) * P(age | gender)&quot;, title = &quot;Marginal independence&quot;) Orginal code for the Monty-Hall problem sims &lt;- 1000 doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) result.switch &lt;- result.noswitch &lt;- rep(NA, sims) for (i in 1:sims) { ## randomly choose the initial door first &lt;- sample(1:3, size = 1) result.noswitch[i] &lt;- doors[first] remain &lt;- doors[-first] # remaining two doors ## Monty chooses one door with a goat monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) result.switch[i] &lt;- remain[-monty] } mean(result.noswitch == &quot;car&quot;) This code doesn’t rely on any non-tidyverse code, but the following is another way to write it. First, create a function for a single iteration. This will save the results as data frame. choose_door &lt;- function(.id) { # put doors inside the function to ensure it doesn&#39;t get changed doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) first &lt;- sample(1:3, 1) remain &lt;- doors[-first] monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) ret &lt;- tribble(~strategy, ~result, &quot;no switch&quot;, doors[first], &quot;switch&quot;, remain[-monty]) ret[[&quot;.id&quot;]] &lt;- .id ret } Now use map_df to run choose_door multiple times, and then summarize the results: sims &lt;- 1000 results &lt;- map_df(seq_len(sims), choose_door) results %&gt;% group_by(strategy) %&gt;% summarise(pct_win = mean(result == &quot;car&quot;)) #&gt; # A tibble: 2 × 2 #&gt; strategy pct_win #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 no switch 0.35 #&gt; 2 switch 0.65 Can we make this even more general? What about a function that takes a data frame as its input with the choices? … 7.3.3 Predicting Race Using Surname and Residence Location Start with the Census names files: cnames &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;names.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; count = col_integer(), #&gt; pctwhite = col_double(), #&gt; pctblack = col_double(), #&gt; pctapi = col_double(), #&gt; pcthispanic = col_double(), #&gt; pctothers = col_double() #&gt; ) glimpse(cnames) #&gt; Observations: 151,671 #&gt; Variables: 7 #&gt; $ surname &lt;chr&gt; &quot;SMITH&quot;, &quot;JOHNSON&quot;, &quot;WILLIAMS&quot;, &quot;BROWN&quot;, &quot;JONES&quot;, ... #&gt; $ count &lt;int&gt; 2376206, 1857160, 1534042, 1380145, 1362755, 11278... #&gt; $ pctwhite &lt;dbl&gt; 73.34, 61.55, 48.52, 60.72, 57.69, 85.80, 64.73, 6... #&gt; $ pctblack &lt;dbl&gt; 22.22, 33.80, 46.72, 34.54, 37.73, 10.41, 30.77, 0... #&gt; $ pctapi &lt;dbl&gt; 0.40, 0.42, 0.37, 0.41, 0.35, 0.42, 0.40, 1.43, 0.... #&gt; $ pcthispanic &lt;dbl&gt; 1.56, 1.50, 1.60, 1.64, 1.44, 1.43, 1.58, 90.82, 9... #&gt; $ pctothers &lt;dbl&gt; 2.48, 2.73, 2.79, 2.69, 2.79, 1.94, 2.52, 1.09, 0.... For each surname, contains variables with the probability that it belongs to an individual of a given race (pctwhite, pctblack, …). We want to find the most-likely race for a given surname, by finding the race with the maximum proportion. Instead of dealing with multiple variables, it is easier to use max on a single variable, so we will rearrange the data to in order to use a grouped summarise, and then merge the new variable back to the original datasets. Note, this code takes a while. most_likely &lt;- cnames %&gt;% select(-count) %&gt;% gather(race_pred, pct, -surname) %&gt;% # remove pct prefix mutate(race_pred = str_replace(race_pred, &quot;^pct&quot;, &quot;&quot;)) %&gt;% # group by surname group_by(surname) %&gt;% filter(pct == max(pct)) %&gt;% # if there are ties, keep only one obs slice(1) %&gt;% # don&#39;t need pct anymore select(-pct) %&gt;% mutate(race_pred = factor(race_pred), # need to convert to factor first, else errors race_pred = fct_recode(race_pred, &quot;asian&quot; = &quot;api&quot;, &quot;other&quot; = &quot;others&quot;)) Now merge that back to the original cnames cnames &lt;- left_join(cnames, most_likely, by = &quot;surname&quot;) Intead of using match, use inner_join to merge the surnames to FLVoters FLVoters &lt;- inner_join(FLVoters, cnames, by = &quot;surname&quot;) dim(FLVoters) #&gt; [1] 8022 14 FLVoters also includes a “native” category that the surname dataset does not. FLVoters &lt;- FLVoters %&gt;% mutate(race2 = fct_recode(race, other = &quot;native&quot;)) Check that the levels of race and race_pred are the same: FLVoters %&gt;% count(race2) #&gt; # A tibble: 5 × 2 #&gt; race2 n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 asian 140 #&gt; 2 black 1078 #&gt; 3 hispanic 1023 #&gt; 4 other 277 #&gt; 5 white 5504 FLVoters %&gt;% count(race_pred) #&gt; # A tibble: 5 × 2 #&gt; race_pred n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 white 6516 #&gt; 2 black 258 #&gt; 3 hispanic 1121 #&gt; 4 asian 120 #&gt; 5 other 7 Now we can calculate True positive rate for all races FLVoters %&gt;% group_by(race2) %&gt;% summarise(tp = mean(race2 == race_pred)) %&gt;% arrange(desc(tp)) #&gt; # A tibble: 5 × 2 #&gt; race2 tp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 white 0.95022 #&gt; 2 hispanic 0.84653 #&gt; 3 asian 0.56429 #&gt; 4 black 0.16048 #&gt; 5 other 0.00361 and the False discovery rate for all races, FLVoters %&gt;% group_by(race_pred) %&gt;% summarise(fp = mean(race2 != race_pred)) %&gt;% arrange(desc(fp)) #&gt; # A tibble: 5 × 2 #&gt; race_pred fp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 other 0.857 #&gt; 2 asian 0.342 #&gt; 3 black 0.329 #&gt; 4 hispanic 0.227 #&gt; 5 white 0.197 Now add get residence data FLCensus &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLCensusVTD.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; county = col_integer(), #&gt; VTD = col_integer(), #&gt; total.pop = col_integer(), #&gt; white = col_double(), #&gt; black = col_double(), #&gt; hispanic = col_double(), #&gt; api = col_double(), #&gt; others = col_double() #&gt; ) \\(P(race)\\) in Florida: race.prop &lt;- FLCensus %&gt;% select(total.pop, white, black, api, hispanic, others) %&gt;% gather(race, pct, -total.pop) %&gt;% group_by(race) %&gt;% summarise(mean = weighted.mean(pct, weights = total.pop)) %&gt;% arrange(desc(mean)) race.prop #&gt; # A tibble: 5 × 2 #&gt; race mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 white 0.6045 #&gt; 2 hispanic 0.2128 #&gt; 3 black 0.1394 #&gt; 4 api 0.0219 #&gt; 5 others 0.0214 7.3.4 Predicting Election Outcomes with Uncertainty Original code pres08 &lt;- read.csv(&quot;pres08.csv&quot;) ## two-party vote share pres08$p &lt;- pres08$Obama / (pres08$Obama + pres08$McCain) Tidyverse code pres08 &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;pres08.csv&quot;)) %&gt;% mutate(p = Obama / (Obama + McCain)) #&gt; Parsed with column specification: #&gt; cols( #&gt; state.name = col_character(), #&gt; state = col_character(), #&gt; Obama = col_integer(), #&gt; McCain = col_integer(), #&gt; EV = col_integer() #&gt; ) Original code n.states &lt;- nrow(pres08) # number of states n &lt;- 1000 # number of respondents sims &lt;- 10000 # number of simulations ## Obama&#39;s electoral votes Obama.ev &lt;- rep(NA, sims) for (i in 1:sims) { ## samples number of votes for Obama in each state draws &lt;- rbinom(n.states, size = n, prob = pres08$p) ## sums state&#39;s electoral college votes if Obama wins the majority Obama.ev[i] &lt;- sum(pres08$EV[draws &gt; n/2]) } hist(Obama.ev, freq = FALSE, main = &quot;Prediction of Election Outcome&quot;, xlab = &quot;Obama&#39;s Electoral College Votes&quot;) abline(v = 364, col = &quot;red&quot;) # actual result Tidyverse code Write a function to simulate the elections. df is the data frame (pres08) with the state, EV, and p columns. n_draws is the size of the binomial distribution to draw from. .id is the simulation number. sim_election &lt;- function(.id, df, n_draws = 1000) { # For each state randomly sample mutate(df, draws = rbinom(n(), n_draws, p)) %&gt;% filter(draws &gt; (n_draws / 2)) %&gt;% summarise(EV = sum(EV), .id = .id) } Now simulate the election 10,000 times: sims &lt;- 10000 sim_results &lt;- map_df(seq_len(sims), ~ sim_election(.x, pres08, n_draws = 1000)) In the 2008 election, Obama received 364 electoral votes ELECTION_EV &lt;- 364 And plot them, ggplot(sim_results, aes(x = EV)) + geom_histogram(binwidth = 10) + geom_vline(xintercept = ELECTION_EV, colour = &quot;red&quot;, size = 2) Original code mean(Obama.ev) ## [1] 352.1646 ## probability of binomial random variable taking greater than n/2 votes sum(pres08$EV * pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE)) ## [1] 352.1388 ## approximate variance using Monte Carlo draws var(Obama.ev) ## [1] 268.7592 ## theoretical variance pres08$pb &lt;- pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE) V &lt;- sum(pres08$pb*(1-pres08$pb)*pres08$EV^2) V ## [1] 268.8008 ## approximate standard deviation using Monte Carlo draws sd(Obama.ev) ## [1] 16.39388 ## theoretical standard deviation sqrt(V) ## [1] 16.39515 tidyverse code Simulation mean, variance, and standard deviations: sim_results %&gt;% select(EV) %&gt;% summarise_all(funs(mean, var, sd)) #&gt; # A tibble: 1 × 3 #&gt; mean var sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 270 16.4 Theoretical probabilities # we cannot use n, because mutate will look for n() first. n_draws &lt;- 1000 pres08 %&gt;% mutate(pb = pbinom(n_draws / 2, size = n_draws, prob = p, lower.tail = FALSE)) %&gt;% summarise(mean = sum(pb * EV), V = sum(pb * (1 - pb) * EV ^ 2), sd = sqrt(V)) #&gt; # A tibble: 1 × 3 #&gt; mean V sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 269 16.4 7.4 Large Sample Theorems 7.4.1 Law of Large Numbers Original: sims &lt;- 1000 ## 3 separate simulations for each x.binom &lt;- rbinom(sims, p = 0.2, size = 10) ## computing sample mean with varying sample size mean.binom &lt;- cumsum(x.binom) / 1:sims Tidyverse: Put the simulation number, x and mean in a tibble: sims &lt;- 1000 p &lt;- 0.2 size = 10 lln_binom &lt;- tibble( n = seq_len(sims), x = rbinom(sims, p = p, size = size), mean = cumsum(x) / n, distrib = str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;)) Original: ## default runif() is uniform(0, 1) x.unif &lt;- runif(sims) mean.unif &lt;- cumsum(x.unif) / 1:sims Tidyverse: lln_unif &lt;- tibble(n = seq_len(sims), x = runif(sims), mean = cumsum(x) / n, distrib = str_c(&quot;Uniform(0, 1)&quot;)) Original: par(cex = 1.5) ## plot for binomial plot(1:sims, mean.binom, type = &quot;l&quot;, ylim = c(1, 3), xlab = &quot;Sample size&quot;, ylab = &quot;Sample mean&quot;, main = &quot;Binomial(10, 0.2)&quot;) abline(h = 2, lty = &quot;dashed&quot;) # expectation ## plot for uniform plot(1:sims, mean.unif, type = &quot;l&quot;, ylim = c(0, 1), xlab = &quot;Sample size&quot;, ylab = &quot;Sample mean&quot;, main = &quot;Uniform(0, 1)&quot;) abline(h = 0.5, lty = &quot;dashed&quot;) # expectation Tidyverse: true_means &lt;- tribble(~distrib, ~mean, &quot;Uniform(0, 1)&quot;, 0.5, str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;), size * p) ggplot() + geom_hline(aes(yintercept = mean), data = true_means, colour = &quot;white&quot;, size = 2) + geom_line(aes(x = n, y = mean), data = bind_rows(lln_binom, lln_unif)) + facet_grid(distrib ~ ., scales = &quot;free_y&quot;) + labs(x = &quot;Sample Size&quot;, y = &quot;Sample Mean&quot;) 7.4.2 Central Limit Theorem Original: par(cex = 1.5) ## sims = number of simulations n.samp &lt;- 1000 z.binom &lt;- z.unif &lt;- rep(NA, sims) for (i in 1:sims) { x &lt;- rbinom(n.samp, p = 0.2, size = 10) z.binom[i] &lt;- (mean(x) - 2) / sqrt(1.6 / n.samp) x &lt;- runif(n.samp, min = 0, max = 1) z.unif[i] &lt;- (mean(x) - 0.5) / sqrt(1 / (12 * n.samp)) } ## histograms; nclass specifies the number of bins hist(z.binom, freq = FALSE, nclass = 40, xlim = c(-4, 4), ylim = c(0, 0.6), xlab = &quot;z-score&quot;, main = &quot;Binomial(0.2, 10)&quot;) x &lt;- seq(from = -3, to = 3, by = 0.01) lines(x, dnorm(x)) # overlay the standard Normal PDF hist(z.unif, freq = FALSE, nclass = 40, xlim = c(-4, 4), ylim = c(0, 0.6), xlab = &quot;z-score&quot;, main = &quot;Uniform(0, 1)&quot;) lines(x, dnorm(x)) tidyverse: Instead of using a for loop, write functions to The population mean of the binomial distribution is \\(\\mu = p n\\) and the variance is \\(\\mu = p (1 - p) n\\). sims &lt;- 1000 n_samp &lt;- 1000 # Mean of binomial distribution binom_mean &lt;- function(size, p) { size * p } # Variance of binomial distribution binom_var &lt;- function(size, p) { size * p * (1 - p) } sim_binom_clt &lt;- function(n_samp, size, p) { x &lt;- rbinom(n_samp, prob = p, size = size) z &lt;- (mean(x) - binom_mean(size, p)) / sqrt(binom_var(size, p) / n_samp) tibble(distrib = str_c(&quot;Binomial(&quot;, p, &quot;, &quot;, size, &quot;)&quot;), z = z) } # Mean of uniform distribution unif_mean &lt;- function(min, max) { 0.5 * (min + max) } # Variance of uniform distribution unif_var &lt;- function(min, max) { (1 / 12) * (max - min) ^ 2 } sim_unif_clt &lt;- function(n_samp, min = 0, max = 1) { x &lt;- runif(n_samp, min = min, max = max) z &lt;- (mean(x) - unif_mean(min, max)) / sqrt(unif_var(min, max) / n_samp) tibble(distrib = str_c(&quot;Uniform(&quot;, min, &quot;, &quot;, max, &quot;)&quot;), z = z) } Since we will calculate this for n_samp = 1000 and n_samp, we might as well write a function for it. clt_plot &lt;- function(n_samp) { bind_rows(map_df(seq_len(sims), ~ sim_binom_clt(n_samp, size, p)), map_df(seq_len(sims), ~ sim_unif_clt(n_samp))) %&gt;% ggplot(aes(x = z)) + geom_density() + geom_rug() + stat_function(fun = dnorm, colour = &quot;red&quot;) + facet_grid(distrib ~ .) + ggtitle(str_c(&quot;Sample size = &quot;, n_samp)) } clt_plot(1000) clt_plot(100) "],
["uncertainty.html", "8 Uncertainty 8.1 Prerequisites 8.2 Estimation 8.3 Hypothesis Testing 8.4 Linear Regression Model with Uncertainty", " 8 Uncertainty 8.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;modelr&quot;) library(&quot;broom&quot;) 8.2 Estimation Original: n &lt;- 100 # sample size mu0 &lt;- 0 # mean of Y_i(0) sd0 &lt;- 1 # standard deviation of Y_i(0) mu1 &lt;- 1 # mean of Y_i(1) sd1 &lt;- 1 # standard deviation of Y_i(1) ## generate a sample Y0 &lt;- rnorm(n, mean = mu0, sd = sd0) Y1 &lt;- rnorm(n, mean = mu1, sd = sd1) tau &lt;- Y1 - Y0 # individual treatment effect ## true value of the sample average treatment effect SATE &lt;- mean(tau) SATE #&gt; [1] 0.931 ## repeatedly conduct randomized controlled trials sims &lt;- 5000 # repeat 5,000 times, we could do more diff.means &lt;- rep(NA, sims) # container for (i in 1:sims) { ## randomize the treatment by sampling of a vector of 0&#39;s and 1&#39;s treat &lt;- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE) ## difference-in-means diff.means[i] &lt;- mean(Y1[treat == 1]) - mean(Y0[treat == 0]) } ## estimation error for SATE est.error &lt;- diff.means - SATE summary(est.error) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.424 -0.082 -0.002 0.001 0.082 0.435 Tidyverse n &lt;- 100 mu0 &lt;- 0 sd0 &lt;- 1 mu1 &lt;- 1 sd1 &lt;- 1 smpl &lt;- tibble(id = seq_len(n), Y0 = rnorm(n, mean = mu0, sd = sd0), Y1 = rnorm(n, mean = mu1, sd = sd1), tau = Y1 - Y0) SATE &lt;- mean(smpl[[&quot;tau&quot;]]) SATE #&gt; [1] 1.01 sims &lt;- 5000 sim_treat &lt;- function(smpl) { SATE &lt;- mean(smpl[[&quot;tau&quot;]]) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(Y_obs = mean(Y_obs)) %&gt;% spread(treat, Y_obs) %&gt;% mutate(diff = `1` - `0`, est_error = diff - SATE) } diff_means &lt;- map_df(seq_len(sims), ~ sim_treat(smpl)) summary(diff_means[[&quot;est_error&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.523 -0.090 0.003 0.001 0.092 0.569 Original: ## PATE simulation PATE &lt;- mu1 - mu0 diff.means &lt;- rep(NA, sims) for (i in 1:sims) { ## generate a sample for each simulation: this used to be outside of loop Y0 &lt;- rnorm(n, mean = mu0, sd = sd0) Y1 &lt;- rnorm(n, mean = mu1, sd = sd1) treat &lt;- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE) diff.means[i] &lt;- mean(Y1[treat == 1]) - mean(Y0[treat == 0]) } ## estimation error for PATE est.error &lt;- diff.means - PATE ## unbiased summary(est.error) tidyverse: PATE &lt;- mu1 - mu0 sim_pate &lt;- function(mu0, mu1, sd0, sd1) { PATE &lt;- mu1 - mu0 smpl &lt;- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0), Y1 = rnorm(n, mean = mu1, sd = sd1), tau = Y1 - Y0) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(Y_obs = mean(Y_obs)) %&gt;% spread(treat, Y_obs) %&gt;% mutate(diff = `1` - `0`, est_error = diff - PATE) } diff_means &lt;- map_df(seq_len(sims), ~ sim_pate(mu0, mu1, sd0, sd1)) summary(diff_means[[&quot;est_error&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.804 -0.139 0.001 0.001 0.137 0.669 8.2.1 Standard Error Original: hist(diff.means, freq = FALSE, xlab = &quot;difference-in-means estimator&quot;, main = &quot;sampling distribution&quot;) abline(v = SATE, col = &quot;red&quot;) # true value of SATE text(0.6, 2.4, &quot;true SATE&quot;, col = &quot;red&quot;) tidyverse: ggplot(diff_means, aes(x = diff)) + geom_histogram() + geom_vline(xintercept = PATE, colour = &quot;white&quot;, size = 2) + ggtitle(&quot;sampling distribution&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Original: ## PATE simulation with standard error sims &lt;- 5000 diff.means &lt;- se &lt;- rep(NA, sims) # container for standard error added for (i in 1:sims) { ## generate a sample Y0 &lt;- rnorm(n, mean = mu0, sd = sd0) Y1 &lt;- rnorm(n, mean = mu1, sd = sd1) ## randomize the treatment by sampling of a vector of 0&#39;s and 1&#39;s treat &lt;- sample(c(rep(1, n / 2), rep(0, n / 2)), size = n, replace = FALSE) diff.means[i] &lt;- mean(Y1[treat == 1]) - mean(Y0[treat == 0]) ## standard error se[i] &lt;- sqrt(var(Y1[treat == 1]) / (n / 2) + var(Y0[treat == 0]) / (n / 2)) } ## standard deviation of difference-in-means sd(diff.means) ## mean of standard errors mean(se) tidyverse: sim_pate_se &lt;- function(mu0, mu1, sd0, sd1) { PATE &lt;- mu1 - mu0 smpl &lt;- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0), Y1 = rnorm(n, mean = mu1, sd = sd1), tau = Y1 - Y0) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(mean = mean(Y_obs), var = var(Y_obs), nobs = n()) %&gt;% summarise(diff_mean = diff(mean), se = sqrt(sum(var / nobs)), est_error = diff_mean - PATE) } diff_means &lt;- map_df(seq_len(sims), ~ sim_pate_se(mu0, mu1, sd0, sd1)) summary(diff_means[[&quot;se&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.149 0.190 0.199 0.200 0.209 0.250 8.2.2 Confidence Intervals original: ## empty container matrices for 2 sets of confidence intervals ci95 &lt;- ci90 &lt;- matrix(NA, ncol = 2, nrow = sims) ## 95 percent confidence intervals ci95[, 1] &lt;- diff.means - qnorm(0.975) * se # lower limit ci95[, 2] &lt;- diff.means + qnorm(0.975) * se # upper limit ## 90 percent confidence intervals ci90[, 1] &lt;- diff.means - qnorm(0.95) * se # lower limit ci90[, 2] &lt;- diff.means + qnorm(0.95) * se # upper limit ## coverage rate for 95% confidence interval mean(ci95[, 1] &lt;= 1 &amp; ci95[, 2] &gt;= 1) ## coverage rate for 90% confidence interval mean(ci90[, 1] &lt;= 1 &amp; ci90[, 2] &gt;= 1) tidyverse: original: p &lt;- 0.6 # true parameter value n &lt;- c(50, 100, 1000) # 3 sample sizes to be examined alpha &lt;- 0.05 sims &lt;- 5000 # number of simulations results &lt;- rep(NA, length(n)) # a container for results ## loop for different sample sizes for (i in 1:length(n)) { ci.results &lt;- rep(NA, sims) # a container for whether CI includes truth ## loop for repeated hypothetical survey sampling for (j in 1:sims) { data &lt;- rbinom(n[i], size = 1, prob = p) # simple random sampling x.bar &lt;- mean(data) # sample proportion as an estimate s.e. &lt;- sqrt(x.bar * (1 - x.bar) / n[i]) # standard errors ci.lower &lt;- x.bar - qnorm(1 - alpha/2) * s.e. ci.upper &lt;- x.bar + qnorm(1 - alpha/2) * s.e. ci.results[j] &lt;- (p &gt;= ci.lower) &amp; (p &lt;= ci.upper) } ## proportion of CIs that contain the true value results[i] &lt;- mean(ci.results) } results tidyverse: sim_binom_ci &lt;- function(size, prob = p) { } 8.2.3 Margin of Error and Sample Size Calculation in Polls Original: par(cex = 1.5, lwd = 2) MoE &lt;- c(0.01, 0.03, 0.05) # the desired margin of error p &lt;- seq(from = 0.01, to = 0.99, by = 0.01) n &lt;- 1.96^2 * p * (1 - p) / MoE[1]^2 plot(p, n, ylim = c(-1000, 11000), xlab = &quot;Population proportion&quot;, ylab = &quot;Sample size&quot;, type = &quot;l&quot;) lines(p, 1.96^2 * p * (1 - p) / MoE[2]^2, lty = &quot;dashed&quot;) lines(p, 1.96^2 * p * (1 - p) / MoE[3]^2, lty = &quot;dotted&quot;) text(0.4, 10000, &quot;margin of error = 0.01&quot;) text(0.4, 1800, &quot;margin of error = 0.03&quot;) text(0.6, -200, &quot;margin of error = 0.05&quot;) tidyverse Write a function to calculate the sample size needed for a given proportion. moe_pop_prop &lt;- function(MoE) { tibble(p = seq(from = 0.01, to = 0.99, by = 0.01), n = 1.96 ^ 2 * p * (1 - p) / MoE ^ 2, MoE = MoE) } moe_pop_prop(0.01) #&gt; # A tibble: 99 × 3 #&gt; p n MoE #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.01 380 0.01 #&gt; 2 0.02 753 0.01 #&gt; 3 0.03 1118 0.01 #&gt; 4 0.04 1475 0.01 #&gt; 5 0.05 1825 0.01 #&gt; 6 0.06 2167 0.01 #&gt; # ... with 93 more rows Then use map_df to call this function for different margins of error, and return the entire thing as a data frame with columns: n, p, and MoE. MoE &lt;- c(0.01, 0.03, 0.05) props &lt;- map_df(MoE, moe_pop_prop) Since its a data frame, its easy to plot with ggplot: ggplot(props, aes(x = p, y = n, colour = factor(MoE))) + geom_line() + labs(colour = &quot;margin of error&quot;, x = &quot;population proportion&quot;, y = &quot;sample size&quot;) + theme(legend.position = &quot;bottom&quot;) Original: ## election and polling results, by state pres08 &lt;- read.csv(&quot;pres08.csv&quot;) polls08 &lt;- read.csv(&quot;polls08.csv&quot;) ## convert to a Date object polls08$middate &lt;- as.Date(polls08$middate) ## number of days to the election day polls08$DaysToElection &lt;- as.Date(&quot;2008-11-04&quot;) - polls08$middate ## create a matrix place holder poll.pred &lt;- matrix(NA, nrow = 51, ncol = 3) ## state names which the loop will iterate through st.names &lt;- unique(pres08$state) ## add labels for easy interpretation later on row.names(poll.pred) &lt;- as.character(st.names) ## loop across 50 states plus DC for (i in 1:51){ ## subset the ith state state.data &lt;- subset(polls08, subset = (state == st.names[i])) ## subset the latest polls within the state latest &lt;- state.data$DaysToElection == min(state.data$DaysToElection) ## compute the mean of latest polls and store it poll.pred[i, 1] &lt;- mean(state.data$Obama[latest]) / 100 } ## upper and lower confidence limits n &lt;- 1000 # sample size alpha &lt;- 0.05 s.e. &lt;- sqrt(poll.pred[, 1] * (1 - poll.pred[, 1]) / n) # standard error poll.pred[, 2] &lt;- poll.pred[, 1] - qnorm(1 - alpha/2) * s.e. poll.pred[, 3] &lt;- poll.pred[, 1] + qnorm(1 - alpha/2) * s.e. tidyverse: read_csv already recognizes the date columns, so we don’t need to convert them. ELECTION_DATE &lt;- ymd(20081104) pres08 &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;pres08.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; state.name = col_character(), #&gt; state = col_character(), #&gt; Obama = col_integer(), #&gt; McCain = col_integer(), #&gt; EV = col_integer() #&gt; ) polls08 &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;polls08.csv&quot;)) %&gt;% mutate(DaysToElection = as.integer(ELECTION_DATE - middate)) #&gt; Parsed with column specification: #&gt; cols( #&gt; state = col_character(), #&gt; Pollster = col_character(), #&gt; Obama = col_integer(), #&gt; McCain = col_integer(), #&gt; middate = col_date(format = &quot;&quot;) #&gt; ) For each state calculate the mean of the latest polls, poll_pred &lt;- polls08 %&gt;% group_by(state) %&gt;% # latest polls in the state filter(DaysToElection == min(DaysToElection)) %&gt;% # take mean of latest polls and convert from 0-100 to 0-1 summarise(Obama = mean(Obama) / 100) # Add confidence itervals # sample size sample_size &lt;- 1000 # confidence level alpha &lt;- 0.05 poll_pred &lt;- poll_pred %&gt;% mutate(se = sqrt(Obama * (1 - Obama) / sample_size), ci_lwr = Obama + qnorm(alpha / 2) * se, ci_upr = Obama + qnorm(1 - alpha / 2) * se) # Add actual outcome poll_pred &lt;- left_join(poll_pred, select(pres08, state, actual = Obama), by = &quot;state&quot;) %&gt;% mutate(actual = actual / 100, covers = (ci_lwr &lt;= actual) &amp; (actual &lt;= ci_upr)) poll_pred #&gt; # A tibble: 51 × 7 #&gt; state Obama se ci_lwr ci_upr actual covers #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 AK 0.390 0.0154 0.360 0.420 0.38 TRUE #&gt; 2 AL 0.360 0.0152 0.330 0.390 0.39 FALSE #&gt; 3 AR 0.440 0.0157 0.409 0.471 0.39 FALSE #&gt; 4 AZ 0.465 0.0158 0.434 0.496 0.45 TRUE #&gt; 5 CA 0.600 0.0155 0.570 0.630 0.61 TRUE #&gt; 6 CO 0.520 0.0158 0.489 0.551 0.54 TRUE #&gt; # ... with 45 more rows original: par(cex = 1.5) alpha &lt;- 0.05 plot(pres08$Obama / 100, poll.pred[, 1], xlim = c(0, 1), ylim = c(0, 1), xlab = &quot;Obama&#39;s vote share&quot;, ylab = &quot;Poll prediction&quot;) abline(0, 1) ## adding 95% confidence intervals for each state for (i in 1:51) { lines(rep(pres08$Obama[i] / 100, 2), c(poll.pred[i, 2], poll.pred[i, 3])) } ## proportion of confidence intervals that contain the election day outcome mean((poll.pred[, 2] &lt;= pres08$Obama / 100) &amp; (poll.pred[, 3] &gt;= pres08$Obama / 100)) tidyverse: In the plot, color the pointranges by whether they include the election day outcome. ggplot(poll_pred, aes(x = actual, y = Obama, ymin = ci_lwr, ymax = ci_upr, colour = covers)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_pointrange() + scale_y_continuous(&quot;Poll prediction&quot;, limits = c(0, 1)) + scale_x_continuous(&quot;Obama&#39;s vote share&quot;, limits = c(0, 1)) + scale_colour_discrete(&quot;CI includes result?&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) Proportion of polls with CIs that include the election outcome? poll_pred %&gt;% summarise(mean(covers)) #&gt; # A tibble: 1 × 1 #&gt; `mean(covers)` #&gt; &lt;dbl&gt; #&gt; 1 0.588 original: ## bias bias &lt;- mean(poll.pred[, 1] - pres08$Obama / 100) bias ## bias corrected estimate poll.bias &lt;- poll.pred[, 1] - bias ## bias-corrected standard error s.e.bias &lt;- sqrt(poll.bias * (1 - poll.bias) / n) ## bias-corrected 95% confidence interval ci.bias.lower &lt;- poll.bias - qnorm(1 - alpha / 2) * s.e.bias ci.bias.upper &lt;- poll.bias + qnorm(1 - alpha / 2) * s.e.bias ## proportion of bias-corrected CIs that contain the election day outcome mean((ci.bias.lower &lt;= pres08$Obama / 100) &amp; (ci.bias.upper &gt;= pres08$Obama / 100)) tidyverse: poll_pred &lt;- poll_pred %&gt;% # calc bias mutate(bias = Obama - actual) %&gt;% # bias corrected prediction, se, and CI mutate(Obama_bc = Obama - mean(bias), se_bc = sqrt(Obama_bc * (1 - Obama_bc) / sample_size), ci_lwr_bc = Obama_bc + qnorm(alpha / 2) * se_bc, ci_upr_bc = Obama_bc + qnorm(1 - alpha / 2) * se_bc, covers_bc = (ci_lwr_bc &lt;= actual) &amp; (actual &lt;= ci_upr_bc)) poll_pred %&gt;% summarise(mean(covers_bc)) #&gt; # A tibble: 1 × 1 #&gt; `mean(covers_bc)` #&gt; &lt;dbl&gt; #&gt; 1 0.765 8.2.4 Analyais of Randomized Controlled Trials original: par(cex = 1.5) ## read in data STAR &lt;- read.csv(&quot;STAR.csv&quot;, head = TRUE) hist(STAR$g4reading[STAR$classtype == 1], freq = FALSE, xlim = c(500, 900), ylim = c(0, 0.01), main = &quot;Small class&quot;, xlab = &quot;Fourth-grade reading test score&quot;) abline(v = mean(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE), col = &quot;red&quot;) hist(STAR$g4reading[STAR$classtype == 2], freq = FALSE, xlim = c(500, 900), ylim = c(0, 0.01), main = &quot;Regular class&quot;, xlab = &quot;Fourth-grade reading test score&quot;) abline(v = mean(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE), col = &quot;red&quot;) tidyverse: STAR &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;STAR.csv&quot;)) %&gt;% mutate(classtype = factor(classtype, labels = c(&quot;small class&quot;, &quot;regular class&quot;, &quot;regular class with aid&quot;))) #&gt; Parsed with column specification: #&gt; cols( #&gt; race = col_integer(), #&gt; classtype = col_integer(), #&gt; yearssmall = col_integer(), #&gt; hsgrad = col_integer(), #&gt; g4math = col_integer(), #&gt; g4reading = col_integer() #&gt; ) classtype_means &lt;- STAR %&gt;% group_by(classtype) %&gt;% summarise(g4reading = mean(g4reading, na.rm = TRUE)) classtypes_used &lt;- c(&quot;small class&quot;, &quot;regular class&quot;) ggplot(filter(STAR, classtype %in% classtypes_used, !is.na(g4reading)), aes(x = g4reading, y = ..density..)) + geom_histogram(binwidth = 20) + geom_vline(data = filter(classtype_means, classtype %in% classtypes_used), mapping = aes(xintercept = g4reading), colour = &quot;white&quot;, size = 2) + facet_grid(classtype ~ .) + labs(x = &quot;Fourth grade reading score&quot;, y = &quot;Density&quot;) Original: ## estimate and standard error for small class n.small &lt;- sum(STAR$classtype == 1 &amp; !is.na(STAR$g4reading)) est.small &lt;- mean(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE) se.small &lt;- sd(STAR$g4reading[STAR$classtype == 1], na.rm = TRUE) / sqrt(n.small) est.small se.small ## estimate and standard error for regular class n.regular &lt;- sum(STAR$classtype == 2 &amp; !is.na(STAR$classtype) &amp; !is.na(STAR$g4reading)) est.regular &lt;- mean(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE) se.regular &lt;- sd(STAR$g4reading[STAR$classtype == 2], na.rm = TRUE) / sqrt(n.regular) est.regular se.regular alpha &lt;- 0.05 ## 95% confidence intervals for small class ci.small &lt;- c(est.small - qnorm(1 - alpha / 2) * se.small, est.small + qnorm(1 - alpha / 2) * se.small) ci.small ## 95% confidence intervals for regular class ci.regular &lt;- c(est.regular - qnorm(1 - alpha / 2) * se.regular, est.regular + qnorm(1 - alpha / 2) * se.regular) ci.regular tidyverse: alpha &lt;- 0.05 star_estimates &lt;- STAR %&gt;% filter(!is.na(g4reading)) %&gt;% group_by(classtype) %&gt;% summarise(n = n(), est = mean(g4reading), se = sd(g4reading) / sqrt(n)) %&gt;% mutate(lwr = est + qnorm(alpha / 2) * se, upr = est + qnorm(1 - alpha / 2) * se) star_estimates #&gt; # A tibble: 3 × 6 #&gt; classtype n est se lwr upr #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 small class 726 723 1.91 720 727 #&gt; 2 regular class 836 720 1.84 716 723 #&gt; 3 regular class with aid 791 721 1.86 717 724 Original: ## difference-in-means estimator ate.est &lt;- est.small - est.regular ate.est ## standard error and 95% confidence interval ate.se &lt;- sqrt(se.small^2 + se.regular^2) ate.se ate.ci &lt;- c(ate.est - qnorm(1 - alpha / 2) * ate.se, ate.est + qnorm(1 - alpha / 2) * ate.se) ate.ci tidyverse: Difference-in-means estimator star_estimates %&gt;% filter(classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;)) %&gt;% # ensure that it is ordered small then regular arrange(desc(classtype)) %&gt;% summarise( se = sqrt(sum(se ^ 2)), est = diff(est) ) %&gt;% mutate(ci_lwr = est + qnorm(alpha / 2) * se, ci_up = est + qnorm(1 - alpha / 2) * se) #&gt; # A tibble: 1 × 4 #&gt; se est ci_lwr ci_up #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2.65 3.5 -1.7 8.7 Use we could use spread and gather: star_ate &lt;- star_estimates %&gt;% filter(classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;)) %&gt;% mutate(classtype = fct_recode(factor(classtype), &quot;small&quot; = &quot;small class&quot;, &quot;regular&quot; = &quot;regular class&quot;)) %&gt;% select(classtype, est, se) %&gt;% gather(stat, value, -classtype) %&gt;% unite(variable, stat, classtype) %&gt;% spread(variable, value) %&gt;% mutate(ate_est = est_small - est_regular, ate_se = sqrt(se_small ^ 2 + se_regular ^ 2), ci_lwr = ate_est + qnorm(alpha / 2) * ate_se, ci_upr = ate_est + qnorm(1 - alpha / 2) * ate_se) star_ate #&gt; # A tibble: 1 × 8 #&gt; est_regular est_small se_regular se_small ate_est ate_se ci_lwr ci_upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 720 723 1.84 1.91 3.5 2.65 -1.7 8.7 8.2.5 Analysis Based on Student’s t-Distribution Original: ## 95% CI for small class c(est.small - qt(0.975, df = n.small - 1) * se.small, est.small + qt(0.975, df = n.small - 1) * se.small) ## 95% CI based on the central limit theorem ci.small ## 95% CI for regular class c(est.regular - qt(0.975, df = n.regular - 1) * se.regular, est.regular + qt(0.975, df = n.regular - 1) * se.regular) ## 95% CI based on the central limit theorem ci.regular No code needs to be adjusted. Original: t.ci &lt;- t.test(STAR$g4reading[STAR$classtype == 1], STAR$g4reading[STAR$classtype == 2]) t.ci tidyverse: Use filter to subset. t.test(filter(STAR, classtype == &quot;small class&quot;)$g4reading, filter(STAR, classtype == &quot;regular class&quot;)$g4reading) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: filter(STAR, classtype == &quot;small class&quot;)$g4reading and filter(STAR, classtype == &quot;regular class&quot;)$g4reading #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 723 720 The function t.test can also take a formula as its first parameter. t.test(g4reading ~ classtype, data = filter(STAR, classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;))) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: g4reading by classtype #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean in group small class mean in group regular class #&gt; 723 720 8.3 Hypothesis Testing 8.3.1 Tea-Testing Experiment Original: ## truth: enumerate the number of assignment combinations true &lt;- c(choose(4, 0) * choose(4, 4), choose(4, 1) * choose(4, 3), choose(4, 2) * choose(4, 2), choose(4, 3) * choose(4, 1), choose(4, 4) * choose(4, 0)) true ## compute probability: divide it by the total number of events true &lt;- true / sum(true) ## number of correctly classified cups as labels names(true) &lt;- c(0, 2, 4, 6, 8) true tidyverse: Generate the combinations more programmatically, and save the true distribution in a data frame rather than a vector. # Number of cups of tea cups &lt;- 4 # Number guessed correctly k &lt;- c(0, seq_len(cups)) true &lt;- tibble(correct = k * 2, n = choose(cups, k) * choose(cups, cups - k)) %&gt;% mutate(prob = n / sum(n)) true #&gt; # A tibble: 5 × 3 #&gt; correct n prob #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 1 0.0143 #&gt; 2 2 16 0.2286 #&gt; 3 4 36 0.5143 #&gt; 4 6 16 0.2286 #&gt; 5 8 1 0.0143 Original: ## simulations sims &lt;- 1000 ## lady&#39;s guess: M stands for `Milk first,&#39; T stands for `Tea first&#39; guess &lt;- c(&quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;, &quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;) correct &lt;- rep(NA, sims) # place holder for number of correct guesses for (i in 1:sims) { ## randomize which cups get Milk/Tea first cups &lt;- sample(c(rep(&quot;T&quot;, 4), rep(&quot;M&quot;, 4)), replace = FALSE) correct[i] &lt;- sum(guess == cups) # number of correct guesses } ## estimated probability for each number of correct guesses prop.table(table(correct)) ## comparison with analytical answers; the differences are small prop.table(table(correct)) - true tidyverse: As with other simulations, rewrite it to use a function and map function rather than a for loop. sims &lt;- 1000 guess &lt;- tibble(guess = c(&quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;, &quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;)) randomize_tea &lt;- function(df) { # randomize the order of teas assignment &lt;- sample_frac(df, 1) %&gt;% rename(actual = guess) bind_cols(df, assignment) %&gt;% summarise(correct = sum(guess == actual)) } approx &lt;- map_df(seq_len(sims), ~ randomize_tea(guess)) %&gt;% count(correct) %&gt;% mutate(prob = n / sum(n)) left_join(select(approx, correct, prob_sim = prob), select(true, correct, prob_exact = prob), by = &quot;correct&quot;) %&gt;% mutate(diff = prob_sim - prob_exact) #&gt; # A tibble: 5 × 4 #&gt; correct prob_sim prob_exact diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.014 0.0143 -0.000286 #&gt; 2 2 0.233 0.2286 0.004429 #&gt; 3 4 0.519 0.5143 0.004714 #&gt; 4 6 0.220 0.2286 -0.008571 #&gt; 5 8 0.014 0.0143 -0.000286 8.3.2 The General Framework The test functions like fisher.test do not work particularly well with data frames, and expect vectors or matrices as input, so tidyverse functions are less directly applicable Original: ## all correct x &lt;- matrix(c(4, 0, 0, 4), byrow = TRUE, ncol = 2, nrow = 2) ## six correct y &lt;- matrix(c(3, 1, 1, 3), byrow = TRUE, ncol = 2, nrow = 2) ## `M&#39; milk first, `T&#39; tea first rownames(x) &lt;- colnames(x) &lt;- rownames(y) &lt;- colnames(y) &lt;- c(&quot;M&quot;, &quot;T&quot;) x y ## one-sided test for 8 correct guesses fisher.test(x, alternative = &quot;greater&quot;) ## two-sided test for 6 correct guesses fisher.test(y) # all guesses correct x &lt;- tribble(~Guess, ~Truth, ~Number, &quot;Milk&quot;, &quot;Milk&quot;, 4L, &quot;Milk&quot;, &quot;Tea&quot;, 0L, &quot;Tea&quot;, &quot;Milk&quot;, 0L, &quot;Tea&quot;, &quot;Tea&quot;, 4L) x #&gt; # A tibble: 4 × 3 #&gt; Guess Truth Number #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Milk Milk 4 #&gt; 2 Milk Tea 0 #&gt; 3 Tea Milk 0 #&gt; 4 Tea Tea 4 # 6 correct guesses y &lt;- x %&gt;% mutate(Number = c(3L, 1L, 1L, 3L)) y #&gt; # A tibble: 4 × 3 #&gt; Guess Truth Number #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Milk Milk 3 #&gt; 2 Milk Tea 1 #&gt; 3 Tea Milk 1 #&gt; 4 Tea Tea 3 # Turn into a 2x2 table for fisher.test select(spread(x, Truth, Number), -Guess) #&gt; # A tibble: 2 × 2 #&gt; Milk Tea #&gt; * &lt;int&gt; &lt;int&gt; #&gt; 1 4 0 #&gt; 2 0 4 # Use spread to make it a 2 x 2 table fisher.test(select(spread(x, Truth, Number), -Guess), alternative = &quot;greater&quot;) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: select(spread(x, Truth, Number), -Guess) #&gt; p-value = 0.01 #&gt; alternative hypothesis: true odds ratio is greater than 1 #&gt; 95 percent confidence interval: #&gt; 2 Inf #&gt; sample estimates: #&gt; odds ratio #&gt; Inf fisher.test(select(spread(y, Truth, Number), -Guess)) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: select(spread(y, Truth, Number), -Guess) #&gt; p-value = 0.5 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.212 621.934 #&gt; sample estimates: #&gt; odds ratio #&gt; 6.41 8.3.3 One-Sample Tests Original: n &lt;- 1018 x.bar &lt;- 550 / n se &lt;- sqrt(0.5 * 0.5 / n) # standard deviation of sampling distribution ## upper red area in the figure upper &lt;- pnorm(x.bar, mean = 0.5, sd = se, lower.tail = FALSE) ## lower red area in the figure; identical to the upper area lower &lt;- pnorm(0.5 - (x.bar - 0.5), mean = 0.5, sd = se) ## two-side p-value upper + lower #&gt; [1] 0.0102 2 * upper #&gt; [1] 0.0102 ## one-sided p-value upper #&gt; [1] 0.00508 z.score &lt;- (x.bar - 0.5) / se z.score #&gt; [1] 2.57 pnorm(z.score, lower.tail = FALSE) # one-sided p-value #&gt; [1] 0.00508 2 * pnorm(z.score, lower.tail = FALSE) # two-sided p-value #&gt; [1] 0.0102 ## 99% confidence interval contains 0.5 c(x.bar - qnorm(0.995) * se, x.bar + qnorm(0.995) * se) #&gt; [1] 0.500 0.581 ## 95% confidence interval does not contain 0.5 c(x.bar - qnorm(0.975) * se, x.bar + qnorm(0.975) * se) #&gt; [1] 0.510 0.571 ## no continuity correction to get the same p-value as above prop.test(550, n = n, p = 0.5, correct = FALSE) #&gt; #&gt; 1-sample proportions test without continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 7, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.510 0.571 #&gt; sample estimates: #&gt; p #&gt; 0.54 ## with continuity correction prop.test(550, n = n, p = 0.5) #&gt; #&gt; 1-sample proportions test with continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 6, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.509 0.571 #&gt; sample estimates: #&gt; p #&gt; 0.54 prop.test(550, n = n, p = 0.5, conf.level = 0.99) #&gt; #&gt; 1-sample proportions test with continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 6, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 99 percent confidence interval: #&gt; 0.499 0.581 #&gt; sample estimates: #&gt; p #&gt; 0.54 None of that is driectly working with data frames or plotting, so nothing to change. Original: ## two-sided one-sample t-test t.test(STAR$g4reading, mu = 710) #&gt; #&gt; One Sample t-test #&gt; #&gt; data: STAR$g4reading #&gt; t = 10, df = 2000, p-value &lt;2e-16 #&gt; alternative hypothesis: true mean is not equal to 710 #&gt; 95 percent confidence interval: #&gt; 719 723 #&gt; sample estimates: #&gt; mean of x #&gt; 721 The only operation on the data frame is extracting a column, so no reason to rewrite this to use dplyr. 8.3.4 Two-sample tests Original: ## one-sided p-value pnorm(-abs(ate.est), mean = 0, sd = ate.se) ## two-sided p-value 2 * pnorm(-abs(ate.est), mean = 0, sd = ate.se) tidyverse: The ATE estimates are stored in a data frame, star_ate. Note that the dplyr function transmute is like mutate, but only returns the variables specified in the function. star_ate %&gt;% transmute(p_value_1sided = pnorm(-abs(ate_est), mean = 0, sd = ate_se), p_value_2sided = 2 * pnorm(-abs(ate_est), mean = 0, sd = ate_se)) #&gt; # A tibble: 1 × 2 #&gt; p_value_1sided p_value_2sided #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0935 0.187 Original: ## testing the null of zero average treatment effect t.test(STAR$g4reading[STAR$classtype == 1], STAR$g4reading[STAR$classtype == 2]) tidyverse: t.test(g4reading ~ classtype, data = filter(STAR, classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;))) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: g4reading by classtype #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean in group small class mean in group regular class #&gt; 723 720 or t.test(filter(STAR, classtype == &quot;small class&quot;)$g4reading, filter(STAR, classtype == &quot;regular class&quot;)$g4reading) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: filter(STAR, classtype == &quot;small class&quot;)$g4reading and filter(STAR, classtype == &quot;regular class&quot;)$g4reading #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 723 720 Original: resume &lt;- read.csv(&quot;resume.csv&quot;) ## organize the data in tables x &lt;- table(resume$race, resume$call) x ## one-sided test prop.test(x, alternative = &quot;greater&quot;) tidyverse: resume &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;resume.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; firstname = col_character(), #&gt; sex = col_character(), #&gt; race = col_character(), #&gt; call = col_integer() #&gt; ) x &lt;- resume %&gt;% count(race, call) %&gt;% spread(call, n) %&gt;% ungroup() x #&gt; # A tibble: 2 × 3 #&gt; race `0` `1` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 2278 157 #&gt; 2 white 2200 235 prop.test(as.matrix(select(x, -race)), alternative = &quot;greater&quot;) #&gt; #&gt; 2-sample test for equality of proportions with continuity #&gt; correction #&gt; #&gt; data: as.matrix(select(x, -race)) #&gt; X-squared = 20, df = 1, p-value = 2e-05 #&gt; alternative hypothesis: greater #&gt; 95 percent confidence interval: #&gt; 0.0188 1.0000 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.936 0.903 Original: ## sample size n0 &lt;- sum(resume$race == &quot;black&quot;) n1 &lt;- sum(resume$race == &quot;white&quot;) ## sample proportions p &lt;- mean(resume$call) # overall p0 &lt;- mean(resume$call[resume$race == &quot;black&quot;]) # black p1 &lt;- mean(resume$call[resume$race == &quot;white&quot;]) # white ## point estimate est &lt;- p1 - p0 est ## standard error se &lt;- sqrt(p * (1 - p) * (1 / n0 + 1 / n1)) se ## z-statistic zstat &lt;- est / se zstat ## one-sided p-value pnorm(-abs(zstat)) tidyverse: ## sample size n0 &lt;- sum(resume$race == &quot;black&quot;) n1 &lt;- sum(resume$race == &quot;white&quot;) ## sample proportions p &lt;- mean(resume$call) # overall p0 &lt;- mean(filter(resume, race == &quot;black&quot;)$call) p1 &lt;- mean(filter(resume, race == &quot;white&quot;)$call) ## point estimate est &lt;- p1 - p0 est #&gt; [1] 0.032 ## standard error se &lt;- sqrt(p * (1 - p) * (1 / n0 + 1 / n1)) se #&gt; [1] 0.0078 ## z-statistic zstat &lt;- est / se zstat #&gt; [1] 4.11 ## one-sided p-value pnorm(-abs(zstat)) #&gt; [1] 1.99e-05 The only thing that changed is using filter for selecting the groups. 8.3.5 Power Analysis Original: ## set the parameters n &lt;- 250 p.star &lt;- 0.48 # data generating process p &lt;- 0.5 # null value alpha &lt;- 0.05 ## critical value cr.value &lt;- qnorm(1 - alpha / 2) ## standard errors under the hypothetical data generating process se.star &lt;- sqrt(p.star * (1 - p.star) / n) ## standard error under the null se &lt;- sqrt(p * (1 - p) / n) ## power pnorm(p - cr.value * se, mean = p.star, sd = se.star) + pnorm(p + cr.value * se, mean = p.star, sd = se.star, lower.tail = FALSE) ## parameters n1 &lt;- 500 n0 &lt;- 500 p1.star &lt;- 0.05 p0.star &lt;- 0.1 ## overall call back rate as a weighted average p &lt;- (n1 * p1.star + n0 * p0.star) / (n1 + n0) ## standard error under the null se &lt;- sqrt(p * (1 - p) * (1 / n1 + 1 / n0)) ## standard error under the hypothetical data generating process se.star &lt;- sqrt(p1.star * (1 - p1.star) / n1 + p0.star * (1 - p0.star) / n0) pnorm(-cr.value * se, mean = p1.star - p0.star, sd = se.star) + pnorm(cr.value * se, mean = p1.star - p0.star, sd = se.star, lower.tail = FALSE) power.prop.test(n = 500, p1 = 0.05, p2 = 0.1, sig.level = 0.05) power.prop.test(p1 = 0.05, p2 = 0.1, sig.level = 0.05, power = 0.9) power.t.test(n = 100, delta = 0.25, sd = 1, type = &quot;one.sample&quot;) power.t.test(power = 0.9, delta = 0.25, sd = 1, type = &quot;one.sample&quot;) power.t.test(delta = 0.25, sd = 1, type = &quot;two.sample&quot;, alternative = &quot;one.sided&quot;, power = 0.9) None of that code uses data frames, so there’s nothing to rewrite. 8.4 Linear Regression Model with Uncertainty 8.4.1 Linear Regression as a Generative Model Original: minwage &lt;- read.csv(&quot;minwage.csv&quot;) ## compute proportion of full employment before minimum wage increase minwage$fullPropBefore &lt;- minwage$fullBefore / (minwage$fullBefore + minwage$partBefore) ## same thing after minimum wage increase minwage$fullPropAfter &lt;- minwage$fullAfter / (minwage$fullAfter + minwage$partAfter) ## an indicator for NJ: 1 if it&#39;s located in NJ and 0 if in PA minwage$NJ &lt;- ifelse(minwage$location == &quot;PA&quot;, 0, 1) tidyverse: minwage &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;minwage.csv&quot;)) %&gt;% mutate(fullPropBefore = fullBefore / (fullBefore + partBefore), fullPropAfter = fullAfter / (fullAfter + partAfter), NJ = as.integer(location == &quot;PA&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; chain = col_character(), #&gt; location = col_character(), #&gt; wageBefore = col_double(), #&gt; wageAfter = col_double(), #&gt; fullBefore = col_double(), #&gt; fullAfter = col_double(), #&gt; partBefore = col_double(), #&gt; partAfter = col_double() #&gt; ) Original: fit.minwage &lt;- lm(fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + chain, data = minwage) ## regression result fit.minwage fit.minwage1 &lt;- lm(fullPropAfter ~ NJ + fullPropBefore + wageBefore + chain, data = minwage) fit.minwage1 predict(fit.minwage, newdata = minwage[1, ]) predict(fit.minwage1, newdata = minwage[1, ]) tidyverse: fit_minwage &lt;- lm(fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + chain, data = minwage) fit_minwage #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Coefficients: #&gt; NJ fullPropBefore wageBefore chainburgerking #&gt; -0.0542 0.1688 0.0813 -0.0614 #&gt; chainkfc chainroys chainwendys #&gt; -0.0966 -0.1522 -0.1659 fit_minwage1 &lt;- lm(fullPropAfter ~ NJ + fullPropBefore + wageBefore + chain, data = minwage) fit_minwage1 #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Coefficients: #&gt; (Intercept) NJ fullPropBefore wageBefore #&gt; -0.0614 -0.0542 0.1688 0.0813 #&gt; chainkfc chainroys chainwendys #&gt; -0.0352 -0.0908 -0.1045 gather_predictions(slice(minwage, 1), fit_minwage, fit_minwage1) %&gt;% select(model, pred) #&gt; # A tibble: 2 × 2 #&gt; model pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 fit_minwage 0.271 #&gt; 2 fit_minwage1 0.271 8.4.2 Inference about coefficients Original: women &lt;- read.csv(&quot;women.csv&quot;) fit.women &lt;- lm(water ~ reserved, data = women) summary(fit.women) confint(fit.women) # 95% confidence intervals tidyverse: Use the tidy function to return the coefficients, including confidence intervals, as a data frame: women &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;women.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; GP = col_integer(), #&gt; village = col_integer(), #&gt; reserved = col_integer(), #&gt; female = col_integer(), #&gt; irrigation = col_integer(), #&gt; water = col_integer() #&gt; ) fit_women &lt;- lm(water ~ reserved, data = women) summary(fit_women) #&gt; #&gt; Call: #&gt; lm(formula = water ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -23.99 -14.74 -7.86 2.26 316.01 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 14.74 2.29 6.45 4.2e-10 *** #&gt; reserved 9.25 3.95 2.34 0.02 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 33.4 on 320 degrees of freedom #&gt; Multiple R-squared: 0.0169, Adjusted R-squared: 0.0138 #&gt; F-statistic: 5.49 on 1 and 320 DF, p-value: 0.0197 tidy(fit_women) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) 14.74 2.29 6.45 4.22e-10 #&gt; 2 reserved 9.25 3.95 2.34 1.97e-02 Original: summary(fit.minwage) confint(fit.minwage)[&quot;NJ&quot;, ] tidyverse: You need to set conf.int = TRUE for tidy to include the confidence interval: summary(fit_minwage) #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.4862 -0.1813 -0.0281 0.1513 0.7509 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; NJ -0.0542 0.0332 -1.63 0.1034 #&gt; fullPropBefore 0.1688 0.0566 2.98 0.0031 ** #&gt; wageBefore 0.0813 0.0389 2.09 0.0374 * #&gt; chainburgerking -0.0614 0.1755 -0.35 0.7266 #&gt; chainkfc -0.0966 0.1793 -0.54 0.5904 #&gt; chainroys -0.1522 0.1832 -0.83 0.4066 #&gt; chainwendys -0.1659 0.1853 -0.90 0.3711 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.244 on 351 degrees of freedom #&gt; Multiple R-squared: 0.635, Adjusted R-squared: 0.628 #&gt; F-statistic: 87.2 on 7 and 351 DF, p-value: &lt;2e-16 tidy(fit_minwage, conf.int = TRUE) #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; 1 NJ -0.0542 0.0332 -1.633 0.10343 -0.11953 0.0111 #&gt; 2 fullPropBefore 0.1688 0.0566 2.981 0.00307 0.05744 0.2801 #&gt; 3 wageBefore 0.0813 0.0389 2.090 0.03737 0.00478 0.1579 #&gt; 4 chainburgerking -0.0614 0.1755 -0.350 0.72657 -0.40648 0.2837 #&gt; 5 chainkfc -0.0966 0.1793 -0.539 0.59044 -0.44919 0.2560 #&gt; 6 chainroys -0.1522 0.1832 -0.831 0.40664 -0.51238 0.2080 #&gt; 7 chainwendys -0.1659 0.1853 -0.895 0.37115 -0.53031 0.1985 8.4.3 Inference about predictions original: ## load the data and subset them into two parties MPs &lt;- read.csv(&quot;MPs.csv&quot;) MPs.labour &lt;- subset(MPs, subset = (party == &quot;labour&quot;)) MPs.tory &lt;- subset(MPs, subset = (party == &quot;tory&quot;)) ## two regressions for labour: negative and positive margin labour.fit1 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &lt; 0, ]) labour.fit2 &lt;- lm(ln.net ~ margin, data = MPs.labour[MPs.labour$margin &gt; 0, ]) ## two regressions for tory: negative and positive margin tory.fit1 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &lt; 0, ]) tory.fit2 &lt;- lm(ln.net ~ margin, data = MPs.tory[MPs.tory$margin &gt; 0, ]) tidyverse: MPs &lt;- read_csv(qss_data_url(&quot;uncertainty&quot;, &quot;MPs.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; surname = col_character(), #&gt; firstname = col_character(), #&gt; party = col_character(), #&gt; ln.gross = col_double(), #&gt; ln.net = col_double(), #&gt; yob = col_integer(), #&gt; yod = col_integer(), #&gt; margin.pre = col_double(), #&gt; region = col_character(), #&gt; margin = col_double() #&gt; ) MPs_labour &lt;- filter(MPs, party == &quot;labour&quot;) MPs_tory &lt;- filter(MPs, party == &quot;tory&quot;) labour_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &lt; 0)) labour_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &gt; 0)) tory_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &gt; 0)) original: ## tory party: prediction at the threshold tory.y0 &lt;- predict(tory.fit1, interval = &quot;confidence&quot;, newdata = data.frame(margin = 0)) tory.y0 tory.y1 &lt;- predict(tory.fit2, interval = &quot;confidence&quot;, newdata = data.frame(margin = 0)) tory.y1 tidyverse: Predictions at the threshold. The broom function augment will return prediction fitted values and standard errors for each value, but not the confidence intervals themselves (we’d have to multiply the correct t-distribution with degrees of freedom.) So instead, we’ll directly use the predict function: tory_y0 &lt;- predict(tory_fit1, interval = &quot;confidence&quot;, newdata = tibble(margin = 0)) %&gt;% as_tibble() tory_y0 #&gt; # A tibble: 1 × 3 #&gt; fit lwr upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 12.5 12.1 13 tory_y1 &lt;- predict(tory_fit2, interval = &quot;confidence&quot;, newdata = tibble(margin = 0)) %&gt;% as_tibble() tory_y1 #&gt; # A tibble: 1 × 3 #&gt; fit lwr upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 13.2 12.8 13.6 Alternatively, using augment (and assuming a normal distribution since the number of observations is so large its not worth worrying about the t-distribution): tory_y0 &lt;- augment(tory_fit1, newdata = tibble(margin = 0)) %&gt;% mutate(lwr = .fitted + qnorm(0.025) * .se.fit, upr = .fitted + qnorm(0.975) * .se.fit) tory_y0 #&gt; margin .fitted .se.fit lwr upr #&gt; 1 0 12.5 0.214 12.1 13 tory_y1 &lt;- augment(tory_fit2, newdata = tibble(margin = 0)) %&gt;% mutate(lwr = .fitted + qnorm(0.025) * .se.fit, upr = .fitted + qnorm(0.975) * .se.fit) tory_y1 #&gt; margin .fitted .se.fit lwr upr #&gt; 1 0 13.2 0.192 12.8 13.6 Original: ## range of predictors; min to 0 and 0 to max y1.range &lt;- seq(from = 0, to = min(MPs.tory$margin), by = -0.01) y2.range &lt;- seq(from = 0, to = max(MPs.tory$margin), by = 0.01) ## prediction using all the values tory.y0 &lt;- predict(tory.fit1, interval = &quot;confidence&quot;, newdata = data.frame(margin = y1.range)) tory.y1 &lt;- predict(tory.fit2, interval = &quot;confidence&quot;, newdata = data.frame(margin = y2.range)) tidyverse: y1_range &lt;- data_grid(filter(MPs_tory, margin &lt;= 0), margin) tory_y0 &lt;- augment(tory_fit1, newdata = y1_range) y2_range &lt;- data_grid(filter(MPs_tory, margin &gt;= 0), margin) tory_y1 &lt;- augment(tory_fit2, newdata = y2_range) original: par(cex = 1.5) ## plotting the first regression with losers plot(y1.range, tory.y0[, &quot;fit&quot;], type = &quot;l&quot;, xlim = c(-0.5, 0.5), ylim = c(10, 15), xlab = &quot;Margin of victory&quot;, ylab = &quot;log net wealth&quot;) abline(v = 0, lty = &quot;dotted&quot;) lines(y1.range, tory.y0[, &quot;lwr&quot;], lty = &quot;dashed&quot;) # lower CI lines(y1.range, tory.y0[, &quot;upr&quot;], lty = &quot;dashed&quot;) # upper CI ## plotting the second regression with winners lines(y2.range, tory.y1[, &quot;fit&quot;], lty = &quot;solid&quot;) # point estimates lines(y2.range, tory.y1[, &quot;lwr&quot;], lty = &quot;dashed&quot;) # lower CI lines(y2.range, tory.y1[, &quot;upr&quot;], lty = &quot;dashed&quot;) # upper CI tidyverse: ggplot() + geom_ref_line(v = 0) + geom_point(aes(y = ln.net, x = margin), data = MPs_tory) + # plot losers geom_ribbon(aes(x = margin, ymin = .fitted + qnorm(0.025) * .se.fit, ymax = .fitted + qnorm(0.975) * .se.fit), data = tory_y0, alpha = 0.3) + geom_line(aes(x = margin, y = .fitted), data = tory_y0) + # plot winners geom_ribbon(aes(x = margin, ymin = .fitted + qnorm(0.025) * .se.fit, ymax = .fitted + qnorm(0.975) * .se.fit), data = tory_y1, alpha = 0.3) + geom_line(aes(x = margin, y = .fitted), data = tory_y1) + labs(x = &quot;Margin of vitory&quot;, y = &quot;log net wealth&quot;) Original: tory.y0 &lt;- predict(tory.fit1, interval = &quot;confidence&quot;, se.fit = TRUE, newdata = data.frame(margin = 0)) tory.y0 tory.y1 &lt;- predict(tory.fit2, interval = &quot;confidence&quot;, se.fit = TRUE, newdata = data.frame(margin = 0)) summary(tory.fit1) tidyverse: tory_y1 &lt;- augment(tory_fit1, newdata = tibble(margin = 0)) tory_y1 #&gt; margin .fitted .se.fit #&gt; 1 0 12.5 0.214 tory_y0 &lt;- augment(tory_fit2, newdata = tibble(margin = 0)) tory_y0 #&gt; margin .fitted .se.fit #&gt; 1 0 13.2 0.192 summary(tory_fit1) #&gt; #&gt; Call: #&gt; lm(formula = ln.net ~ margin, data = filter(MPs_tory, margin &lt; #&gt; 0)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.320 -0.472 -0.035 0.663 3.580 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.538 0.214 58.54 &lt;2e-16 *** #&gt; margin 1.491 1.291 1.15 0.25 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.43 on 119 degrees of freedom #&gt; Multiple R-squared: 0.0111, Adjusted R-squared: 0.00277 #&gt; F-statistic: 1.33 on 1 and 119 DF, p-value: 0.251 summary(tory_fit2) #&gt; #&gt; Call: #&gt; lm(formula = ln.net ~ margin, data = filter(MPs_tory, margin &gt; #&gt; 0)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.858 -0.877 0.001 0.830 3.126 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.188 0.192 68.69 &lt;2e-16 *** #&gt; margin -0.728 1.982 -0.37 0.71 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.29 on 100 degrees of freedom #&gt; Multiple R-squared: 0.00135, Adjusted R-squared: -0.00864 #&gt; F-statistic: 0.135 on 1 and 100 DF, p-value: 0.714 Original: # standard error se.diff &lt;- sqrt(tory.y0$se.fit^2 + tory.y1$se.fit^2) se.diff ## point estimate diff.est &lt;- tory.y1$fit[1, &quot;fit&quot;] - tory.y0$fit[1, &quot;fit&quot;] diff.est ## confidence interval CI &lt;- c(diff.est - se.diff * qnorm(0.975), diff.est + se.diff * qnorm(0.975)) CI ## hypothesis test z.score &lt;- diff.est / se.diff p.value &lt;- 2 * pnorm(abs(z.score), lower.tail = FALSE) # two-sided p-value p.value tidyverse: Since we aren’t doing anything more with these values, there isn’t much benefit in keeping them in data frames. I just adjust the names to be consistent with earlier code. # standard error se_diff &lt;- sqrt(tory_y0$.se.fit ^ 2 + tory_y1$.se.fit ^ 2) se_diff #&gt; [1] 0.288 ## point estimate diff_est &lt;- tory_y1$.fitted - tory_y0$.fitted diff_est #&gt; [1] -0.65 ## confidence interval CI &lt;- c(diff_est - se_diff * qnorm(0.975), diff_est + se_diff * qnorm(0.975)) CI #&gt; [1] -1.2134 -0.0859 ## hypothesis test z.score &lt;- diff_est / se_diff p.value &lt;- 2 * pnorm(abs(z.score), lower.tail = FALSE) # two-sided p-value p.value #&gt; [1] 0.0239 "]
]
