[
["index.html", "QSS Tidyverse Code 1 Preface", " QSS Tidyverse Code Jeffrey B. Arnold 2016-12-31 1 Preface This is tidyverse R code to supplement the book, Quantitative Social Science: An Introduction, by Kosuke Imai, to be published by Princeton University Press in March 2017. The R code included with the text of QSS and the supplementary materials relies mostly on base R functions. This translates the code examples provided with QSS to tidyverse R code. Tidyverse refers to a set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, and a few others) that share common data representations, especially the use of data frames for return values. The book R for Data Science by Hadley Wickham and Garrett Grolemond is an introduction. I wrote this code while teaching course that employed both texts in order to make the excellent examples and statistical material in QSS more compatible with the modern data science R approach in R4DS. "],
["introduction.html", "2 Introduction 2.1 Prerequisites 2.2 Introduction to R", " 2 Introduction 2.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats We also load the haven package to load Stata dta files, library(&quot;haven&quot;) and the rio package to load multiple types of files, library(&quot;rio&quot;) 2.2 Introduction to R 2.2.1 Data Files Don’t use setwd() within scripts. It is much better to organize your code in projects. When reading from csv files use readr::read_csv instead of the base R function read.csv. It is slightly faster, and returns a tibble instead of a data frame. See r4ds Ch 11: Data Import for more dicussion. We also can load it directly from a URL. UNpop &lt;- read_csv(&quot;https://raw.githubusercontent.com/jrnold/qss/master/INTRO/UNpop.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; year = col_integer(), #&gt; world.pop = col_integer() #&gt; ) class(UNpop) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; UNpop #&gt; # A tibble: 7 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; # ... with 1 more rows The single bracket, [, is useful to select rows and columns in simple cases, but the dplyr functions slice() to select rows by number, filter to select by certain criteria, or select() to select columns. UNpop[c(1, 2, 3), ] #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 is equivalent to UNpop %&gt;% slice(1:3) #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 UNpop[, &quot;world.pop&quot;] #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows is almost equivalent to select(UNpop, world.pop) #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows However, note that select() always returns a tibble, and never a vector, even if only one column is selected. Also, note that since world.pop is a tibble, using [ also returns tibbles rather than a vector if it is only one column. UNpop[1:3, &quot;year&quot;] #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 is almost equivalent to UNpop %&gt;% slice(1:3) %&gt;% select(year) #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 For this example using these functions and %&gt;% to chain them together may seem a little excessive, but later we will see how chaining simple functions togther like that becomes a very powerful way to build up complicated logic. UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] #&gt; [1] 2525779 3691173 5320817 6916183 can be rewritten as UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% select(world.pop) #&gt; # A tibble: 4 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3691173 #&gt; 3 5320817 #&gt; 4 6916183 The function n() when used in a dplyr functions returns the number of rows in the data frame (or the number of rows in the group if used with group_by). 2.2.2 Saving Objects Do not save the workspace using save.image. This is an extremely bad idea for reproducibility. See r4ds Ch 8. You should uncheck the options in RStudio to avoid saving and rstoring from .RData files. This will help ensure that your R code runs the way you think it does, instead of depending on some long forgotten code that is only saved in the workspace image. Everything important should be in a script. Anything saved or loaded from file should be done explicitly. As with reading CSVs, use the readr package functions. In this case, write_csv writes a csv file write_csv(UNpop, &quot;UNpop.csv&quot;) 2.2.3 Packages Instead of foreign for reading and writing Stata and SPSS files, use haven. One reason to do so is that it is better maintained. The R function read.dta does not read files created by the most recent versions of Stata (13+). read_dta(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; # A tibble: 7 × 2 #&gt; year world_pop #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; # ... with 1 more rows There is also the equivalent write_dta function to create Stata datasets. write_dta(UNpop, &quot;UNpop.dta&quot;) Also see the rio package which makes loading data even easier with smart defaults. You can use the import function to load many types of files: import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.csv&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.RData&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; year world_pop #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; 7 2010 6916 2.2.4 Style Guide Follow Hadley Wickham’s Style Guide not the Google R style guide. In addition to lintr, the R package formatR has methods to clean up your code. "],
["causality.html", "3 Causality 3.1 Prerequistes 3.2 Racial Discrimination in the Labor Market 3.3 Subsetting Data in R 3.4 Causal Affects and the Counterfactual 3.5 Observational Studies 3.6 Descriptive Statistics for a Single Variable", " 3 Causality 3.1 Prerequistes library(&quot;tidyverse&quot;) 3.2 Racial Discrimination in the Labor Market The code in the book uses table and addmargins to construct the table. However, this can be done easily with dplyr using grouping and summarizing. resume_url &lt;- &quot;https://raw.githubusercontent.com/jrnold/qss/master/CAUSALITY/resume.csv&quot; resume &lt;- read_csv(resume_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; firstname = col_character(), #&gt; sex = col_character(), #&gt; race = col_character(), #&gt; call = col_integer() #&gt; ) In addition to the functions shown in the text, dim(resume) #&gt; [1] 4870 4 summary(resume) #&gt; firstname sex race call #&gt; Length:4870 Length:4870 Length:4870 Min. :0.00 #&gt; Class :character Class :character Class :character 1st Qu.:0.00 #&gt; Mode :character Mode :character Mode :character Median :0.00 #&gt; Mean :0.08 #&gt; 3rd Qu.:0.00 #&gt; Max. :1.00 head(resume) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Allison female white 0 #&gt; 2 Kristen female white 0 #&gt; 3 Lakisha female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Carrie female white 0 #&gt; 6 Jay male white 0 we can also use glimpse to get a quick understanding of the variables in the data frame, glimpse(resume) #&gt; Observations: 4,870 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... For each combination of race and call let’s count the observations: race_call_tab &lt;- resume %&gt;% group_by(race, call) %&gt;% count() race_call_tab #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race call n #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 0 2278 #&gt; 2 black 1 157 #&gt; 3 white 0 2200 #&gt; 4 white 1 235 If we want to calculate callback rates by race: race_call_rate &lt;- race_call_tab %&gt;% group_by(race) %&gt;% mutate(call_rate = n / sum(n)) %&gt;% filter(call == 1) %&gt;% select(race, call_rate) race_call_rate #&gt; Source: local data frame [2 x 2] #&gt; Groups: race [2] #&gt; #&gt; race call_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black 0.0645 #&gt; 2 white 0.0965 If we want the overall callback rate, we can calculate it from the original data, resume %&gt;% summarise(call_back = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_back #&gt; &lt;dbl&gt; #&gt; 1 0.0805 3.3 Subsetting Data in R 3.3.1 Subsetting The dplyr function filter is a much improved version of subset. To select black individuals in the data: resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) dim(resumeB) #&gt; [1] 2435 4 head(resumeB) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Lakisha female black 0 #&gt; 2 Latonya female black 0 #&gt; 3 Kenya female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Tyrone male black 0 #&gt; 6 Aisha female black 0 And to calculate the callback rate resumeB %&gt;% summarise(call_rate = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_rate #&gt; &lt;dbl&gt; #&gt; 1 0.0645 To keep call and firstname variables and those with black-sounding first names. resumeBf &lt;- resume %&gt;% filter(race == &quot;black&quot;, sex == &quot;female&quot;) %&gt;% select(call, firstname) head(resumeBf) #&gt; # A tibble: 6 × 2 #&gt; call firstname #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 0 Lakisha #&gt; 2 0 Latonya #&gt; 3 0 Kenya #&gt; 4 0 Latonya #&gt; 5 0 Aisha #&gt; 6 0 Aisha Now we can calculate the gender gap by group. One way to do this is to calculate the call back rates for both sexes of black sounding names, resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) %&gt;% group_by(sex) %&gt;% summarise(black_rate = mean(call)) and white-sounding names resumeW &lt;- resume %&gt;% filter(race == &quot;white&quot;) %&gt;% group_by(sex) %&gt;% summarise(white_rate = mean(call)) resumeW #&gt; # A tibble: 2 × 2 #&gt; sex white_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 0.0989 #&gt; 2 male 0.0887 Then, merge resumeB and resumeW on sex and calculate the difference for both sexes. inner_join(resumeB, resumeW, by = &quot;sex&quot;) %&gt;% mutate(race_gap = white_rate - black_rate) #&gt; # A tibble: 2 × 4 #&gt; sex black_rate white_rate race_gap #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This seems to be a little more code, but we didn’t duplicate as much as in QSS, and this would easily scale to more than two categories. A way to do this using the spread and gather functions from tidy are, First, caclulate the resume_race_sex &lt;- resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) head(resume_race_sex) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [2] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 Now, use spread() to make each value of race a new column: library(&quot;tidyr&quot;) resume_sex &lt;- resume_race_sex %&gt;% ungroup() %&gt;% spread(race, call) resume_sex #&gt; # A tibble: 2 × 3 #&gt; sex black white #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 #&gt; 2 male 0.0583 0.0887 Now we can calculate the race wage differences by sex as before, resume_sex %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This could be combined into a single chain with only six lines of code: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% ungroup() %&gt;% spread(race, call) %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 3.3.2 Simple conditional statements See the dlpyr functions if_else, recode and case_when. The function if_else is like ifelse bue corrects for some weird behavior that ifelse has in certain cases. resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1, 0)) %&gt;% group_by(BlackFemale, race, sex) %&gt;% count() #&gt; Source: local data frame [4 x 4] #&gt; Groups: BlackFemale, race [?] #&gt; #&gt; BlackFemale race sex n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 black male 549 #&gt; 2 0 white female 1860 #&gt; 3 0 white male 575 #&gt; 4 1 black female 1886 3.3.3 Factor Variables See R4DS Chapter 15 “Factors” and the package forcats The code in this section works, but can be simplified by using the function case_when which works in exactly thease cases. resume %&gt;% mutate(type = as.factor(case_when( .$race == &quot;black&quot; &amp; .$sex == &quot;female&quot; ~ &quot;BlackFemale&quot;, .$race == &quot;black&quot; &amp; .$sex == &quot;male&quot; ~ &quot;BlackMale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;female&quot; ~ &quot;WhiteFemale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;male&quot; ~ &quot;WhiteMale&quot;, TRUE ~ as.character(NA) ))) #&gt; # A tibble: 4,870 × 5 #&gt; firstname sex race call type #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fctr&gt; #&gt; 1 Allison female white 0 WhiteFemale #&gt; 2 Kristen female white 0 WhiteFemale #&gt; 3 Lakisha female black 0 BlackFemale #&gt; 4 Latonya female black 0 BlackFemale #&gt; 5 Carrie female white 0 WhiteFemale #&gt; 6 Jay male white 0 WhiteMale #&gt; # ... with 4,864 more rows Since the logic of this is so simple, we can create this variable by using str_c to combine the vectors of sex and race, after using str_to_title to capitalize them first. library(stringr) resume &lt;- resume %&gt;% mutate(type = str_c(str_to_title(race), str_to_title(sex))) Some of the reasons given for using factors in this chapter are not as important given the functionality in modern tidyverse packages. For example, there is no reason to use tapply, as that can use group_by and summarise, resume %&gt;% group_by(type) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 × 2 #&gt; type call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 BlackFemale 0.0663 #&gt; 2 BlackMale 0.0583 #&gt; 3 WhiteFemale 0.0989 #&gt; 4 WhiteMale 0.0887 What’s nice about this approach is that we wouldn’t have needed to create the factor variable first, resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 We can use that same appraoch to calculate the mean of firstnames, and use arrange to sort in ascending order. resume %&gt;% group_by(firstname) %&gt;% summarise(call = mean(call)) %&gt;% arrange(call) #&gt; # A tibble: 36 × 2 #&gt; firstname call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aisha 0.0222 #&gt; 2 Rasheed 0.0299 #&gt; 3 Keisha 0.0383 #&gt; 4 Tremayne 0.0435 #&gt; 5 Kareem 0.0469 #&gt; 6 Darnell 0.0476 #&gt; # ... with 30 more rows 3.4 Causal Affects and the Counterfactual Load the data using the readr function read_csv social_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot; social &lt;- read_csv(social_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2006 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) summary(social) #&gt; sex yearofbirth primary2004 messages #&gt; Length:305866 Min. :1900 Min. :0.000 Length:305866 #&gt; Class :character 1st Qu.:1947 1st Qu.:0.000 Class :character #&gt; Mode :character Median :1956 Median :0.000 Mode :character #&gt; Mean :1956 Mean :0.401 #&gt; 3rd Qu.:1965 3rd Qu.:1.000 #&gt; Max. :1986 Max. :1.000 #&gt; primary2006 hhsize #&gt; Min. :0.000 Min. :1.00 #&gt; 1st Qu.:0.000 1st Qu.:2.00 #&gt; Median :0.000 Median :2.00 #&gt; Mean :0.312 Mean :2.18 #&gt; 3rd Qu.:1.000 3rd Qu.:2.00 #&gt; Max. :1.000 Max. :8.00 Use a grouped summarize instead of tapply, gotv_by_group &lt;- social %&gt;% group_by(messages) %&gt;% summarize(turnout = mean(primary2006)) gotv_by_group #&gt; # A tibble: 4 × 2 #&gt; messages turnout #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Get the turnout for the control group gotv_control &lt;- (filter(gotv_by_group, messages == &quot;Control&quot;))[[&quot;turnout&quot;]] Subtract the control group turnout from all groups gotv_by_group %&gt;% mutate(diff_control = turnout - gotv_control) #&gt; # A tibble: 4 × 3 #&gt; messages turnout diff_control #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.0179 #&gt; 2 Control 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.0257 #&gt; 4 Neighbors 0.378 0.0813 We could have also done this in one step like, gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 We can compare the differences of variables across the groups easily using a grouped summarize gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Pro-tip The summarise_at functions allows you summarize one-or-more columns with one-or-more functions. In addition to age, 2004 turnout, and household size, we’ll also compare propotion female, social %&gt;% group_by(messages) %&gt;% mutate(age = 2006 - yearofbirth, female = (sex == &quot;female&quot;)) %&gt;% select(-age, -sex) %&gt;% summarise_all(mean) #&gt; # A tibble: 4 × 6 #&gt; messages yearofbirth primary2004 primary2006 hhsize female #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 1956 0.399 0.315 2.19 0.500 #&gt; 2 Control 1956 0.400 0.297 2.18 0.499 #&gt; 3 Hawthorne 1956 0.403 0.322 2.18 0.499 #&gt; 4 Neighbors 1956 0.407 0.378 2.19 0.500 3.5 Observational Studies Load the minwage dataset from its URL using readr::read_csv: minwage_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/minwage.csv&quot; minwage &lt;- read_csv(minwage_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; chain = col_character(), #&gt; location = col_character(), #&gt; wageBefore = col_double(), #&gt; wageAfter = col_double(), #&gt; fullBefore = col_double(), #&gt; fullAfter = col_double(), #&gt; partBefore = col_double(), #&gt; partAfter = col_double() #&gt; ) glimpse(minwage) #&gt; Observations: 358 #&gt; Variables: 8 #&gt; $ chain &lt;chr&gt; &quot;wendys&quot;, &quot;wendys&quot;, &quot;burgerking&quot;, &quot;burgerking&quot;, &quot;kf... #&gt; $ location &lt;chr&gt; &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA... #&gt; $ wageBefore &lt;dbl&gt; 5.00, 5.50, 5.00, 5.00, 5.25, 5.00, 5.00, 5.00, 5.0... #&gt; $ wageAfter &lt;dbl&gt; 5.25, 4.75, 4.75, 5.00, 5.00, 5.00, 4.75, 5.00, 4.5... #&gt; $ fullBefore &lt;dbl&gt; 20.0, 6.0, 50.0, 10.0, 2.0, 2.0, 2.5, 40.0, 8.0, 10... #&gt; $ fullAfter &lt;dbl&gt; 0.0, 28.0, 15.0, 26.0, 3.0, 2.0, 1.0, 9.0, 7.0, 18.... #&gt; $ partBefore &lt;dbl&gt; 20.0, 26.0, 35.0, 17.0, 8.0, 10.0, 20.0, 30.0, 27.0... #&gt; $ partAfter &lt;dbl&gt; 36, 3, 18, 9, 12, 9, 25, 32, 39, 10, 20, 4, 13, 20,... summary(minwage) #&gt; chain location wageBefore wageAfter #&gt; Length:358 Length:358 Min. :4.25 Min. :4.25 #&gt; Class :character Class :character 1st Qu.:4.25 1st Qu.:5.05 #&gt; Mode :character Mode :character Median :4.50 Median :5.05 #&gt; Mean :4.62 Mean :4.99 #&gt; 3rd Qu.:4.99 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :6.25 #&gt; fullBefore fullAfter partBefore partAfter #&gt; Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 2.1 1st Qu.: 2.0 1st Qu.:11.0 1st Qu.:11.0 #&gt; Median : 6.0 Median : 6.0 Median :16.2 Median :17.0 #&gt; Mean : 8.5 Mean : 8.4 Mean :18.8 Mean :18.7 #&gt; 3rd Qu.:12.0 3rd Qu.:12.0 3rd Qu.:25.0 3rd Qu.:25.0 #&gt; Max. :60.0 Max. :40.0 Max. :60.0 Max. :60.0 First, calcualte the proportion of restraunts by state whose hourly wages were less than the minimum wage in NJ, $5.05, for wageBefore and wageAfter: Since the NJ minimum wage was $5.05, we’ll define a variable with that value. Even if you use them only once or twice, it is a good idea to put values like this in variables. It makes your code closer to self-documenting.n NJ_MINWAGE &lt;- 5.05 Later, it will be easier to understand wageAfter &lt; NJ_MINWAGE without any comments than it would be to understand wageAfter &lt; 5.05. In the latter case you’d have to remember that the new NJ minimum wage was 5.05 and that’s why you were using that value. This is an example of a magic number: try to avoid them. Note that location has multiple values: PA and four regions of NJ. So we’ll add a state variable to the data. minwage %&gt;% count(location) #&gt; # A tibble: 5 × 2 #&gt; location n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 centralNJ 45 #&gt; 2 northNJ 146 #&gt; 3 PA 67 #&gt; 4 shoreNJ 33 #&gt; 5 southNJ 67 We can extract the state from the final two characters of the location variable using the stringr function str_sub (R4DS Ch 14: Strings): library(stringr) minwage &lt;- mutate(minwage, state = str_sub(location, -2L)) Alternatively, since everything is either PA or NJ minwage &lt;- mutate(minwage, state = if_else(location == &quot;PA&quot;, &quot;PA&quot;, &quot;NJ&quot;)) Let’s confirm that the restraunts followed the law: minwage %&gt;% group_by(state) %&gt;% summarise(prop_after = mean(wageAfter &lt; NJ_MINWAGE), prop_Before = mean(wageBefore &lt; NJ_MINWAGE)) #&gt; # A tibble: 2 × 3 #&gt; state prop_after prop_Before #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.00344 0.911 #&gt; 2 PA 0.95522 0.940 Create a variable for the proportion of full-time employees in NJ and PA minwage &lt;- minwage %&gt;% mutate(totalAfter = fullAfter + partAfter, fullPropAfter = fullAfter / totalAfter) Now calculate the average for each state: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state #&gt; # A tibble: 2 × 2 #&gt; state fullPropAfter #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ 0.320 #&gt; 2 PA 0.272 We could compute the difference by (filter(full_prop_by_state, state == &quot;NJ&quot;)[[&quot;fullPropAfter&quot;]] - filter(full_prop_by_state, state == &quot;PA&quot;)[[&quot;fullPropAfter&quot;]]) #&gt; [1] 0.0481 or using tidyr functions spread (R4DS Ch 11: Tidy Data): spread(full_prop_by_state, state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 1 × 3 #&gt; NJ PA diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.32 0.272 0.0481 3.5.1 Confounding Bias We can calculate the proportion of fast-food restraunts in each chain in each state: chains_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% count(chain) %&gt;% mutate(prop = n / sum(n)) We can easily compare these using a simple dot-plot: ggplot(chains_by_state, aes(x = chain, y = prop, colour = state)) + geom_point() + coord_flip() In the QSS text, only Burger King restraunts are compared. However, dplyr makes this easy. All we have to do is change the group_by statement we used last time, and add chain to it: full_prop_by_state_chain &lt;- minwage %&gt;% group_by(state, chain) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state_chain #&gt; Source: local data frame [8 x 3] #&gt; Groups: state [?] #&gt; #&gt; state chain fullPropAfter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ burgerking 0.358 #&gt; 2 NJ kfc 0.328 #&gt; 3 NJ roys 0.283 #&gt; 4 NJ wendys 0.260 #&gt; 5 PA burgerking 0.321 #&gt; 6 PA kfc 0.236 #&gt; # ... with 2 more rows We can plot and compare the proportions easily in this format. In general, ordering categorical variables alphabetically is useless, so we’ll order the chains by the average of the NJ and PA fullPropAfter, using forcats::fct_reorder: ggplot(full_prop_by_state_chain, aes(x = forcats::fct_reorder(chain, fullPropAfter), y = fullPropAfter, colour = state)) + geom_point() + coord_flip() + labs(x = &quot;chains&quot;) To calculate the differences, we need to get the data frame The join method. Create New Jersey and Pennsylvania datasets with chain and prop full employed columns. Merge the two datasets on chain. chains_nj &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(-state) %&gt;% rename(NJ = fullPropAfter) chains_pa &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;PA&quot;) %&gt;% select(-state) %&gt;% rename(PA = fullPropAfter) full_prop_state_chain_diff &lt;- full_join(chains_nj, chains_pa, by = &quot;chain&quot;) %&gt;% mutate(diff = NJ - PA) full_prop_state_chain_diff #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 Q: In the code above why did I remove the state variable and rename the fullPropAfter variable before merging? What happens if I didn’t? The spread/gather method. We can also use the spread and gather functions from tidyr. In this example it is much more compact code. full_prop_by_state_chain %&gt;% spread(state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 3.5.2 Before and After and Difference-in-Difference Designs To compute the estimates in the before and after design first create a variable for the difference before and after the law passed. minwage &lt;- minwage %&gt;% mutate(totalBefore = fullBefore + partBefore, fullPropBefore = fullBefore / totalBefore) The before-and-after analysis is the difference between the full-time employment before and after the minimum wage law passed looking only at NJ: filter(minwage, state == &quot;NJ&quot;) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) #&gt; # A tibble: 1 × 1 #&gt; diff #&gt; &lt;dbl&gt; #&gt; 1 0.0239 The difference-in-differences design uses the difference in the before-and-after differences for each state. diff_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) filter(diff_by_state, state == &quot;NJ&quot;)[[&quot;diff&quot;]] - filter(diff_by_state, state == &quot;PA&quot;)[[&quot;diff&quot;]] #&gt; [1] 0.0616 Let’s create a single dataset with the mean values of each state before and after to visually look at each of these designs: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(fullPropAfter, fullPropBefore), mean) %&gt;% gather(period, fullProp, -state) %&gt;% mutate(period = recode(period, fullPropAfter = 1, fullPropBefore = 0)) full_prop_by_state #&gt; # A tibble: 4 × 3 #&gt; state period fullProp #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 1 0.320 #&gt; 2 PA 1 0.272 #&gt; 3 NJ 0 0.297 #&gt; 4 PA 0 0.310 ggplot(full_prop_by_state, aes(x = period, y = fullProp, colour = state)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(0, 1), labels = c(&quot;Before&quot;, &quot;After&quot;)) 3.6 Descriptive Statistics for a Single Variable To calculate the summary for the variables wageBefore and wageAfter: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(wageBefore, wageAfter) %&gt;% summary() #&gt; wageBefore wageAfter #&gt; Min. :4.25 Min. :5.00 #&gt; 1st Qu.:4.25 1st Qu.:5.05 #&gt; Median :4.50 Median :5.05 #&gt; Mean :4.61 Mean :5.08 #&gt; 3rd Qu.:4.87 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :5.75 We calculate the IQR for each state’s wages after the passage of the law using the same grouped summarise as we used before: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter = IQR(wageAfter), wageBefore = IQR(wageBefore)) #&gt; # A tibble: 2 × 3 #&gt; state wageAfter wageBefore #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.000 0.62 #&gt; 2 PA 0.575 0.75 Calculate the variance and standard deviation of wageAfter and wageBefore for each state: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter_sd = sd(wageAfter), wageAfter_var = var(wageAfter), wageBefore_sd = sd(wageBefore), wageBefore_var = var(wageBefore)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageAfter_var wageBefore_sd wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.0112 0.343 0.118 #&gt; 2 PA 0.359 0.1291 0.358 0.128 or, more compactly, using summarise_at: minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(wageAfter, wageBefore), funs(sd, var)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageBefore_sd wageAfter_var wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.343 0.0112 0.118 #&gt; 2 PA 0.359 0.358 0.1291 0.128 "],
["measurement.html", "4 Measurement 4.1 Prerequisites 4.2 Measuring Civilian Victimization during Wartime 4.3 Visualizing the Univariate Distribution 4.4 Survey Sampling 4.5 load village data 4.6 Measuring Political Polarization 4.7 Clustering", " 4 Measurement 4.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats library(&quot;forcats&quot;) library(&quot;broom&quot;) 4.2 Measuring Civilian Victimization during Wartime afghan_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan.csv&quot; afghan &lt;- read_csv(afghan_url) Summarize the variables of interest afghan %&gt;% select(age, educ.years, employed, income) %&gt;% summary() #&gt; age educ.years employed income #&gt; Min. :15.0 Min. : 0 Min. :0.000 Length:2754 #&gt; 1st Qu.:22.0 1st Qu.: 0 1st Qu.:0.000 Class :character #&gt; Median :30.0 Median : 1 Median :1.000 Mode :character #&gt; Mean :32.4 Mean : 4 Mean :0.583 #&gt; 3rd Qu.:40.0 3rd Qu.: 8 3rd Qu.:1.000 #&gt; Max. :80.0 Max. :18 Max. :1.000 With income, read_csv never converts strings to factors by default. To get a summary of the different levels, either convert it to a factor (R4DS Ch 15), or use count() afghan %&gt;% count(income) #&gt; # A tibble: 6 × 2 #&gt; income n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 10,001-20,000 616 #&gt; 2 2,001-10,000 1420 #&gt; 3 20,001-30,000 93 #&gt; 4 less than 2,000 457 #&gt; 5 over 30,000 14 #&gt; 6 &lt;NA&gt; 154 Count the number a proportion of respondents who answer that they were harmed by the ISF (violent.exp.ISAF) and (violent.exp.taliban) respectively, afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 9 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.48293 #&gt; 2 0 1 354 0.12854 #&gt; 3 0 NA 22 0.00799 #&gt; 4 1 0 475 0.17248 #&gt; 5 1 1 526 0.19099 #&gt; 6 1 NA 22 0.00799 #&gt; # ... with 3 more rows We need to use ungroup() in order to ensure that sum(n) sums over the entire dataset as opposed to only within categories of violent.exp.ISAF. Unlike prop.table, the code above does not drop missing values. We can drop those values by adding a filter verb and using !is.na() to test for missing values in those variables: afghan %&gt;% filter(!is.na(violent.exp.ISAF), !is.na(violent.exp.taliban)) %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 4 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.495 #&gt; 2 0 1 354 0.132 #&gt; 3 1 0 475 0.177 #&gt; 4 1 1 526 0.196 4.2.1 Handling Missing Data in R We already observed the issues with NA values in calculating the proportion answering the “experienced violence” questions. You can filter rows with specific variables having missing values using filter as shown above. Howeer, na.omit works with tibbles just like any other data frame. na.omit(afghan) #&gt; # A tibble: 2,554 × 11 #&gt; province district village.id age educ.years employed income #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Logar Baraki Barak 80 26 10 0 2,001-10,000 #&gt; 2 Logar Baraki Barak 80 49 3 1 2,001-10,000 #&gt; 3 Logar Baraki Barak 80 60 0 1 2,001-10,000 #&gt; 4 Logar Baraki Barak 80 34 14 1 2,001-10,000 #&gt; 5 Logar Baraki Barak 80 21 12 1 2,001-10,000 #&gt; 6 Logar Baraki Barak 80 42 6 1 10,001-20,000 #&gt; # ... with 2,548 more rows, and 4 more variables: violent.exp.ISAF &lt;int&gt;, #&gt; # violent.exp.taliban &lt;int&gt;, list.group &lt;chr&gt;, list.response &lt;int&gt; 4.3 Visualizing the Univariate Distribution 4.3.1 Barplot library(forcats) afghan &lt;- afghan %&gt;% mutate(violent.exp.ISAF.fct = fct_explicit_na(fct_recode(factor(violent.exp.ISAF), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the ISAF&quot;) afghan &lt;- afghan %&gt;% mutate(violent.exp.taliban.fct = fct_explicit_na(fct_recode(factor(violent.exp.taliban), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the Taliban&quot;) TODO This plot could improved by plotting the two values simultaneously to be able to better compare them. dodged bar plot dot-plot This will require creating a data frame that has the following columns: perpetrator (ISAF, Taliban), response (No Harm, Harm, No response). See the section on Tidy Data and spread gather. TODO Compare them by region, ? 4.3.2 Boxplot ggplot(afghan, aes(x = 1, y = age)) + geom_boxplot() + labs(y = &quot;Age&quot;, x = &quot;&quot;) + ggtitle(&quot;Distribution of Age&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_boxplot() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;) + ggtitle(&quot;Education by Provice&quot;) Helmand and Uruzgan have much lower levels of education than the other provicnces, and also report higher levels of violence. afghan %&gt;% group_by(province) %&gt;% summarise(educ.years = mean(educ.years, na.rm = TRUE), violent.exp.taliban = mean(violent.exp.taliban, na.rm = TRUE), violent.exp.ISAF = mean(violent.exp.ISAF, na.rm = TRUE)) %&gt;% arrange(educ.years) #&gt; # A tibble: 5 × 4 #&gt; province educ.years violent.exp.taliban violent.exp.ISAF #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 1.04 0.4545 0.496 #&gt; 2 Helmand 1.60 0.5042 0.541 #&gt; 3 Khost 5.79 0.2332 0.242 #&gt; 4 Kunar 5.93 0.3030 0.399 #&gt; 5 Logar 6.70 0.0802 0.144 4.3.3 Printing and saving graphics Use the function ggsave() to save ggplot graphics. Also, RMarkdown files have their own means of creating and saving plots created by code-chunks. 4.4 Survey Sampling 4.4.1 The Role of Randomization 4.5 load village data afghan_village_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan-village.csv&quot; afghan.village &lt;- read_csv(afghan_village_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; altitude = col_double(), #&gt; population = col_integer(), #&gt; village.surveyed = col_integer() #&gt; ) Box-plots of altitude ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = altitude)) + geom_boxplot() + labs(y = &quot;Altitude (meter)&quot;, x = &quot;&quot;) Boxplots log-population values of sampled and non-sampled ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = log(population))) + geom_boxplot() + labs(y = &quot;log(population)&quot;, x = &quot;&quot;) You can also compare these distributions by plotting their densities: ggplot(afghan.village, aes(colour = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), x = log(population))) + geom_density() + geom_rug() + labs(x = &quot;log(population)&quot;, colour = &quot;&quot;) 4.5.1 Non-response and other sources of bias Calculate the rates of non-response by province to violent.exp.ISAF and violent.exp.taliban: afghan %&gt;% group_by(province) %&gt;% summarise(ISAF = mean(is.na(violent.exp.ISAF)), taliban = mean(is.na(violent.exp.taliban))) %&gt;% arrange(-ISAF) #&gt; # A tibble: 5 × 3 #&gt; province ISAF taliban #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 0.02067 0.06202 #&gt; 2 Helmand 0.01637 0.03041 #&gt; 3 Khost 0.00476 0.00635 #&gt; 4 Kunar 0.00000 0.00000 #&gt; 5 Logar 0.00000 0.00000 Calculat the proportion who support the ISAF using the difference in means between the ISAF and control groups: (mean(filter(afghan, list.group == &quot;ISAF&quot;)$list.response) - mean(filter(afghan, list.group == &quot;control&quot;)$list.response)) #&gt; [1] 0.049 To calculate the table responses to the list expriment in the control, ISAF, and taliban groups&gt; afghan %&gt;% group_by(list.response, list.group) %&gt;% count() %T&gt;% glimpse() %&gt;% spread(list.group, n, fill = 0) #&gt; Observations: 12 #&gt; Variables: 3 #&gt; $ list.response &lt;int&gt; 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4 #&gt; $ list.group &lt;chr&gt; &quot;control&quot;, &quot;ISAF&quot;, &quot;control&quot;, &quot;ISAF&quot;, &quot;taliban&quot;,... #&gt; $ n &lt;int&gt; 188, 174, 265, 278, 433, 265, 260, 287, 200, 182... #&gt; Source: local data frame [5 x 4] #&gt; Groups: list.response [5] #&gt; #&gt; list.response control ISAF taliban #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 188 174 0 #&gt; 2 1 265 278 433 #&gt; 3 2 265 260 287 #&gt; 4 3 200 182 198 #&gt; 5 4 0 24 0 4.6 Measuring Political Polarization congress_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/congress.csv&quot; congress &lt;- read_csv(congress_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; congress = col_integer(), #&gt; district = col_integer(), #&gt; state = col_character(), #&gt; party = col_character(), #&gt; name = col_character(), #&gt; dwnom1 = col_double(), #&gt; dwnom2 = col_double() #&gt; ) glimpse(congress) #&gt; Observations: 14,552 #&gt; Variables: 7 #&gt; $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8... #&gt; $ district &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 98, 98, 1, 2, 3, 4, 5, ... #&gt; $ state &lt;chr&gt; &quot;USA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;A... #&gt; $ party &lt;chr&gt; &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Demo... #&gt; $ name &lt;chr&gt; &quot;TRUMAN&quot;, &quot;BOYKIN F.&quot;, &quot;GRANT G.&quot;, &quot;ANDREWS G.&quot;, &quot;... #&gt; $ dwnom1 &lt;dbl&gt; -0.276, -0.026, -0.042, -0.008, -0.082, -0.170, -0.12... #&gt; $ dwnom2 &lt;dbl&gt; 0.016, 0.796, 0.999, 1.005, 1.066, 0.870, 0.990, 0.89... To create the scatterplot in 3.6, we can congress %&gt;% filter(congress %in% c(80, 112), party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) congress %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) #&gt; Warning: Removed 2 rows containing missing values (geom_point). congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = congress, y = dwnom1, colour = fct_reorder2(party, congress, dwnom1))) + geom_line() + labs(y = &quot;DW-NOMINATE score (1st Dimension)&quot;, x = &quot;Congress&quot;, colour = &quot;Party&quot;) 4.6.1 Correlation Let’s plot the Gini coefficient usgini_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/USGini.csv&quot; USGini &lt;- read_csv(usgini_url) #&gt; Warning: Missing column names filled in: &#39;X1&#39; [1] #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_integer(), #&gt; year = col_integer(), #&gt; gini = col_double() #&gt; ) ggplot(USGini, aes(x = year, y = gini)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Gini coefficient&quot;) + ggtitle(&quot;Income Inequality&quot;) To calculate a measure of party polarization take the code used in the plot of Republican and Democratic party median ideal points and adapt it to calculate the difference in the party medians: party_polarization &lt;- congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% spread(party, dwnom1) %&gt;% mutate(polarization = Republican - Democrat) party_polarization #&gt; Source: local data frame [33 x 4] #&gt; Groups: congress [33] #&gt; #&gt; congress Democrat Republican polarization #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 80 -0.146 0.276 0.421 #&gt; 2 81 -0.195 0.264 0.459 #&gt; 3 82 -0.180 0.265 0.445 #&gt; 4 83 -0.181 0.261 0.442 #&gt; 5 84 -0.209 0.261 0.471 #&gt; 6 85 -0.214 0.250 0.464 #&gt; # ... with 27 more rows ggplot(party_polarization, aes(x = congress, y = polarization)) + geom_point() + ggtitle(&quot;Political Polarization&quot;) + labs(x = &quot;Year&quot;, y = &quot;Republican median − Democratic median&quot;) 4.6.2 Quantile-Quantile Plot To create histogram plots similar congress %&gt;% filter(congress == 112, party %in% c(&quot;Republican&quot;, &quot;Democrat&quot;)) %&gt;% ggplot(aes(x = dwnom2, y = ..density..)) + geom_histogram() + facet_grid(. ~ party) + labs(x = &quot;racial liberalism/conservatism dimension&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot2 includes a stat_qq which can be used to create qq-plots but it is more suited to comparing a sample distribution with a theoretical distibution, usually the normal one. However, we can calculate one by hand, which may give more insight into exactly what the qq-plot is doing. party_qtiles &lt;- tibble( probs = seq(0, 1, by = 0.01), Democrat = quantile(filter(congress, congress == 112, party == &quot;Democrat&quot;)$dwnom2, probs = probs), Republican = quantile(filter(congress, congress == 112, party == &quot;Republican&quot;)$dwnom2, probs = probs) ) party_qtiles #&gt; # A tibble: 101 × 3 #&gt; probs Democrat Republican #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00 -0.925 -1.381 #&gt; 2 0.01 -0.672 -0.720 #&gt; 3 0.02 -0.619 -0.566 #&gt; 4 0.03 -0.593 -0.526 #&gt; 5 0.04 -0.567 -0.468 #&gt; 6 0.05 -0.560 -0.436 #&gt; # ... with 95 more rows The plot looks different than the one in the text since the x- and y-scales are in the original values instead of z-scores (see the next section). party_qtiles %&gt;% ggplot(aes(x = Democrat, y = Republican)) + geom_point() + geom_abline() + coord_fixed() 4.7 Clustering 4.7.1 Matrices While matrices are great for numerical computations, such as when you are implementing algorithms, generally keeping data in data frames is more convenient for data wrangling. 4.7.2 Lists See R4DS Chapter 20: Vectors, Chapter 21: Iteration and the purrr package for more powerful methods of computing on lists. 4.7.3 k-means algorithms TODO A good visualization of the k-means algorithm and a simple, naive implementation in R. Calculate the clusters by the 80th and 112th congresses, k80two.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 2, nstart = 5) Add the cluster ids to datasets congress80 &lt;- congress %&gt;% filter(congress == 80) %&gt;% mutate(cluster2 = factor(k80two.out$cluster)) We will also create a data sets with the cluster centroids. These are in the centers element of the cluster object. k80two.out$centers #&gt; dwnom1 dwnom2 #&gt; 1 -0.0484 0.783 #&gt; 2 0.1468 -0.339 To make it easier to use with ggplot, we need to convert this to a data frame. The tidy function from the broom package: k80two.clusters &lt;- tidy(k80two.out) k80two.clusters #&gt; x1 x2 size withinss cluster #&gt; 1 -0.0484 0.783 135 10.9 1 #&gt; 2 0.1468 -0.339 311 54.9 2 Plot the ideal points and clusters ggplot() + geom_point(data = congress80, aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80two.clusters, mapping = aes(x = x1, y = x2)) We can also plot, congress80 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [5 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 132 #&gt; 2 Democrat 2 62 #&gt; 3 Other 2 2 #&gt; 4 Republican 1 3 #&gt; 5 Republican 2 247 And now we can repeat these steps for the 112th congress: k112two.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 2, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112two.out$cluster)) k112two.clusters &lt;- tidy(k112two.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112two.clusters, mapping = aes(x = x1, y = x2)) congress112 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [3 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 2 200 #&gt; 2 Republican 1 242 #&gt; 3 Republican 2 1 Now repeat the same with four clusters on the 80th congress: k80four.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 4, nstart = 5) congress80 &lt;- filter(congress, congress == 80) %&gt;% mutate(cluster2 = factor(k80four.out$cluster)) k80four.clusters &lt;- tidy(k80four.out) ggplot() + geom_point(data = congress80, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80four.clusters, mapping = aes(x = x1, y = x2), size = 3) and on the 112th congress: k112four.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 4, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112four.out$cluster)) k112four.clusters &lt;- tidy(k112four.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112four.clusters, mapping = aes(x = x1, y = x2), size = 3) "],
["prediction.html", "5 Prediction 5.1 Prerequisites", " 5 Prediction 5.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "],
["discovery.html", "6 Discovery 6.1 Prerequisites", " 6 Discovery 6.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "],
["probability.html", "7 Probability 7.1 Prerequisites", " 7 Probability 7.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "],
["uncertainty.html", "8 Uncertainty 8.1 Prerequisites", " 8 Uncertainty 8.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "]
]
