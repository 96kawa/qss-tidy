[
["index.html", "QSS Tidyverse Code 1 Preface", " QSS Tidyverse Code Jeffrey B. Arnold 2017-01-02 1 Preface This is tidyverse R code to supplement the book, Quantitative Social Science: An Introduction, by Kosuke Imai, to be published by Princeton University Press in March 2017. The R code included with the text of QSS and the supplementary materials relies mostly on base R functions. This translates the code examples provided with QSS to tidyverse R code. Tidyverse refers to a set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, and a few others) that share common data representations, especially the use of data frames for return values. The book R for Data Science by Hadley Wickham and Garrett Grolemond is an introduction. I wrote this code while teaching course that employed both texts in order to make the excellent examples and statistical material in QSS more compatible with the modern data science R approach in R4DS. "],
["introduction.html", "2 Introduction 2.1 Prerequisites 2.2 Introduction to R", " 2 Introduction 2.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats We also load the haven package to load Stata dta files, library(&quot;haven&quot;) and the rio package to load multiple types of files, library(&quot;rio&quot;) 2.2 Introduction to R 2.2.1 Data Files Don’t use setwd() within scripts. It is much better to organize your code in projects. When reading from csv files use readr::read_csv instead of the base R function read.csv. It is slightly faster, and returns a tibble instead of a data frame. See r4ds Ch 11: Data Import for more dicussion. We also can load it directly from a URL. UNpop &lt;- read_csv(&quot;https://raw.githubusercontent.com/jrnold/qss/master/INTRO/UNpop.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; year = col_integer(), #&gt; world.pop = col_integer() #&gt; ) class(UNpop) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; UNpop #&gt; # A tibble: 7 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; # ... with 1 more rows The single bracket, [, is useful to select rows and columns in simple cases, but the dplyr functions slice() to select rows by number, filter to select by certain criteria, or select() to select columns. UNpop[c(1, 2, 3), ] #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 is equivalent to UNpop %&gt;% slice(1:3) #&gt; # A tibble: 3 × 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 UNpop[, &quot;world.pop&quot;] #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows is almost equivalent to select(UNpop, world.pop) #&gt; # A tibble: 7 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more rows However, note that select() always returns a tibble, and never a vector, even if only one column is selected. Also, note that since world.pop is a tibble, using [ also returns tibbles rather than a vector if it is only one column. UNpop[1:3, &quot;year&quot;] #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 is almost equivalent to UNpop %&gt;% slice(1:3) %&gt;% select(year) #&gt; # A tibble: 3 × 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 For this example using these functions and %&gt;% to chain them together may seem a little excessive, but later we will see how chaining simple functions togther like that becomes a very powerful way to build up complicated logic. UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] #&gt; [1] 2525779 3691173 5320817 6916183 can be rewritten as UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% select(world.pop) #&gt; # A tibble: 4 × 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3691173 #&gt; 3 5320817 #&gt; 4 6916183 The function n() when used in a dplyr functions returns the number of rows in the data frame (or the number of rows in the group if used with group_by). 2.2.2 Saving Objects Do not save the workspace using save.image. This is an extremely bad idea for reproducibility. See r4ds Ch 8. You should uncheck the options in RStudio to avoid saving and rstoring from .RData files. This will help ensure that your R code runs the way you think it does, instead of depending on some long forgotten code that is only saved in the workspace image. Everything important should be in a script. Anything saved or loaded from file should be done explicitly. As with reading CSVs, use the readr package functions. In this case, write_csv writes a csv file write_csv(UNpop, &quot;UNpop.csv&quot;) 2.2.3 Packages Instead of foreign for reading and writing Stata and SPSS files, use haven. One reason to do so is that it is better maintained. The R function read.dta does not read files created by the most recent versions of Stata (13+). read_dta(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; # A tibble: 7 × 2 #&gt; year world_pop #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; # ... with 1 more rows There is also the equivalent write_dta function to create Stata datasets. write_dta(UNpop, &quot;UNpop.dta&quot;) Also see the rio package which makes loading data even easier with smart defaults. You can use the import function to load many types of files: import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.csv&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.RData&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.dta&quot;) #&gt; year world_pop #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; 7 2010 6916 2.2.4 Style Guide Follow Hadley Wickham’s Style Guide not the Google R style guide. In addition to lintr, the R package formatR has methods to clean up your code. "],
["causality.html", "3 Causality 3.1 Prerequistes 3.2 Racial Discrimination in the Labor Market 3.3 Subsetting Data in R 3.4 Causal Affects and the Counterfactual 3.5 Observational Studies 3.6 Descriptive Statistics for a Single Variable", " 3 Causality 3.1 Prerequistes library(&quot;tidyverse&quot;) 3.2 Racial Discrimination in the Labor Market The code in the book uses table and addmargins to construct the table. However, this can be done easily with dplyr using grouping and summarizing. resume_url &lt;- &quot;https://raw.githubusercontent.com/jrnold/qss/master/CAUSALITY/resume.csv&quot; resume &lt;- read_csv(resume_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; firstname = col_character(), #&gt; sex = col_character(), #&gt; race = col_character(), #&gt; call = col_integer() #&gt; ) In addition to the functions shown in the text, dim(resume) #&gt; [1] 4870 4 summary(resume) #&gt; firstname sex race call #&gt; Length:4870 Length:4870 Length:4870 Min. :0.00 #&gt; Class :character Class :character Class :character 1st Qu.:0.00 #&gt; Mode :character Mode :character Mode :character Median :0.00 #&gt; Mean :0.08 #&gt; 3rd Qu.:0.00 #&gt; Max. :1.00 head(resume) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Allison female white 0 #&gt; 2 Kristen female white 0 #&gt; 3 Lakisha female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Carrie female white 0 #&gt; 6 Jay male white 0 we can also use glimpse to get a quick understanding of the variables in the data frame, glimpse(resume) #&gt; Observations: 4,870 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... For each combination of race and call let’s count the observations: race_call_tab &lt;- resume %&gt;% group_by(race, call) %&gt;% count() race_call_tab #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race call n #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 0 2278 #&gt; 2 black 1 157 #&gt; 3 white 0 2200 #&gt; 4 white 1 235 If we want to calculate callback rates by race: race_call_rate &lt;- race_call_tab %&gt;% group_by(race) %&gt;% mutate(call_rate = n / sum(n)) %&gt;% filter(call == 1) %&gt;% select(race, call_rate) race_call_rate #&gt; Source: local data frame [2 x 2] #&gt; Groups: race [2] #&gt; #&gt; race call_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black 0.0645 #&gt; 2 white 0.0965 If we want the overall callback rate, we can calculate it from the original data, resume %&gt;% summarise(call_back = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_back #&gt; &lt;dbl&gt; #&gt; 1 0.0805 3.3 Subsetting Data in R 3.3.1 Subsetting The dplyr function filter is a much improved version of subset. To select black individuals in the data: resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) dim(resumeB) #&gt; [1] 2435 4 head(resumeB) #&gt; # A tibble: 6 × 4 #&gt; firstname sex race call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Lakisha female black 0 #&gt; 2 Latonya female black 0 #&gt; 3 Kenya female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Tyrone male black 0 #&gt; 6 Aisha female black 0 And to calculate the callback rate resumeB %&gt;% summarise(call_rate = mean(call)) #&gt; # A tibble: 1 × 1 #&gt; call_rate #&gt; &lt;dbl&gt; #&gt; 1 0.0645 To keep call and firstname variables and those with black-sounding first names. resumeBf &lt;- resume %&gt;% filter(race == &quot;black&quot;, sex == &quot;female&quot;) %&gt;% select(call, firstname) head(resumeBf) #&gt; # A tibble: 6 × 2 #&gt; call firstname #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 0 Lakisha #&gt; 2 0 Latonya #&gt; 3 0 Kenya #&gt; 4 0 Latonya #&gt; 5 0 Aisha #&gt; 6 0 Aisha Now we can calculate the gender gap by group. One way to do this is to calculate the call back rates for both sexes of black sounding names, resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) %&gt;% group_by(sex) %&gt;% summarise(black_rate = mean(call)) and white-sounding names resumeW &lt;- resume %&gt;% filter(race == &quot;white&quot;) %&gt;% group_by(sex) %&gt;% summarise(white_rate = mean(call)) resumeW #&gt; # A tibble: 2 × 2 #&gt; sex white_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 0.0989 #&gt; 2 male 0.0887 Then, merge resumeB and resumeW on sex and calculate the difference for both sexes. inner_join(resumeB, resumeW, by = &quot;sex&quot;) %&gt;% mutate(race_gap = white_rate - black_rate) #&gt; # A tibble: 2 × 4 #&gt; sex black_rate white_rate race_gap #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This seems to be a little more code, but we didn’t duplicate as much as in QSS, and this would easily scale to more than two categories. A way to do this using the spread and gather functions from tidy are, First, caclulate the resume_race_sex &lt;- resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) head(resume_race_sex) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [2] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 Now, use spread() to make each value of race a new column: library(&quot;tidyr&quot;) resume_sex &lt;- resume_race_sex %&gt;% ungroup() %&gt;% spread(race, call) resume_sex #&gt; # A tibble: 2 × 3 #&gt; sex black white #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 #&gt; 2 male 0.0583 0.0887 Now we can calculate the race wage differences by sex as before, resume_sex %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This could be combined into a single chain with only six lines of code: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% ungroup() %&gt;% spread(race, call) %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 × 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 3.3.2 Simple conditional statements See the dlpyr functions if_else, recode and case_when. The function if_else is like ifelse bue corrects for some weird behavior that ifelse has in certain cases. resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1, 0)) %&gt;% group_by(BlackFemale, race, sex) %&gt;% count() #&gt; Source: local data frame [4 x 4] #&gt; Groups: BlackFemale, race [?] #&gt; #&gt; BlackFemale race sex n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 black male 549 #&gt; 2 0 white female 1860 #&gt; 3 0 white male 575 #&gt; 4 1 black female 1886 3.3.3 Factor Variables See R4DS Chapter 15 “Factors” and the package forcats The code in this section works, but can be simplified by using the function case_when which works in exactly thease cases. resume %&gt;% mutate(type = as.factor(case_when( .$race == &quot;black&quot; &amp; .$sex == &quot;female&quot; ~ &quot;BlackFemale&quot;, .$race == &quot;black&quot; &amp; .$sex == &quot;male&quot; ~ &quot;BlackMale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;female&quot; ~ &quot;WhiteFemale&quot;, .$race == &quot;white&quot; &amp; .$sex == &quot;male&quot; ~ &quot;WhiteMale&quot;, TRUE ~ as.character(NA) ))) #&gt; # A tibble: 4,870 × 5 #&gt; firstname sex race call type #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fctr&gt; #&gt; 1 Allison female white 0 WhiteFemale #&gt; 2 Kristen female white 0 WhiteFemale #&gt; 3 Lakisha female black 0 BlackFemale #&gt; 4 Latonya female black 0 BlackFemale #&gt; 5 Carrie female white 0 WhiteFemale #&gt; 6 Jay male white 0 WhiteMale #&gt; # ... with 4,864 more rows Since the logic of this is so simple, we can create this variable by using str_c to combine the vectors of sex and race, after using str_to_title to capitalize them first. library(stringr) resume &lt;- resume %&gt;% mutate(type = str_c(str_to_title(race), str_to_title(sex))) Some of the reasons given for using factors in this chapter are not as important given the functionality in modern tidyverse packages. For example, there is no reason to use tapply, as that can use group_by and summarise, resume %&gt;% group_by(type) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 × 2 #&gt; type call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 BlackFemale 0.0663 #&gt; 2 BlackMale 0.0583 #&gt; 3 WhiteFemale 0.0989 #&gt; 4 WhiteMale 0.0887 What’s nice about this approach is that we wouldn’t have needed to create the factor variable first, resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) #&gt; Source: local data frame [4 x 3] #&gt; Groups: race [?] #&gt; #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 We can use that same appraoch to calculate the mean of firstnames, and use arrange to sort in ascending order. resume %&gt;% group_by(firstname) %&gt;% summarise(call = mean(call)) %&gt;% arrange(call) #&gt; # A tibble: 36 × 2 #&gt; firstname call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aisha 0.0222 #&gt; 2 Rasheed 0.0299 #&gt; 3 Keisha 0.0383 #&gt; 4 Tremayne 0.0435 #&gt; 5 Kareem 0.0469 #&gt; 6 Darnell 0.0476 #&gt; # ... with 30 more rows 3.4 Causal Affects and the Counterfactual Load the data using the readr function read_csv social_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot; social &lt;- read_csv(social_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; sex = col_character(), #&gt; yearofbirth = col_integer(), #&gt; primary2004 = col_integer(), #&gt; messages = col_character(), #&gt; primary2006 = col_integer(), #&gt; hhsize = col_integer() #&gt; ) summary(social) #&gt; sex yearofbirth primary2004 messages #&gt; Length:305866 Min. :1900 Min. :0.000 Length:305866 #&gt; Class :character 1st Qu.:1947 1st Qu.:0.000 Class :character #&gt; Mode :character Median :1956 Median :0.000 Mode :character #&gt; Mean :1956 Mean :0.401 #&gt; 3rd Qu.:1965 3rd Qu.:1.000 #&gt; Max. :1986 Max. :1.000 #&gt; primary2006 hhsize #&gt; Min. :0.000 Min. :1.00 #&gt; 1st Qu.:0.000 1st Qu.:2.00 #&gt; Median :0.000 Median :2.00 #&gt; Mean :0.312 Mean :2.18 #&gt; 3rd Qu.:1.000 3rd Qu.:2.00 #&gt; Max. :1.000 Max. :8.00 Use a grouped summarize instead of tapply, gotv_by_group &lt;- social %&gt;% group_by(messages) %&gt;% summarize(turnout = mean(primary2006)) gotv_by_group #&gt; # A tibble: 4 × 2 #&gt; messages turnout #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Get the turnout for the control group gotv_control &lt;- (filter(gotv_by_group, messages == &quot;Control&quot;))[[&quot;turnout&quot;]] Subtract the control group turnout from all groups gotv_by_group %&gt;% mutate(diff_control = turnout - gotv_control) #&gt; # A tibble: 4 × 3 #&gt; messages turnout diff_control #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.0179 #&gt; 2 Control 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.0257 #&gt; 4 Neighbors 0.378 0.0813 We could have also done this in one step like, gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 We can compare the differences of variables across the groups easily using a grouped summarize gotv_by_group %&gt;% mutate(control = mean(turnout[messages == &quot;Control&quot;]), control_diff = turnout - control) #&gt; # A tibble: 4 × 4 #&gt; messages turnout control control_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0.0000 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Pro-tip The summarise_at functions allows you summarize one-or-more columns with one-or-more functions. In addition to age, 2004 turnout, and household size, we’ll also compare propotion female, social %&gt;% group_by(messages) %&gt;% mutate(age = 2006 - yearofbirth, female = (sex == &quot;female&quot;)) %&gt;% select(-age, -sex) %&gt;% summarise_all(mean) #&gt; # A tibble: 4 × 6 #&gt; messages yearofbirth primary2004 primary2006 hhsize female #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 1956 0.399 0.315 2.19 0.500 #&gt; 2 Control 1956 0.400 0.297 2.18 0.499 #&gt; 3 Hawthorne 1956 0.403 0.322 2.18 0.499 #&gt; 4 Neighbors 1956 0.407 0.378 2.19 0.500 3.5 Observational Studies Load the minwage dataset from its URL using readr::read_csv: minwage_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/minwage.csv&quot; minwage &lt;- read_csv(minwage_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; chain = col_character(), #&gt; location = col_character(), #&gt; wageBefore = col_double(), #&gt; wageAfter = col_double(), #&gt; fullBefore = col_double(), #&gt; fullAfter = col_double(), #&gt; partBefore = col_double(), #&gt; partAfter = col_double() #&gt; ) glimpse(minwage) #&gt; Observations: 358 #&gt; Variables: 8 #&gt; $ chain &lt;chr&gt; &quot;wendys&quot;, &quot;wendys&quot;, &quot;burgerking&quot;, &quot;burgerking&quot;, &quot;kf... #&gt; $ location &lt;chr&gt; &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA... #&gt; $ wageBefore &lt;dbl&gt; 5.00, 5.50, 5.00, 5.00, 5.25, 5.00, 5.00, 5.00, 5.0... #&gt; $ wageAfter &lt;dbl&gt; 5.25, 4.75, 4.75, 5.00, 5.00, 5.00, 4.75, 5.00, 4.5... #&gt; $ fullBefore &lt;dbl&gt; 20.0, 6.0, 50.0, 10.0, 2.0, 2.0, 2.5, 40.0, 8.0, 10... #&gt; $ fullAfter &lt;dbl&gt; 0.0, 28.0, 15.0, 26.0, 3.0, 2.0, 1.0, 9.0, 7.0, 18.... #&gt; $ partBefore &lt;dbl&gt; 20.0, 26.0, 35.0, 17.0, 8.0, 10.0, 20.0, 30.0, 27.0... #&gt; $ partAfter &lt;dbl&gt; 36, 3, 18, 9, 12, 9, 25, 32, 39, 10, 20, 4, 13, 20,... summary(minwage) #&gt; chain location wageBefore wageAfter #&gt; Length:358 Length:358 Min. :4.25 Min. :4.25 #&gt; Class :character Class :character 1st Qu.:4.25 1st Qu.:5.05 #&gt; Mode :character Mode :character Median :4.50 Median :5.05 #&gt; Mean :4.62 Mean :4.99 #&gt; 3rd Qu.:4.99 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :6.25 #&gt; fullBefore fullAfter partBefore partAfter #&gt; Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 2.1 1st Qu.: 2.0 1st Qu.:11.0 1st Qu.:11.0 #&gt; Median : 6.0 Median : 6.0 Median :16.2 Median :17.0 #&gt; Mean : 8.5 Mean : 8.4 Mean :18.8 Mean :18.7 #&gt; 3rd Qu.:12.0 3rd Qu.:12.0 3rd Qu.:25.0 3rd Qu.:25.0 #&gt; Max. :60.0 Max. :40.0 Max. :60.0 Max. :60.0 First, calcualte the proportion of restraunts by state whose hourly wages were less than the minimum wage in NJ, $5.05, for wageBefore and wageAfter: Since the NJ minimum wage was $5.05, we’ll define a variable with that value. Even if you use them only once or twice, it is a good idea to put values like this in variables. It makes your code closer to self-documenting.n NJ_MINWAGE &lt;- 5.05 Later, it will be easier to understand wageAfter &lt; NJ_MINWAGE without any comments than it would be to understand wageAfter &lt; 5.05. In the latter case you’d have to remember that the new NJ minimum wage was 5.05 and that’s why you were using that value. This is an example of a magic number: try to avoid them. Note that location has multiple values: PA and four regions of NJ. So we’ll add a state variable to the data. minwage %&gt;% count(location) #&gt; # A tibble: 5 × 2 #&gt; location n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 centralNJ 45 #&gt; 2 northNJ 146 #&gt; 3 PA 67 #&gt; 4 shoreNJ 33 #&gt; 5 southNJ 67 We can extract the state from the final two characters of the location variable using the stringr function str_sub (R4DS Ch 14: Strings): library(stringr) minwage &lt;- mutate(minwage, state = str_sub(location, -2L)) Alternatively, since everything is either PA or NJ minwage &lt;- mutate(minwage, state = if_else(location == &quot;PA&quot;, &quot;PA&quot;, &quot;NJ&quot;)) Let’s confirm that the restraunts followed the law: minwage %&gt;% group_by(state) %&gt;% summarise(prop_after = mean(wageAfter &lt; NJ_MINWAGE), prop_Before = mean(wageBefore &lt; NJ_MINWAGE)) #&gt; # A tibble: 2 × 3 #&gt; state prop_after prop_Before #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.00344 0.911 #&gt; 2 PA 0.95522 0.940 Create a variable for the proportion of full-time employees in NJ and PA minwage &lt;- minwage %&gt;% mutate(totalAfter = fullAfter + partAfter, fullPropAfter = fullAfter / totalAfter) Now calculate the average for each state: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state #&gt; # A tibble: 2 × 2 #&gt; state fullPropAfter #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ 0.320 #&gt; 2 PA 0.272 We could compute the difference by (filter(full_prop_by_state, state == &quot;NJ&quot;)[[&quot;fullPropAfter&quot;]] - filter(full_prop_by_state, state == &quot;PA&quot;)[[&quot;fullPropAfter&quot;]]) #&gt; [1] 0.0481 or using tidyr functions spread (R4DS Ch 11: Tidy Data): spread(full_prop_by_state, state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 1 × 3 #&gt; NJ PA diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.32 0.272 0.0481 3.5.1 Confounding Bias We can calculate the proportion of fast-food restraunts in each chain in each state: chains_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% count(chain) %&gt;% mutate(prop = n / sum(n)) We can easily compare these using a simple dot-plot: ggplot(chains_by_state, aes(x = chain, y = prop, colour = state)) + geom_point() + coord_flip() In the QSS text, only Burger King restraunts are compared. However, dplyr makes this easy. All we have to do is change the group_by statement we used last time, and add chain to it: full_prop_by_state_chain &lt;- minwage %&gt;% group_by(state, chain) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state_chain #&gt; Source: local data frame [8 x 3] #&gt; Groups: state [?] #&gt; #&gt; state chain fullPropAfter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ burgerking 0.358 #&gt; 2 NJ kfc 0.328 #&gt; 3 NJ roys 0.283 #&gt; 4 NJ wendys 0.260 #&gt; 5 PA burgerking 0.321 #&gt; 6 PA kfc 0.236 #&gt; # ... with 2 more rows We can plot and compare the proportions easily in this format. In general, ordering categorical variables alphabetically is useless, so we’ll order the chains by the average of the NJ and PA fullPropAfter, using forcats::fct_reorder: ggplot(full_prop_by_state_chain, aes(x = forcats::fct_reorder(chain, fullPropAfter), y = fullPropAfter, colour = state)) + geom_point() + coord_flip() + labs(x = &quot;chains&quot;) To calculate the differences, we need to get the data frame The join method. Create New Jersey and Pennsylvania datasets with chain and prop full employed columns. Merge the two datasets on chain. chains_nj &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(-state) %&gt;% rename(NJ = fullPropAfter) chains_pa &lt;- full_prop_by_state_chain %&gt;% ungroup() %&gt;% filter(state == &quot;PA&quot;) %&gt;% select(-state) %&gt;% rename(PA = fullPropAfter) full_prop_state_chain_diff &lt;- full_join(chains_nj, chains_pa, by = &quot;chain&quot;) %&gt;% mutate(diff = NJ - PA) full_prop_state_chain_diff #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 Q: In the code above why did I remove the state variable and rename the fullPropAfter variable before merging? What happens if I didn’t? The spread/gather method. We can also use the spread and gather functions from tidyr. In this example it is much more compact code. full_prop_by_state_chain %&gt;% spread(state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 4 × 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 3.5.2 Before and After and Difference-in-Difference Designs To compute the estimates in the before and after design first create a variable for the difference before and after the law passed. minwage &lt;- minwage %&gt;% mutate(totalBefore = fullBefore + partBefore, fullPropBefore = fullBefore / totalBefore) The before-and-after analysis is the difference between the full-time employment before and after the minimum wage law passed looking only at NJ: filter(minwage, state == &quot;NJ&quot;) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) #&gt; # A tibble: 1 × 1 #&gt; diff #&gt; &lt;dbl&gt; #&gt; 1 0.0239 The difference-in-differences design uses the difference in the before-and-after differences for each state. diff_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) filter(diff_by_state, state == &quot;NJ&quot;)[[&quot;diff&quot;]] - filter(diff_by_state, state == &quot;PA&quot;)[[&quot;diff&quot;]] #&gt; [1] 0.0616 Let’s create a single dataset with the mean values of each state before and after to visually look at each of these designs: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(fullPropAfter, fullPropBefore), mean) %&gt;% gather(period, fullProp, -state) %&gt;% mutate(period = recode(period, fullPropAfter = 1, fullPropBefore = 0)) full_prop_by_state #&gt; # A tibble: 4 × 3 #&gt; state period fullProp #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 1 0.320 #&gt; 2 PA 1 0.272 #&gt; 3 NJ 0 0.297 #&gt; 4 PA 0 0.310 ggplot(full_prop_by_state, aes(x = period, y = fullProp, colour = state)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(0, 1), labels = c(&quot;Before&quot;, &quot;After&quot;)) 3.6 Descriptive Statistics for a Single Variable To calculate the summary for the variables wageBefore and wageAfter: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(wageBefore, wageAfter) %&gt;% summary() #&gt; wageBefore wageAfter #&gt; Min. :4.25 Min. :5.00 #&gt; 1st Qu.:4.25 1st Qu.:5.05 #&gt; Median :4.50 Median :5.05 #&gt; Mean :4.61 Mean :5.08 #&gt; 3rd Qu.:4.87 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :5.75 We calculate the IQR for each state’s wages after the passage of the law using the same grouped summarise as we used before: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter = IQR(wageAfter), wageBefore = IQR(wageBefore)) #&gt; # A tibble: 2 × 3 #&gt; state wageAfter wageBefore #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.000 0.62 #&gt; 2 PA 0.575 0.75 Calculate the variance and standard deviation of wageAfter and wageBefore for each state: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter_sd = sd(wageAfter), wageAfter_var = var(wageAfter), wageBefore_sd = sd(wageBefore), wageBefore_var = var(wageBefore)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageAfter_var wageBefore_sd wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.0112 0.343 0.118 #&gt; 2 PA 0.359 0.1291 0.358 0.128 or, more compactly, using summarise_at: minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(wageAfter, wageBefore), funs(sd, var)) #&gt; # A tibble: 2 × 5 #&gt; state wageAfter_sd wageBefore_sd wageAfter_var wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.343 0.0112 0.118 #&gt; 2 PA 0.359 0.358 0.1291 0.128 "],
["measurement.html", "4 Measurement 4.1 Prerequisites 4.2 Measuring Civilian Victimization during Wartime 4.3 Visualizing the Univariate Distribution 4.4 Survey Sampling 4.5 load village data 4.6 Measuring Political Polarization 4.7 Clustering", " 4 Measurement 4.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats library(&quot;forcats&quot;) library(&quot;broom&quot;) 4.2 Measuring Civilian Victimization during Wartime afghan_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan.csv&quot; afghan &lt;- read_csv(afghan_url) Summarize the variables of interest afghan %&gt;% select(age, educ.years, employed, income) %&gt;% summary() #&gt; age educ.years employed income #&gt; Min. :15.0 Min. : 0 Min. :0.000 Length:2754 #&gt; 1st Qu.:22.0 1st Qu.: 0 1st Qu.:0.000 Class :character #&gt; Median :30.0 Median : 1 Median :1.000 Mode :character #&gt; Mean :32.4 Mean : 4 Mean :0.583 #&gt; 3rd Qu.:40.0 3rd Qu.: 8 3rd Qu.:1.000 #&gt; Max. :80.0 Max. :18 Max. :1.000 With income, read_csv never converts strings to factors by default. To get a summary of the different levels, either convert it to a factor (R4DS Ch 15), or use count() afghan %&gt;% count(income) #&gt; # A tibble: 6 × 2 #&gt; income n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 10,001-20,000 616 #&gt; 2 2,001-10,000 1420 #&gt; 3 20,001-30,000 93 #&gt; 4 less than 2,000 457 #&gt; 5 over 30,000 14 #&gt; 6 &lt;NA&gt; 154 Count the number a proportion of respondents who answer that they were harmed by the ISF (violent.exp.ISAF) and (violent.exp.taliban) respectively, afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 9 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.48293 #&gt; 2 0 1 354 0.12854 #&gt; 3 0 NA 22 0.00799 #&gt; 4 1 0 475 0.17248 #&gt; 5 1 1 526 0.19099 #&gt; 6 1 NA 22 0.00799 #&gt; # ... with 3 more rows We need to use ungroup() in order to ensure that sum(n) sums over the entire dataset as opposed to only within categories of violent.exp.ISAF. Unlike prop.table, the code above does not drop missing values. We can drop those values by adding a filter verb and using !is.na() to test for missing values in those variables: afghan %&gt;% filter(!is.na(violent.exp.ISAF), !is.na(violent.exp.taliban)) %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 4 × 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.495 #&gt; 2 0 1 354 0.132 #&gt; 3 1 0 475 0.177 #&gt; 4 1 1 526 0.196 4.2.1 Handling Missing Data in R We already observed the issues with NA values in calculating the proportion answering the “experienced violence” questions. You can filter rows with specific variables having missing values using filter as shown above. Howeer, na.omit works with tibbles just like any other data frame. na.omit(afghan) #&gt; # A tibble: 2,554 × 11 #&gt; province district village.id age educ.years employed income #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Logar Baraki Barak 80 26 10 0 2,001-10,000 #&gt; 2 Logar Baraki Barak 80 49 3 1 2,001-10,000 #&gt; 3 Logar Baraki Barak 80 60 0 1 2,001-10,000 #&gt; 4 Logar Baraki Barak 80 34 14 1 2,001-10,000 #&gt; 5 Logar Baraki Barak 80 21 12 1 2,001-10,000 #&gt; 6 Logar Baraki Barak 80 42 6 1 10,001-20,000 #&gt; # ... with 2,548 more rows, and 4 more variables: violent.exp.ISAF &lt;int&gt;, #&gt; # violent.exp.taliban &lt;int&gt;, list.group &lt;chr&gt;, list.response &lt;int&gt; 4.3 Visualizing the Univariate Distribution 4.3.1 Barplot library(forcats) afghan &lt;- afghan %&gt;% mutate(violent.exp.ISAF.fct = fct_explicit_na(fct_recode(factor(violent.exp.ISAF), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the ISAF&quot;) afghan &lt;- afghan %&gt;% mutate(violent.exp.taliban.fct = fct_explicit_na(fct_recode(factor(violent.exp.taliban), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the Taliban&quot;) TODO This plot could improved by plotting the two values simultaneously to be able to better compare them. dodged bar plot dot-plot This will require creating a data frame that has the following columns: perpetrator (ISAF, Taliban), response (No Harm, Harm, No response). See the section on Tidy Data and spread gather. TODO Compare them by region, ? 4.3.2 Boxplot ggplot(afghan, aes(x = 1, y = age)) + geom_boxplot() + labs(y = &quot;Age&quot;, x = &quot;&quot;) + ggtitle(&quot;Distribution of Age&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_boxplot() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;) + ggtitle(&quot;Education by Provice&quot;) Helmand and Uruzgan have much lower levels of education than the other provicnces, and also report higher levels of violence. afghan %&gt;% group_by(province) %&gt;% summarise(educ.years = mean(educ.years, na.rm = TRUE), violent.exp.taliban = mean(violent.exp.taliban, na.rm = TRUE), violent.exp.ISAF = mean(violent.exp.ISAF, na.rm = TRUE)) %&gt;% arrange(educ.years) #&gt; # A tibble: 5 × 4 #&gt; province educ.years violent.exp.taliban violent.exp.ISAF #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 1.04 0.4545 0.496 #&gt; 2 Helmand 1.60 0.5042 0.541 #&gt; 3 Khost 5.79 0.2332 0.242 #&gt; 4 Kunar 5.93 0.3030 0.399 #&gt; 5 Logar 6.70 0.0802 0.144 4.3.3 Printing and saving graphics Use the function ggsave() to save ggplot graphics. Also, RMarkdown files have their own means of creating and saving plots created by code-chunks. 4.4 Survey Sampling 4.4.1 The Role of Randomization 4.5 load village data afghan_village_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/afghan-village.csv&quot; afghan.village &lt;- read_csv(afghan_village_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; altitude = col_double(), #&gt; population = col_integer(), #&gt; village.surveyed = col_integer() #&gt; ) Box-plots of altitude ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = altitude)) + geom_boxplot() + labs(y = &quot;Altitude (meter)&quot;, x = &quot;&quot;) Boxplots log-population values of sampled and non-sampled ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = log(population))) + geom_boxplot() + labs(y = &quot;log(population)&quot;, x = &quot;&quot;) You can also compare these distributions by plotting their densities: ggplot(afghan.village, aes(colour = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), x = log(population))) + geom_density() + geom_rug() + labs(x = &quot;log(population)&quot;, colour = &quot;&quot;) 4.5.1 Non-response and other sources of bias Calculate the rates of non-response by province to violent.exp.ISAF and violent.exp.taliban: afghan %&gt;% group_by(province) %&gt;% summarise(ISAF = mean(is.na(violent.exp.ISAF)), taliban = mean(is.na(violent.exp.taliban))) %&gt;% arrange(-ISAF) #&gt; # A tibble: 5 × 3 #&gt; province ISAF taliban #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 0.02067 0.06202 #&gt; 2 Helmand 0.01637 0.03041 #&gt; 3 Khost 0.00476 0.00635 #&gt; 4 Kunar 0.00000 0.00000 #&gt; 5 Logar 0.00000 0.00000 Calculat the proportion who support the ISAF using the difference in means between the ISAF and control groups: (mean(filter(afghan, list.group == &quot;ISAF&quot;)$list.response) - mean(filter(afghan, list.group == &quot;control&quot;)$list.response)) #&gt; [1] 0.049 To calculate the table responses to the list expriment in the control, ISAF, and taliban groups&gt; afghan %&gt;% group_by(list.response, list.group) %&gt;% count() %T&gt;% glimpse() %&gt;% spread(list.group, n, fill = 0) #&gt; Observations: 12 #&gt; Variables: 3 #&gt; $ list.response &lt;int&gt; 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4 #&gt; $ list.group &lt;chr&gt; &quot;control&quot;, &quot;ISAF&quot;, &quot;control&quot;, &quot;ISAF&quot;, &quot;taliban&quot;,... #&gt; $ n &lt;int&gt; 188, 174, 265, 278, 433, 265, 260, 287, 200, 182... #&gt; Source: local data frame [5 x 4] #&gt; Groups: list.response [5] #&gt; #&gt; list.response control ISAF taliban #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 188 174 0 #&gt; 2 1 265 278 433 #&gt; 3 2 265 260 287 #&gt; 4 3 200 182 198 #&gt; 5 4 0 24 0 4.6 Measuring Political Polarization congress_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/congress.csv&quot; congress &lt;- read_csv(congress_url) #&gt; Parsed with column specification: #&gt; cols( #&gt; congress = col_integer(), #&gt; district = col_integer(), #&gt; state = col_character(), #&gt; party = col_character(), #&gt; name = col_character(), #&gt; dwnom1 = col_double(), #&gt; dwnom2 = col_double() #&gt; ) glimpse(congress) #&gt; Observations: 14,552 #&gt; Variables: 7 #&gt; $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8... #&gt; $ district &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 98, 98, 1, 2, 3, 4, 5, ... #&gt; $ state &lt;chr&gt; &quot;USA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;A... #&gt; $ party &lt;chr&gt; &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Demo... #&gt; $ name &lt;chr&gt; &quot;TRUMAN&quot;, &quot;BOYKIN F.&quot;, &quot;GRANT G.&quot;, &quot;ANDREWS G.&quot;, &quot;... #&gt; $ dwnom1 &lt;dbl&gt; -0.276, -0.026, -0.042, -0.008, -0.082, -0.170, -0.12... #&gt; $ dwnom2 &lt;dbl&gt; 0.016, 0.796, 0.999, 1.005, 1.066, 0.870, 0.990, 0.89... To create the scatterplot in 3.6, we can congress %&gt;% filter(congress %in% c(80, 112), party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) congress %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) #&gt; Warning: Removed 2 rows containing missing values (geom_point). congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = congress, y = dwnom1, colour = fct_reorder2(party, congress, dwnom1))) + geom_line() + labs(y = &quot;DW-NOMINATE score (1st Dimension)&quot;, x = &quot;Congress&quot;, colour = &quot;Party&quot;) 4.6.1 Correlation Let’s plot the Gini coefficient usgini_url &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/MEASUREMENT/USGini.csv&quot; USGini &lt;- read_csv(usgini_url) #&gt; Warning: Missing column names filled in: &#39;X1&#39; [1] #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_integer(), #&gt; year = col_integer(), #&gt; gini = col_double() #&gt; ) ggplot(USGini, aes(x = year, y = gini)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Gini coefficient&quot;) + ggtitle(&quot;Income Inequality&quot;) To calculate a measure of party polarization take the code used in the plot of Republican and Democratic party median ideal points and adapt it to calculate the difference in the party medians: party_polarization &lt;- congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% spread(party, dwnom1) %&gt;% mutate(polarization = Republican - Democrat) party_polarization #&gt; Source: local data frame [33 x 4] #&gt; Groups: congress [33] #&gt; #&gt; congress Democrat Republican polarization #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 80 -0.146 0.276 0.421 #&gt; 2 81 -0.195 0.264 0.459 #&gt; 3 82 -0.180 0.265 0.445 #&gt; 4 83 -0.181 0.261 0.442 #&gt; 5 84 -0.209 0.261 0.471 #&gt; 6 85 -0.214 0.250 0.464 #&gt; # ... with 27 more rows ggplot(party_polarization, aes(x = congress, y = polarization)) + geom_point() + ggtitle(&quot;Political Polarization&quot;) + labs(x = &quot;Year&quot;, y = &quot;Republican median − Democratic median&quot;) 4.6.2 Quantile-Quantile Plot To create histogram plots similar congress %&gt;% filter(congress == 112, party %in% c(&quot;Republican&quot;, &quot;Democrat&quot;)) %&gt;% ggplot(aes(x = dwnom2, y = ..density..)) + geom_histogram() + facet_grid(. ~ party) + labs(x = &quot;racial liberalism/conservatism dimension&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot2 includes a stat_qq which can be used to create qq-plots but it is more suited to comparing a sample distribution with a theoretical distibution, usually the normal one. However, we can calculate one by hand, which may give more insight into exactly what the qq-plot is doing. party_qtiles &lt;- tibble( probs = seq(0, 1, by = 0.01), Democrat = quantile(filter(congress, congress == 112, party == &quot;Democrat&quot;)$dwnom2, probs = probs), Republican = quantile(filter(congress, congress == 112, party == &quot;Republican&quot;)$dwnom2, probs = probs) ) party_qtiles #&gt; # A tibble: 101 × 3 #&gt; probs Democrat Republican #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00 -0.925 -1.381 #&gt; 2 0.01 -0.672 -0.720 #&gt; 3 0.02 -0.619 -0.566 #&gt; 4 0.03 -0.593 -0.526 #&gt; 5 0.04 -0.567 -0.468 #&gt; 6 0.05 -0.560 -0.436 #&gt; # ... with 95 more rows The plot looks different than the one in the text since the x- and y-scales are in the original values instead of z-scores (see the next section). party_qtiles %&gt;% ggplot(aes(x = Democrat, y = Republican)) + geom_point() + geom_abline() + coord_fixed() 4.7 Clustering 4.7.1 Matrices While matrices are great for numerical computations, such as when you are implementing algorithms, generally keeping data in data frames is more convenient for data wrangling. 4.7.2 Lists See R4DS Chapter 20: Vectors, Chapter 21: Iteration and the purrr package for more powerful methods of computing on lists. 4.7.3 k-means algorithms TODO A good visualization of the k-means algorithm and a simple, naive implementation in R. Calculate the clusters by the 80th and 112th congresses, k80two.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 2, nstart = 5) Add the cluster ids to datasets congress80 &lt;- congress %&gt;% filter(congress == 80) %&gt;% mutate(cluster2 = factor(k80two.out$cluster)) We will also create a data sets with the cluster centroids. These are in the centers element of the cluster object. k80two.out$centers #&gt; dwnom1 dwnom2 #&gt; 1 -0.0484 0.783 #&gt; 2 0.1468 -0.339 To make it easier to use with ggplot, we need to convert this to a data frame. The tidy function from the broom package: k80two.clusters &lt;- tidy(k80two.out) k80two.clusters #&gt; x1 x2 size withinss cluster #&gt; 1 -0.0484 0.783 135 10.9 1 #&gt; 2 0.1468 -0.339 311 54.9 2 Plot the ideal points and clusters ggplot() + geom_point(data = congress80, aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80two.clusters, mapping = aes(x = x1, y = x2)) We can also plot, congress80 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [5 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 132 #&gt; 2 Democrat 2 62 #&gt; 3 Other 2 2 #&gt; 4 Republican 1 3 #&gt; 5 Republican 2 247 And now we can repeat these steps for the 112th congress: k112two.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 2, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112two.out$cluster)) k112two.clusters &lt;- tidy(k112two.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112two.clusters, mapping = aes(x = x1, y = x2)) congress112 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; Source: local data frame [3 x 3] #&gt; Groups: party [?] #&gt; #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 2 200 #&gt; 2 Republican 1 242 #&gt; 3 Republican 2 1 Now repeat the same with four clusters on the 80th congress: k80four.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 4, nstart = 5) congress80 &lt;- filter(congress, congress == 80) %&gt;% mutate(cluster2 = factor(k80four.out$cluster)) k80four.clusters &lt;- tidy(k80four.out) ggplot() + geom_point(data = congress80, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80four.clusters, mapping = aes(x = x1, y = x2), size = 3) and on the 112th congress: k112four.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 4, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112four.out$cluster)) k112four.clusters &lt;- tidy(k112four.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112four.clusters, mapping = aes(x = x1, y = x2), size = 3) "],
["prediction.html", "5 Prediction 5.1 Prerequisites 5.2 Loops in R 5.3 General Conditional Statements in R 5.4 Poll Predictions 5.5 Linear Regression", " 5 Prediction 5.1 Prerequisites library(&quot;tidyverse&quot;) We will use the package lubridate to work with dates. library(&quot;lubridate&quot;) #&gt; Loading required package: methods #&gt; #&gt; Attaching package: &#39;lubridate&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; date The packages modelr and broom are used to wrangle the results of linear regressions, library(&quot;broom&quot;) library(&quot;modelr&quot;) #&gt; #&gt; Attaching package: &#39;modelr&#39; #&gt; The following object is masked from &#39;package:broom&#39;: #&gt; #&gt; bootstrap 5.2 Loops in R For more on looping and iteration in R see the R for Data Science chapter Iteration. RStudio provides many features to help debugging, which will be useful in for loops and function: see this article for an example. 5.3 General Conditional Statements in R TODO Is this in R4DS, find good cites. If you are using conditional statements to assign values for data frame, see the dplyr functions if_else, recode, and case_when 5.4 Poll Predictions Load the election polls and election results for 2008, polls08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;polls08.csv&quot;)) glimpse(polls08) #&gt; Observations: 1,332 #&gt; Variables: 5 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;,... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;SurveyUSA-2&quot;, &quot;Ca... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34, 37, 36, 36, 3... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47, 55, 51, 49, 5... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 2008-10-06, 2008... pres08 &lt;- read_csv(qss_data_url(&quot;UNCERTAINTY&quot;, &quot;pres08.csv&quot;)) glimpse(pres08) #&gt; Observations: 51 #&gt; Variables: 5 #&gt; $ state.name &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo... #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE... #&gt; $ Obama &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36,... #&gt; $ McCain &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, ... #&gt; $ EV &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, ... Compute Obama’s margin in polls and final election polls08 &lt;- polls08 %&gt;% mutate(margin = Obama - McCain) pres08 &lt;- pres08 %&gt;% mutate(margin = Obama - McCain) To work with dates, the R package lubridate makes wrangling them much easier. See the R for Data Science chapter Dates and Times. The function ymd will convert character strings like year-month-day and more into dates, as long as the order is (year, month, day). See dmy, ymd, and others for other ways to convert strings to dates. x &lt;- ymd(&quot;2008-11-04&quot;) y &lt;- ymd(&quot;2008/9/1&quot;) x - y #&gt; Time difference of 64 days However, note that in polls08, the date middate is already a date object, class(polls08$middate) #&gt; [1] &quot;Date&quot; The function read_csv by default will check character vectors to see if they have patterns that appear to be dates, and if so, will parse those columns as dates. We’ll create a variable for election day ELECTION_DAY &lt;- ymd(&quot;2008-11-04&quot;) and add a new column to poll08 with the days to the election polls08 &lt;- mutate(polls08, ELECTION_DAY - middate) Although the code in the chapter uses a for loop, there is no reason to do so. We can accomplish the same task by merging the election results data to the polling data by state. polls_w_results &lt;- left_join(polls08, select(pres08, state, elec_margin = margin), by = &quot;state&quot;) %&gt;% mutate(error = elec_margin - margin) glimpse(polls_w_results) #&gt; Observations: 1,332 #&gt; Variables: 9 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;Sur... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34,... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47,... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 20... #&gt; $ margin &lt;int&gt; -25, -20, -27, -20, -21, -30, -22, -27,... #&gt; $ ELECTION_DAY - middate &lt;time&gt; 8 days, 20 days, 27 days, 29 days, 43 ... #&gt; $ elec_margin &lt;int&gt; -21, -21, -21, -21, -21, -21, -21, -21,... #&gt; $ error &lt;int&gt; 4, -1, 6, -1, 0, 9, 1, 6, -1, -8, -3, -... To get the last poll in each state, arrange and filter on middate last_polls &lt;- polls_w_results %&gt;% arrange(state, desc(middate)) %&gt;% group_by(state) %&gt;% slice(1) last_polls #&gt; Source: local data frame [51 x 9] #&gt; Groups: state [51] #&gt; #&gt; state Pollster Obama McCain middate margin #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 AK Research 2000-3 39 58 2008-10-29 -19 #&gt; 2 AL SurveyUSA-2 36 61 2008-10-27 -25 #&gt; 3 AR ARG-4 44 51 2008-10-29 -7 #&gt; 4 AZ ARG-3 46 50 2008-10-29 -4 #&gt; 5 CA SurveyUSA-3 60 36 2008-10-30 24 #&gt; 6 CO ARG-3 52 45 2008-10-29 7 #&gt; # ... with 45 more rows, and 3 more variables: `ELECTION_DAY - #&gt; # middate` &lt;time&gt;, elec_margin &lt;int&gt;, error &lt;int&gt; Challenge: Instead of using the last poll, use the average of polls in the last week? Last month? How do the margins on the polls change over the election period? To simplify things for later, let’s define a function rmse which calculates the root mean squared error, as defined in the book. See the R for Data Science chapter Functions for more on writing functions. rmse &lt;- function(actual, pred) { sqrt(mean((actual - pred) ^ 2)) } Now we can use rmse() to calculate the RMSE for all the final polls: rmse(last_polls$margin, last_polls$elec_margin) #&gt; [1] 5.88 Or since we already have a variable error, sqrt(mean(last_polls$error ^ 2)) #&gt; [1] 5.88 The mean prediction error is mean(last_polls$error) #&gt; [1] 1.08 This is slightly different than what is in the book due to the difference in the poll used as the final poll; many states have many polls on the last day. I’ll choose binwidths of 1%, since that is fairly interpretable: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 1, boundary = 0) The text uses bindwidths of 5%: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 5, boundary = 0) Challenge: What other ways could you visualize the results? How would you show all states? What about plotting the absolute or squared errors instead of the errors? Challenge: What happens to prediction error if you average polls? Consider averaging back over time? What happens if you take the averages of the state poll average and average of all polls - does that improve prediction? To create a scatterplot using the state abbreviations instead of points use geom_text instead of geom_point. ggplot(last_polls, aes(x = margin, y = elec_margin, label = state)) + geom_text() + geom_abline(col = &quot;red&quot;) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + coord_fixed() + labs(x = &quot;Poll Results&quot;, y = &quot;Actual Election Results&quot;) We can create a confusion matrix as follow. Create a new columns classification which shows whether how the poll’s classification was related to the actual election outcome (“true positive”, “false positive”, “false negative”, “false positive”). This can be accomplished easily with the dplyr funtion case_when. Note: You need to use . to refer to the data frame when using case_when with a mutate function. last_polls &lt;- last_polls %&gt;% ungroup() %&gt;% mutate(classification = case_when( (.$margin &gt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;true positive&quot;, (.$margin &gt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;false positive&quot;, (.$margin &lt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;true negative&quot;, (.$margin &lt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;false negative&quot; )) No simply count the last_polls %&gt;% group_by(classification) %&gt;% count() #&gt; # A tibble: 4 × 2 #&gt; classification n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 false negative 2 #&gt; 2 false positive 1 #&gt; 3 true negative 21 #&gt; 4 true positive 27 Which states were incorrectly predicted? last_polls %&gt;% filter(classification %in% c(&quot;false positive&quot;, &quot;false negative&quot;)) %&gt;% select(state, margin, elec_margin, classification) %&gt;% arrange(desc(elec_margin)) #&gt; # A tibble: 3 × 4 #&gt; state margin elec_margin classification #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 IN -5 1 false negative #&gt; 2 NC -1 1 false negative #&gt; 3 MO 1 -1 false positive What was the difference in the poll prediction of electoral votes and actual electoral votes. We hadn’t included the variable EV when we first merged, but that’s no problem, we’ll just merge again in order to grab that variable: last_polls %&gt;% left_join(select(pres08, state, EV), by = &quot;state&quot;) %&gt;% summarise(EV_pred = sum((margin &gt; 0) * EV), EV_actual = sum((elec_margin &gt; 0) * EV)) #&gt; # A tibble: 1 × 2 #&gt; EV_pred EV_actual #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 349 364 To look at predictions of the # load the data pollsUS08 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pollsUS08.csv&quot;)) %&gt;% mutate(DaysToElection = ELECTION_DAY - middate) We’ll produce the seven-day averages slightly differently than the method used in the text. For all dates in the data, we’ll calculate the moving average. all_dates &lt;- seq(min(polls08$middate), ELECTION_DAY, by = &quot;days&quot;) # Number of poll days to use POLL_DAYS &lt;- 7 pop_vote_avg &lt;- vector(length(all_dates), mode = &quot;list&quot;) for (i in seq_along(all_dates)) { date &lt;- all_dates[i] # summarise the seven day week_data &lt;- pollsUS08 %&gt;% filter(as.integer(middate - date) &lt;= 0, as.integer(middate - date) &gt; -POLL_DAYS) %&gt;% summarise(Obama = mean(Obama, na.rm = TRUE), McCain = mean(McCain, na.rm = TRUE)) # add date for the observation week_data$date &lt;- date pop_vote_avg[[i]] &lt;- week_data } pop_vote_avg &lt;- bind_rows(pop_vote_avg) It is easier to plot this if the data are tidy, with Obama and McCain as categories of a column candidate. pop_vote_avg_tidy &lt;- pop_vote_avg %&gt;% gather(candidate, share, -date, na.rm = TRUE) pop_vote_avg_tidy #&gt; # A tibble: 598 × 3 #&gt; date candidate share #&gt; * &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2008-01-09 Obama 49 #&gt; 2 2008-01-10 Obama 46 #&gt; 3 2008-01-11 Obama 45 #&gt; 4 2008-01-12 Obama 45 #&gt; 5 2008-01-13 Obama 45 #&gt; 6 2008-01-14 Obama 45 #&gt; # ... with 592 more rows ggplot(pop_vote_avg_tidy, aes(x = date, y = share, colour = forcats::fct_reorder2(candidate, date, share))) + geom_point() + geom_line() Challenge read R for Data Science chapter Iteration and use the function map_df instead of a for loop. The 7-day average is similar to the simple method used by Real Clear Politics. The RCP average is simply the average of all polls in their data for the last seven days. Sites like 538 and the Huffington Post on the other hand, also use what amounts to averaging polls, but using more sophisticated statistical methods to assign different weights to different polls. Challenge Why do we need to use different polls for the popular vote data? Why not simply average all the state polls? What would you have to do? Would the overall popular vote be useful in predicting state-level polling, or vice-versa? How would you use them? 5.5 Linear Regression 5.5.1 Facial Appearance and Election Outcomes Load the face dataset: face &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;face.csv&quot;)) Add Democrat and Republican vote shares, and the difference in shares: face &lt;- face %&gt;% mutate(d.share = d.votes / (d.votes + r.votes), r.share = r.votes / (d.votes + r.votes), diff.share = d.share - r.share) Plot facial competence vs. vote share: ggplot(face, aes(x = d.comp, y = diff.share, colour = w.party)) + geom_point() + labs(x = &quot;Competence scores for Democrats&quot;, y = &quot;Democratic margin in vote share&quot;) 5.5.2 Correlation 5.5.3 Least Squares Run the linear regression fit &lt;- lm(diff.share ~ d.comp, data = face) summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = diff.share ~ d.comp, data = face) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.675 -0.166 0.014 0.177 0.743 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.312 0.066 -4.73 6.2e-06 *** #&gt; d.comp 0.660 0.127 5.19 8.9e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.266 on 117 degrees of freedom #&gt; Multiple R-squared: 0.187, Adjusted R-squared: 0.18 #&gt; F-statistic: 27 on 1 and 117 DF, p-value: 8.85e-07 There are many functions to get data out of the lm model. In addition to these, the broom package provides three functions: glance, tidy, and augment that always return data frames. The function glance returns a one-row data-frame summary of the model, glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.187 0.18 0.266 27 8.85e-07 2 -10.5 27 35.3 #&gt; deviance df.residual #&gt; 1 8.31 117 The function tidy returns a data frame in which each row is a coefficient, tidy(fit) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) -0.312 0.066 -4.73 6.24e-06 #&gt; 2 d.comp 0.660 0.127 5.19 8.85e-07 The function augment returns the original data with fitted values, residuals, and other observation level stats from the model appended to it. augment(fit) %&gt;% head() #&gt; diff.share d.comp .fitted .se.fit .resid .hat .sigma .cooksd #&gt; 1 0.2101 0.565 0.0606 0.0266 0.1495 0.00996 0.267 0.001600 #&gt; 2 0.1194 0.342 -0.0864 0.0302 0.2059 0.01286 0.267 0.003938 #&gt; 3 0.0499 0.612 0.0922 0.0295 -0.0423 0.01229 0.268 0.000158 #&gt; 4 0.1965 0.542 0.0454 0.0256 0.1511 0.00922 0.267 0.001510 #&gt; 5 0.4958 0.680 0.1370 0.0351 0.3588 0.01737 0.266 0.016307 #&gt; 6 -0.3495 0.321 -0.1006 0.0319 -0.2490 0.01433 0.267 0.006436 #&gt; .std.resid #&gt; 1 0.564 #&gt; 2 0.778 #&gt; 3 -0.160 #&gt; 4 0.570 #&gt; 5 1.358 #&gt; 6 -0.941 We can plot the results of the bivariate linear regression as follows: ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_abline(slope = coef(fit)[&quot;d.comp&quot;], intercept = coef(fit)[&quot;(Intercept)&quot;]) A more general way to plot the predictions of the model against the data is to use the methods described in Ch 23.3.3 of R4DS. Create an evenly spaced grid of values of d.comp, and add predictions of the model to it. grid &lt;- face %&gt;% data_grid(d.comp) %&gt;% add_predictions(fit) head(grid) #&gt; # A tibble: 6 × 2 #&gt; d.comp pred #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0640 -0.270 #&gt; 2 0.0847 -0.256 #&gt; 3 0.0893 -0.253 #&gt; 4 0.1145 -0.237 #&gt; 5 0.1148 -0.236 #&gt; 6 0.1639 -0.204 Now we can plot the regression line and the original data just like any other plot. ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_line(data = grid, mapping = aes(x = d.comp, y = pred)) This method is more complicated than the geom_abline method for a bivariate regerssion, but will work for more complicated models, while the geom_abline method won’t. Note that geom_smooth can be used to add a regression line to a data-set. ggplot(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The argument method = &quot;lm&quot; specifies that the function lm is to be used to generate fitted values. It is equivalent to running the regression lm(y ~ x) and plotting the regression line, where y and x are the aesthetics specified by the mappings. The argument se = FALSE tells the function not to plot the confidence interval of the regresion (discussed later). 5.5.4 Regresion towards the mean 5.5.5 Merging Data Sets in R See the R for Data Science chapter Relational data. Merge - or intter join the data frames by state: pres12 &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;pres12.csv&quot;)) full_join(pres08, pres12, by = &quot;state&quot;) #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama.x McCain EV.x margin Obama.y Romney EV.y #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows Note: this could be a question. How would you change the .x, .y suffixes to something more informative To avoid the duplicate names, or change them, you can rename before merging, or use the suffix argument: pres &lt;- full_join(pres08, pres12, by = &quot;state&quot;, suffix = c(&quot;_08&quot;, &quot;_12&quot;)) pres #&gt; # A tibble: 51 × 9 #&gt; state.name state Obama_08 McCain EV_08 margin Obama_12 Romney EV_12 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 #&gt; # ... with 45 more rows The dplyr equivalent functions for cbind is bind_cols. pres &lt;- pres %&gt;% mutate(Obama2008.z = scale(Obama_08), Obama2012.z = scale(Obama_12)) Scatterplot of states with vote shares in 2008 and 2012 ggplot(pres, aes(x = Obama2008.z, y = Obama2012.z, label = state)) + geom_text() + geom_abline() + coord_fixed() + scale_x_continuous(&quot;Obama&#39;s standardized vote share in 2008&quot;, limits = c(-4, 4)) + scale_y_continuous(&quot;Obama&#39;s standardized vote share in 2012&quot;, limits = c(-4, 4)) To calcualte the bottom and top quartiles pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.25)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.583 pres %&gt;% filter(Obama2008.z &lt; quantile(Obama2008.z, 0.75)) %&gt;% summarise(improve = mean(Obama2012.z &gt; Obama2008.z)) #&gt; # A tibble: 1 × 1 #&gt; improve #&gt; &lt;dbl&gt; #&gt; 1 0.5 Challenge: Why is it important to standardize the vote shares? 5.5.6 Model Fit florida &lt;- read_csv(qss_data_url(&quot;prediction&quot;, &quot;florida.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; county = col_character(), #&gt; Clinton96 = col_integer(), #&gt; Dole96 = col_integer(), #&gt; Perot96 = col_integer(), #&gt; Bush00 = col_integer(), #&gt; Gore00 = col_integer(), #&gt; Buchanan00 = col_integer() #&gt; ) fit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida) summary(fit2) #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -612.7 -66.0 1.9 32.9 2301.7 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.34575 49.75931 0.03 0.98 #&gt; Perot96 0.03592 0.00434 8.28 9.5e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 316 on 65 degrees of freedom #&gt; Multiple R-squared: 0.513, Adjusted R-squared: 0.506 #&gt; F-statistic: 68.5 on 1 and 65 DF, p-value: 9.47e-12 In addition to, summary(fit2)$r.squared #&gt; [1] 0.513 we can get the R2 squared value from the data frame glance returns: glance(fit2) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.513 0.506 316 68.5 9.47e-12 2 -480 966 972 #&gt; deviance df.residual #&gt; 1 6506118 65 We can get predictions and residuals on the original data frame using the modelr functions add_residuals and add_predictions florida &lt;- florida %&gt;% add_predictions(fit2) %&gt;% add_residuals(fit2) glimpse(florida) #&gt; Observations: 67 #&gt; Variables: 9 #&gt; $ county &lt;chr&gt; &quot;Alachua&quot;, &quot;Baker&quot;, &quot;Bay&quot;, &quot;Bradford&quot;, &quot;Brevard&quot;, &quot;... #&gt; $ Clinton96 &lt;int&gt; 40144, 2273, 17020, 3356, 80416, 320736, 1794, 2712... #&gt; $ Dole96 &lt;int&gt; 25303, 3684, 28290, 4038, 87980, 142834, 1717, 2783... #&gt; $ Perot96 &lt;int&gt; 8072, 667, 5922, 819, 25249, 38964, 630, 7783, 7244... #&gt; $ Bush00 &lt;int&gt; 34124, 5610, 38637, 5414, 115185, 177323, 2873, 354... #&gt; $ Gore00 &lt;int&gt; 47365, 2392, 18850, 3075, 97318, 386561, 2155, 2964... #&gt; $ Buchanan00 &lt;int&gt; 263, 73, 248, 65, 570, 788, 90, 182, 270, 186, 122,... #&gt; $ pred &lt;dbl&gt; 291.3, 25.3, 214.0, 30.8, 908.2, 1400.7, 24.0, 280.... #&gt; $ resid &lt;dbl&gt; -2.83e+01, 4.77e+01, 3.40e+01, 3.42e+01, -3.38e+02,... There are now two new columns in florida, pred with the fitted values (predictions), and resid with the residuals. Use fit2_augment to create a residual plot: fit2_resid_plot &lt;- ggplot(florida, aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) fit2_resid_plot Note, we use the function geom_refline to add a reference line at 0. Let’s add some labels to points, who is that outlier? fit2_resid_plot + geom_label(aes(label = county)) The outlier county is “Palm Beach” arrange(florida) %&gt;% arrange(desc(abs(resid))) %&gt;% select(county, resid) %&gt;% head() #&gt; # A tibble: 6 × 2 #&gt; county resid #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 PalmBeach 2302 #&gt; 2 Broward -613 #&gt; 3 Lee -357 #&gt; 4 Brevard -338 #&gt; 5 Miami-Dade -329 #&gt; 6 Pinellas -317 Ted Mosby dressed as a Hanging Chad in “How I Met Your Mother” "],
["discovery.html", "6 Discovery 6.1 Prerequisites", " 6 Discovery 6.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "],
["probability.html", "7 Probability 7.1 Prerequisites", " 7 Probability 7.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "],
["uncertainty.html", "8 Uncertainty 8.1 Prerequisites", " 8 Uncertainty 8.1 Prerequisites library(&quot;tidyverse&quot;) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats "]
]
