[
["_main.html", "QSS Tidyverse Code Preface Colophon 1 Introduction Prerequisites 1.1 Overview of the Book 1.2 How to use the Book 1.3 Introduction to R 2 Causality Prerequisites 2.1 Racial Discrimination in the Labor Market 2.2 Subsetting Data in R 2.3 Causal Affects and the Counterfactual 2.4 Observational Studies 2.5 Descriptive Statistics for a Single Variable 3 Measurement Prerequisites 3.1 Measuring Civilian Victimization during Wartime 3.2 Handling Missing Data in R 3.3 Visualizing the Univariate Distribution 3.4 Survey Sampling 3.5 Measuring Political Polarization 3.6 Clustering 4 Prediction Prerequisites 4.1 Predicting Election Outcomes 4.2 Linear Regression 4.3 Regression and Causation 5 Discovery 5.1 Textual data 5.2 Network data 5.3 Spatial Data 6 Probability Prerequisites 6.1 Probability 6.2 Conditional Probability 6.3 Random Variables and Probability Distributions 6.4 Large Sample Theorems 7 Uncertainty Prerequisites 7.1 Estimation 7.2 Hypothesis Testing 7.3 Linear Regression Model with Uncertainty", " QSS Tidyverse Code Jeffrey B. Arnold 2018-02-12 Preface This is tidyverse R code to supplement the book, Quantitative Social Science: An Introduction, by Kosuke Imai, to be published by Princeton University Press in March 2017. The R code included with the text of QSS and the supplementary materials relies mostly on base R functions. This translates the code examples provided with QSS to tidyverse R code. Tidyverse refers to a set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, and a few others) that share common data representations, especially the use of data frames for return values. These notes do not aim to teach tidyverse, The book R for Data Science by Hadley Wickham and Garrett Grolemond is an introduction. I wrote this code while teaching course that employed both texts in order to make the excellent examples and statistical material in QSS more compatible with the modern data science using R approach in R4DS. Colophon To install the R packages used in this work run the following code, installs the qsstidy package which contains no code or data, but will install the needed dependencies. install.packages(&quot;devtools&quot;) install_github(&quot;jrnold/qss-tidy&quot;) Additionally, the gganimate package requires installing ffmpeg with libvpx support. The source of the book is available here and was built with versions of packages below: #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.4.3 (2017-11-30) #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/Los_Angeles #&gt; date 2018-01-10 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; animation * 2.5 2017-03-30 cran (@2.5) #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.4.0) #&gt; backports 1.1.2 2017-12-13 CRAN (R 3.4.3) #&gt; base * 3.4.3 2017-12-07 local #&gt; bindr 0.1 2016-11-13 CRAN (R 3.4.0) #&gt; bindrcpp 0.2 2017-06-17 CRAN (R 3.4.0) #&gt; bookdown 0.5 2017-08-20 CRAN (R 3.4.1) #&gt; broom 0.4.3 2017-11-20 CRAN (R 3.4.3) #&gt; cellranger 1.1.0 2016-07-27 CRAN (R 3.4.0) #&gt; cli 1.0.0 2017-11-05 CRAN (R 3.4.2) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.4.0) #&gt; compiler 3.4.3 2017-12-07 local #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.4.1) #&gt; datasets * 3.4.3 2017-12-07 local #&gt; devtools 1.13.4 2017-11-09 CRAN (R 3.4.2) #&gt; digest 0.6.13 2017-12-14 CRAN (R 3.4.3) #&gt; dplyr * 0.7.4 2017-09-28 CRAN (R 3.4.2) #&gt; evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) #&gt; forcats * 0.2.0 2017-01-23 CRAN (R 3.4.0) #&gt; foreign 0.8-69 2017-06-22 CRAN (R 3.4.3) #&gt; ggplot2 * 2.2.1 2016-12-30 CRAN (R 3.4.0) #&gt; glue 1.2.0 2017-10-29 CRAN (R 3.4.2) #&gt; graphics * 3.4.3 2017-12-07 local #&gt; grDevices * 3.4.3 2017-12-07 local #&gt; grid 3.4.3 2017-12-07 local #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.4.0) #&gt; haven 1.1.0 2017-07-09 CRAN (R 3.4.1) #&gt; hms 0.4.0 2017-11-23 CRAN (R 3.4.3) #&gt; htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) #&gt; httr 1.3.1 2017-08-20 CRAN (R 3.4.1) #&gt; jsonlite 1.5 2017-06-01 CRAN (R 3.4.0) #&gt; knitr 1.18 2017-12-27 CRAN (R 3.4.3) #&gt; lattice 0.20-35 2017-03-25 CRAN (R 3.4.3) #&gt; lazyeval 0.2.1 2017-10-29 CRAN (R 3.4.2) #&gt; lubridate 1.7.1 2017-11-03 CRAN (R 3.4.2) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.4.0) #&gt; memoise 1.1.0 2017-04-21 CRAN (R 3.4.0) #&gt; methods 3.4.3 2017-12-07 local #&gt; mnormt 1.5-5 2016-10-15 CRAN (R 3.4.0) #&gt; modelr 0.1.1 2017-07-24 CRAN (R 3.4.1) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.4.0) #&gt; nlme 3.1-131 2017-02-06 CRAN (R 3.4.3) #&gt; parallel 3.4.3 2017-12-07 local #&gt; pillar 1.0.1 2017-11-27 CRAN (R 3.4.3) #&gt; pkgconfig 2.0.1 2017-03-21 CRAN (R 3.4.0) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.4.0) #&gt; psych 1.7.8 2017-09-09 CRAN (R 3.4.3) #&gt; purrr * 0.2.4 2017-10-18 CRAN (R 3.4.2) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.4.0) #&gt; Rcpp 0.12.14 2017-11-23 CRAN (R 3.4.3) #&gt; readr * 1.1.1 2017-05-16 CRAN (R 3.4.0) #&gt; readxl 1.0.0 2017-04-18 CRAN (R 3.4.0) #&gt; reshape2 1.4.3 2017-12-11 CRAN (R 3.4.3) #&gt; rlang 0.1.6 2017-12-21 CRAN (R 3.4.3) #&gt; rmarkdown 1.8 2017-11-17 CRAN (R 3.4.2) #&gt; rprojroot 1.3-2 2018-01-03 CRAN (R 3.4.3) #&gt; rstudioapi 0.7 2017-09-07 CRAN (R 3.4.1) #&gt; rvest 0.3.2 2016-06-17 CRAN (R 3.4.0) #&gt; scales 0.5.0 2017-08-24 CRAN (R 3.4.1) #&gt; stats * 3.4.3 2017-12-07 local #&gt; stringi 1.1.6 2017-11-17 CRAN (R 3.4.2) #&gt; stringr * 1.2.0 2017-02-18 CRAN (R 3.4.0) #&gt; tibble * 1.4.1 2017-12-25 CRAN (R 3.4.3) #&gt; tidyr * 0.7.2 2017-10-16 CRAN (R 3.4.2) #&gt; tidyverse * 1.2.1 2017-11-14 CRAN (R 3.4.2) #&gt; tools 3.4.3 2017-12-07 local #&gt; utils * 3.4.3 2017-12-07 local #&gt; withr 2.1.1 2017-12-19 CRAN (R 3.4.3) #&gt; xml2 1.1.1 2017-01-24 CRAN (R 3.4.0) #&gt; yaml 2.1.16 2017-12-12 CRAN (R 3.4.3) 1 Introduction Prerequisites In this and other chapters we will make use of data from the qss package, which is available on github. Install it using the install_github() function from the library devtools. devtools::install_github(&quot;kosukeimai/qss-package&quot;) #&gt; Skipping install of &#39;qss&#39; from a github remote, the SHA1 (1dc4b858) has not changed since last install. #&gt; Use `force = TRUE` to force installation library(&quot;qss&quot;) In the prerequisites section of each chapter, we’ll load any packages needed for the chapter, and possibly define some functions or load data. library(&quot;tidyverse&quot;) We also load the readr package to load csv files, library(&quot;readr&quot;) the haven package to load Stata dta files, library(&quot;haven&quot;) and the rio package to load multiple types of files. library(&quot;rio&quot;) 1.1 Overview of the Book This sections contains no code to translate – seeQSS* text.* 1.2 How to use the Book This sections contains no code to translate – seeQSS* text.* 1.3 Introduction to R These notes do not aim to completely teach R and the tidyverse. However, there are many other resources for that. R for Data Science is a comprehensive introduction to R using the tidyverse. Data Camp has interactive courses. In particular, I recommend starting with the following two courses. Introduction to R Introduction to the Tidyverse 1.3.1 Arithmetic Operations This sections contains no code to translate – seeQSS* text.* 1.3.2 Objects This sections contains no code to translate – seeQSS* text.* Also see R4DS: Workflow basics. 1.3.3 Vectors This sections contains no code to translate – seeQSS* text.* Also see R4DS: Vectors. In R for Data Science vectors are introduced much later, after data frames. 1.3.4 Functions This sections contains no code to translate – seeQSS* text.* Also see R4DS: Functions. 1.3.5 Data Files Rather than using setwd() in scripts, data analysis should be organized in projects. Read the introduction on RStudio projects in R4DS.1 Datasets used in R are accessed in two ways. Datasets can be distributed with R packages. These are often smaller datasets used in examples and tutorials in packages. These are loaded with the data() function. For example you can load UN data on demographic statistics from the qss library, which distributes the data sets used in the QSS textbook. (The function data() called without any arguments will list all the datasets distributed with installed packages.) data(&quot;UNpop&quot;, package = &quot;qss&quot;) Datasets can be loaded from external files including both stored R objects (.RData, .rda) and other formats (.csv, .dta, .sav). To read a csv file into R use the read_csv function from the readr library, part of the tidyverse. UNpop_URL &lt;- &quot;https://raw.githubusercontent.com/kosukeimai/qss/master/INTRO/UNpop.csv&quot; UNpop &lt;- read_csv(UNpop_URL) #&gt; Parsed with column specification: #&gt; cols( #&gt; year = col_integer(), #&gt; world.pop = col_integer() #&gt; ) We use the readr functionread_csv() instead of the base R function read.csv() used in the QSS text. It is slightly faster, and returns a tibble instead of a data frame. Check this by calling class() on the new object. class(UNpop) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; UNpop #&gt; # A tibble: 7 x 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; # ... with 1 more row See R for Data Science Ch 11: Data Import for more discussion. Note that in the previous code we loaded the file directly from a URL, but we could also work with local files on your computer, e.g. UNpop &lt;- read_csv(&quot;INTRO/UNpop.csv&quot;) See R for Data Science Ch 10: Tibbles for a deeper discussion of data frames. The single bracket, [, is useful to select rows and columns in simple cases. UNpop[c(1, 2, 3), ] #&gt; # A tibble: 3 x 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 There are dplyr functions to select rows by number, to select rows by certain criteria, or to select columns. To select rows 1–3, use slice(). slice(UNpop, 1:3) #&gt; # A tibble: 3 x 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 Base R allows you to choose the column world.pop column from the UNpop data frame: UNpop[, &quot;world.pop&quot;] #&gt; # A tibble: 7 x 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more row UNpop$world.pop #&gt; [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 UNpop[[&quot;world.pop&quot;]] #&gt; [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 select(UNpop, world.pop) #&gt; # A tibble: 7 x 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3026003 #&gt; 3 3691173 #&gt; 4 4449049 #&gt; 5 5320817 #&gt; 6 6127700 #&gt; # ... with 1 more row Unlike [, the [[ and $ operators can only select a single column and return a vector.2 The dplyr function select() always returns a tibble (data frame), and never a vector, even if only one column is selected. Select rows 1–3 of the year column: UNpop[1:3, &quot;year&quot;] #&gt; # A tibble: 3 x 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 or, select(slice(UNpop, 1:3), year) #&gt; # A tibble: 3 x 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 The same series of functions can be performed using the pipe operator, %&gt;%. UNpop %&gt;% slice(1:3) %&gt;% select(year) #&gt; # A tibble: 3 x 1 #&gt; year #&gt; &lt;int&gt; #&gt; 1 1950 #&gt; 2 1960 #&gt; 3 1970 This example may seem verbose, but later we can produce more complicated transformations of the data by chaining together simple functions. Select every other row from UNpop: UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] #&gt; [1] 2525779 3691173 5320817 6916183 or UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% select(world.pop) #&gt; # A tibble: 4 x 1 #&gt; world.pop #&gt; &lt;int&gt; #&gt; 1 2525779 #&gt; 2 3691173 #&gt; 3 5320817 #&gt; 4 6916183 or UNpop %&gt;% filter(row_number() %% 2 == 1) #&gt; # A tibble: 4 x 2 #&gt; year world.pop #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1950 2525779 #&gt; 2 1970 3691173 #&gt; 3 1990 5320817 #&gt; 4 2010 6916183 The function n() when used in a dplyr function returns the number of rows in the data frame (or the number of rows in the group if used with group_by()). The function row_number() returns the row number of an observation. The %% operator returns the modulus, i.e. division remainder. 1.3.6 Saving Objects It is not recommended that you save the entire R workspace using save.image due to the negative and unexpected impacts it can have on reproducibility.See the R for Data Science chapter Workflow Projects. You should uncheck the options in RStudio to avoid saving and restoring from .RData files (go to Tools &gt; Global Options &gt; General). This will help ensure that your R code runs the way you think it does, instead of depending on some long forgotten code that is only saved in the workspace image. Everything important should be in a script. Anything saved or loaded from file should be done explicitly. Your motto should be that the source is real, not the objects created by it. The source code is real. The objects are realizations of the source code. Source for EVERY user modified object is placed in a particular directory or directories, for later editing and retrieval. – from the ESS manual This means that while you should not save the entire workplace it is perfectly fine practice to run a script and save or load R objects to files, using or . As with reading CSV files, use the readr package functions. In this case, write_csv() writes a csv file and takes at least two objects: the data that you want to write to a csv and the name that you want to give the file. write_csv(UNpop, &quot;UNpop.csv&quot;) 1.3.7 Programming and Learning Tips Use the haven package to read and write Stata (.dta) and SPSS (.sav) files. Stata and SPSS are two other statistical programs commonly used in social science. Even if you don’t ever use them, you’ll almost certainly encounter data stored in their native formats. UNpop_dta_url &lt;- &quot;https://github.com/kosukeimai/qss/raw/master/INTRO/UNpop.dta&quot; UNpop &lt;- read_dta(UNpop_dta_url) UNpop #&gt; # A tibble: 7 x 2 #&gt; year world_pop #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; # ... with 1 more row There is also the equivalent write_dta() function to create Stata datasets. write_dta(UNpop, &quot;UNpop.dta&quot;) While Stata and SPSS data sets are quite similar to data frames, they differ slightly in definitions of acceptable data types of columns and what metadata they store with the data. +Be careful when reading and writing from these formats to ensure that information is not lost. Also see the rio package which makes loading data even easier with smart defaults. You can use the import() function to load many types of files: import(&quot;https://github.com/kosukeimai/qss/raw/master/INTRO/UNpop.csv&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://github.com/kosukeimai/qss/raw/master/INTRO/UNpop.RData&quot;) #&gt; year world.pop #&gt; 1 1950 2525779 #&gt; 2 1960 3026003 #&gt; 3 1970 3691173 #&gt; 4 1980 4449049 #&gt; 5 1990 5320817 #&gt; 6 2000 6127700 #&gt; 7 2010 6916183 import(&quot;https://github.com/kosukeimai/qss/raw/master/INTRO/UNpop.dta&quot;) #&gt; year world_pop #&gt; 1 1950 2526 #&gt; 2 1960 3026 #&gt; 3 1970 3691 #&gt; 4 1980 4449 #&gt; 5 1990 5321 #&gt; 6 2000 6128 #&gt; 7 2010 6916 R also includes the foreign package, which contains functions for reading and writing files using haven. One reason to use these packages is that they are better maintained. For example, the R function read.dta() does not read files created by the most recent versions of Stata (13+), whereas haven does. 1.3.8 Style Guide Following a consistent coding style is important for your code to be readable by you and others. The preferred style is the tidyverse style guide, which differs slightly from Google’s R style guide. The lintr package will check files for style errors. The styler package provides functions for automatically formatting R code according to style guides. In RStudio, go to the Tools &gt; Global Options &gt; Code &gt; Diagnostics pane and check the box to activate style warnings. On this pane, there are other options that can be set in order to increase or decrease the amount of warnings while writing R code in RStudio. 2 Causality Prerequisites library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) 2.1 Racial Discrimination in the Labor Market Load the data from the qss package. data(&quot;resume&quot;, package = &quot;qss&quot;) In addition to the dim(), summary(), and head() functions shown in the text, dim(resume) #&gt; [1] 4870 4 summary(resume) #&gt; firstname sex race call #&gt; Length:4870 Length:4870 Length:4870 Min. :0.00 #&gt; Class :character Class :character Class :character 1st Qu.:0.00 #&gt; Mode :character Mode :character Mode :character Median :0.00 #&gt; Mean :0.08 #&gt; 3rd Qu.:0.00 #&gt; Max. :1.00 head(resume) #&gt; firstname sex race call #&gt; 1 Allison female white 0 #&gt; 2 Kristen female white 0 #&gt; 3 Lakisha female black 0 #&gt; 4 Latonya female black 0 #&gt; 5 Carrie female white 0 #&gt; 6 Jay male white 0 we can also use glimpse() to get a quick understanding of the variables in the data frame: glimpse(resume) #&gt; Observations: 4,870 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... The code in QSS uses table() and addmargins() to construct the table. However, this can be done easily with the dplyr package using grouping and summarizing. Use group_by() to identify each combination of race and call, and then count() the observations: race_call_tab &lt;- resume %&gt;% group_by(race, call) %&gt;% count() race_call_tab #&gt; # A tibble: 4 x 3 #&gt; # Groups: race, call [4] #&gt; race call n #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 0 2278 #&gt; 2 black 1 157 #&gt; 3 white 0 2200 #&gt; 4 white 1 235 If we want to calculate callback rates by race, we can use the mutate() function from dplyr. race_call_rate &lt;- race_call_tab %&gt;% group_by(race) %&gt;% mutate(call_rate = n / sum(n)) %&gt;% filter(call == 1) %&gt;% select(race, call_rate) race_call_rate #&gt; # A tibble: 2 x 2 #&gt; # Groups: race [2] #&gt; race call_rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black 0.0645 #&gt; 2 white 0.0965 If we want the overall callback rate, we can calculate it from the original data. Use the summarise() function from dplyr. resume %&gt;% summarise(call_back = mean(call)) #&gt; call_back #&gt; 1 0.0805 2.2 Subsetting Data in R 2.2.1 Subsetting Create a new object of all individuals whose race variable equals black in the resume data: resumeB &lt;- resume %&gt;% filter(race == &quot;black&quot;) glimpse(resumeB) #&gt; Observations: 2,435 #&gt; Variables: 4 #&gt; $ firstname &lt;chr&gt; &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Kenya&quot;, &quot;Latonya&quot;, &quot;Tyrone&quot;, ... #&gt; $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fem... #&gt; $ race &lt;chr&gt; &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;... #&gt; $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... Calculate the callback rate for black individuals: resumeB %&gt;% summarise(call_rate = mean(call)) #&gt; call_rate #&gt; 1 0.0645 You can combine the filter() and select() functions with multiple conditions. For example, to keep the call and first name variables for female individuals with stereotypically black names: resumeBf &lt;- resume %&gt;% filter(race == &quot;black&quot;, sex == &quot;female&quot;) %&gt;% select(call, firstname) head(resumeBf) #&gt; call firstname #&gt; 1 0 Lakisha #&gt; 2 0 Latonya #&gt; 3 0 Kenya #&gt; 4 0 Latonya #&gt; 5 0 Aisha #&gt; 6 0 Aisha Now we can calculate the gender gap by group. Now we can calculate the gender gap by group. Doing so may seem to require a little more code, but we will not duplicate as much as in QSS, and this would easily scale to more than two categories. First, group by race and sex and calculate the callback rate for each group: resume_race_sex &lt;- resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) head(resume_race_sex) #&gt; # A tibble: 4 x 3 #&gt; # Groups: race [2] #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 Use spread() from the tidyr package to make each value of race a new column: resume_sex &lt;- resume_race_sex %&gt;% ungroup() %&gt;% spread(race, call) resume_sex #&gt; # A tibble: 2 x 3 #&gt; sex black white #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 #&gt; 2 male 0.0583 0.0887 Now we can calculate the race wage differences by sex as before, resume_sex %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 x 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 This could be combined into a single chain with only six lines of code: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% ungroup() %&gt;% spread(race, call) %&gt;% mutate(call_diff = white - black) #&gt; # A tibble: 2 x 4 #&gt; sex black white call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 female 0.0663 0.0989 0.0326 #&gt; 2 male 0.0583 0.0887 0.0304 For more information on a way to do this using the spread and gather functions from tidyr package, see the R for Data Science chapter “Tidy Data”. WARNING The function ungroup removes the groupings in group_by. The function spread will not allow a grouping variable to be reshaped. Since many dplyr functions work differently depending on whether the data frame is grouped or not, I find that I can encounter many errors due to forgetting that a data frame is grouped. As such, I tend to ungroup data frames as soon as I am no longer are using the groupings. Alternatively, we could have used summarise and the diff function: resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) %&gt;% group_by(sex) %&gt;% arrange(race) %&gt;% summarise(call_diff = diff(call)) #&gt; # A tibble: 2 x 2 #&gt; sex call_diff #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 0.0326 #&gt; 2 male 0.0304 I find the spread code preferable since the individual race callback rates are retained in the data, and since there is no natural ordering of the race variable (unlike if it were a time-series), it is not obvious from reading the code whether call_diff is black - white or white - black. 2.2.2 Simple conditional statements dlpyr has three conditional statement functions if_else, recode and case_when. The function if_else is like ifelse but corrects inconsistent behavior that ifelse exhibits in certain cases. Create a variable BlackFemale using if_else() and confirm it is only equal to 1 for black and female observations: resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1, 0)) %&gt;% group_by(BlackFemale, race, sex) %&gt;% count() #&gt; # A tibble: 4 x 4 #&gt; # Groups: BlackFemale, race, sex [4] #&gt; BlackFemale race sex n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 black male 549 #&gt; 2 0 white female 1860 #&gt; 3 0 white male 575 #&gt; 4 1.00 black female 1886 Warning The function if_else is more strict about the variable types than ifelse. While most R functions are forgiving about variables types, and will automatically convert integers to numeric or vice-versa, they are distinct. For example, these examples will produce errors: resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, TRUE, 0)) #&gt; Error in mutate_impl(.data, dots): Evaluation error: `false` must be type logical, not double. because TRUE is logical and 0 is numeric. resume %&gt;% mutate(BlackFemale = if_else(race == &quot;black&quot; &amp; sex == &quot;female&quot;, 1L, 0)) #&gt; Error in mutate_impl(.data, dots): Evaluation error: `false` must be type integer, not double. because 1L is an integer and 0 is numeric vector (floating-point number). The distinction between integers and numeric variables is often invisible because most functions coerce variables between integer and numeric vectors. class(1) #&gt; [1] &quot;numeric&quot; class(1L) #&gt; [1] &quot;integer&quot; The : operator returns integers and as.integer coerces numeric vectors to integer vectors: class(1:5) #&gt; [1] &quot;integer&quot; class(c(1, 2, 3)) #&gt; [1] &quot;numeric&quot; class(as.integer(c(1, 2, 3))) #&gt; [1] &quot;integer&quot; 2.2.3 Factor Variables For more on factors see the R for Data Science chapter “Factors” and the package forcats. Also see the R for Data Science chapter “Strings” for working with strings. The function case_when is a generalization of the if_else function to multiple conditions. For example, to create categories for all combinations of race and sex, resume %&gt;% mutate( race_sex = case_when( race == &quot;black&quot; &amp; sex == &quot;female&quot; ~ &quot;black, female&quot;, race == &quot;white&quot; &amp; sex == &quot;female&quot; ~ &quot;white female&quot;, race == &quot;black&quot; &amp; sex == &quot;male&quot; ~ &quot;black male&quot;, race == &quot;white&quot; &amp; sex == &quot;male&quot; ~ &quot;white male&quot; ) ) %&gt;% head() #&gt; firstname sex race call race_sex #&gt; 1 Allison female white 0 white female #&gt; 2 Kristen female white 0 white female #&gt; 3 Lakisha female black 0 black, female #&gt; 4 Latonya female black 0 black, female #&gt; 5 Carrie female white 0 white female #&gt; 6 Jay male white 0 white male Each condition is a formula (an R object created with the “tilde” ~). You will see formulas used extensively in the modeling section. The condition is on the left-hand side of the formula. The value to assign to observations meeting that condition is on the right-hand side. Observations are given the value of the first matching condition, so the order of these can matter. The case_when function also supports a default value by using a condition TRUE as the last condition. This will match anything not already matched. For example, if you wanted three categories (“black male”, “black female”, “white”): resume %&gt;% mutate( race_sex = case_when( race == &quot;black&quot; &amp; sex == &quot;female&quot; ~ &quot;black female&quot;, race == &quot;black&quot; &amp; sex == &quot;male&quot; ~ &quot;black male&quot;, TRUE ~ &quot;white&quot; ) ) %&gt;% head() #&gt; firstname sex race call race_sex #&gt; 1 Allison female white 0 white #&gt; 2 Kristen female white 0 white #&gt; 3 Lakisha female black 0 black female #&gt; 4 Latonya female black 0 black female #&gt; 5 Carrie female white 0 white #&gt; 6 Jay male white 0 white Alternatively, we could have created this variable using string manipulation functions. Use mutate() to create a new variable, type, str_to_title to capitalize sex and race, and str_c to concatenate these vectors. resume &lt;- resume %&gt;% mutate(type = str_c(str_to_title(race), str_to_title(sex))) Some of the reasons given in QSS for using factors in this chapter are less important due to the functionality of modern tidyverse packages. For example, there is no reason to use tapply, as you can use group_by and summarise, resume %&gt;% group_by(type) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 x 2 #&gt; type call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 BlackFemale 0.0663 #&gt; 2 BlackMale 0.0583 #&gt; 3 WhiteFemale 0.0989 #&gt; 4 WhiteMale 0.0887 or, resume %&gt;% group_by(race, sex) %&gt;% summarise(call = mean(call)) #&gt; # A tibble: 4 x 3 #&gt; # Groups: race [?] #&gt; race sex call #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black female 0.0663 #&gt; 2 black male 0.0583 #&gt; 3 white female 0.0989 #&gt; 4 white male 0.0887 What’s nice about this approach is that we wouldn’t have needed to create the factor variable first as in QSS. We can use that same approach to calculate the mean of first names, and use arrange() to sort in ascending order. resume %&gt;% group_by(firstname) %&gt;% summarise(call = mean(call)) %&gt;% arrange(call) #&gt; # A tibble: 36 x 2 #&gt; firstname call #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aisha 0.0222 #&gt; 2 Rasheed 0.0299 #&gt; 3 Keisha 0.0383 #&gt; 4 Tremayne 0.0435 #&gt; 5 Kareem 0.0469 #&gt; 6 Darnell 0.0476 #&gt; # ... with 30 more rows Tip: General advice for working (or not) with factors: Use character vectors instead of factors. They are easier to manipulate with string functions. Use factor vectors only when you need a specific ordering of string values in a variable, e.g. in a model or a plot. 2.3 Causal Affects and the Counterfactual Load the social dataset included in the qss package. data(&quot;social&quot;, package = &quot;qss&quot;) summary(social) #&gt; sex yearofbirth primary2004 messages #&gt; Length:305866 Min. :1900 Min. :0.000 Length:305866 #&gt; Class :character 1st Qu.:1947 1st Qu.:0.000 Class :character #&gt; Mode :character Median :1956 Median :0.000 Mode :character #&gt; Mean :1956 Mean :0.401 #&gt; 3rd Qu.:1965 3rd Qu.:1.000 #&gt; Max. :1986 Max. :1.000 #&gt; primary2006 hhsize #&gt; Min. :0.000 Min. :1.00 #&gt; 1st Qu.:0.000 1st Qu.:2.00 #&gt; Median :0.000 Median :2.00 #&gt; Mean :0.312 Mean :2.18 #&gt; 3rd Qu.:1.000 3rd Qu.:2.00 #&gt; Max. :1.000 Max. :8.00 Calculate the mean turnout by message: turnout_by_message &lt;- social %&gt;% group_by(messages) %&gt;% summarize(turnout = mean(primary2006)) turnout_by_message #&gt; # A tibble: 4 x 2 #&gt; messages turnout #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Since we want to calculate the difference by group, spread() the data set so each group is a column, then use mutate() to calculate the difference of each from the control group. Finally, use select() and matches() to return a dataframe with only those new variables that you have created: turnout_by_message %&gt;% spread(messages, turnout) %&gt;% mutate(diff_civic_duty = `Civic Duty` - Control, diff_Hawthorne = Hawthorne - Control, diff_Neighbors = Neighbors - Control) %&gt;% select(matches(&quot;diff_&quot;)) #&gt; # A tibble: 1 x 3 #&gt; diff_civic_duty diff_Hawthorne diff_Neighbors #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0179 0.0257 0.0813 Find the mean values of age, 2004 turnout, and household size for each group: social %&gt;% mutate(age = 2006 - yearofbirth) %&gt;% group_by(messages) %&gt;% summarise(primary2004 = mean(primary2004), age = mean(age), hhsize = mean(hhsize)) #&gt; # A tibble: 4 x 4 #&gt; messages primary2004 age hhsize #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.399 49.7 2.19 #&gt; 2 Control 0.400 49.8 2.18 #&gt; 3 Hawthorne 0.403 49.7 2.18 #&gt; 4 Neighbors 0.407 49.9 2.19 The function summarise_at allows you to summarize multiple variables, using multiple functions, or both. social %&gt;% mutate(age = 2006 - yearofbirth) %&gt;% group_by(messages) %&gt;% summarise_at(vars(primary2004, age, hhsize), funs(mean)) #&gt; # A tibble: 4 x 4 #&gt; messages primary2004 age hhsize #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.399 49.7 2.19 #&gt; 2 Control 0.400 49.8 2.18 #&gt; 3 Hawthorne 0.403 49.7 2.18 #&gt; 4 Neighbors 0.407 49.9 2.19 2.4 Observational Studies Load and inspect the minimum wage data from the qss package: data(&quot;minwage&quot;, package = &quot;qss&quot;) glimpse(minwage) #&gt; Observations: 358 #&gt; Variables: 8 #&gt; $ chain &lt;chr&gt; &quot;wendys&quot;, &quot;wendys&quot;, &quot;burgerking&quot;, &quot;burgerking&quot;, &quot;kf... #&gt; $ location &lt;chr&gt; &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA&quot;, &quot;PA... #&gt; $ wageBefore &lt;dbl&gt; 5.00, 5.50, 5.00, 5.00, 5.25, 5.00, 5.00, 5.00, 5.0... #&gt; $ wageAfter &lt;dbl&gt; 5.25, 4.75, 4.75, 5.00, 5.00, 5.00, 4.75, 5.00, 4.5... #&gt; $ fullBefore &lt;dbl&gt; 20.0, 6.0, 50.0, 10.0, 2.0, 2.0, 2.5, 40.0, 8.0, 10... #&gt; $ fullAfter &lt;dbl&gt; 0.0, 28.0, 15.0, 26.0, 3.0, 2.0, 1.0, 9.0, 7.0, 18.... #&gt; $ partBefore &lt;dbl&gt; 20.0, 26.0, 35.0, 17.0, 8.0, 10.0, 20.0, 30.0, 27.0... #&gt; $ partAfter &lt;dbl&gt; 36, 3, 18, 9, 12, 9, 25, 32, 39, 10, 20, 4, 13, 20,... summary(minwage) #&gt; chain location wageBefore wageAfter #&gt; Length:358 Length:358 Min. :4.25 Min. :4.25 #&gt; Class :character Class :character 1st Qu.:4.25 1st Qu.:5.05 #&gt; Mode :character Mode :character Median :4.50 Median :5.05 #&gt; Mean :4.62 Mean :4.99 #&gt; 3rd Qu.:4.99 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :6.25 #&gt; fullBefore fullAfter partBefore partAfter #&gt; Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 2.1 1st Qu.: 2.0 1st Qu.:11.0 1st Qu.:11.0 #&gt; Median : 6.0 Median : 6.0 Median :16.2 Median :17.0 #&gt; Mean : 8.5 Mean : 8.4 Mean :18.8 Mean :18.7 #&gt; 3rd Qu.:12.0 3rd Qu.:12.0 3rd Qu.:25.0 3rd Qu.:25.0 #&gt; Max. :60.0 Max. :40.0 Max. :60.0 Max. :60.0 First, calculate the proportion of restaurants by state whose hourly wages were less than the minimum wage in NJ, $5.05, for wageBefore and wageAfter: Since the NJ minimum wage was $5.05, we’ll define a variable with that value. Even if you use them only once or twice, it is a good idea to put values like this in variables. It makes your code closer to self-documenting, i.e. easier for others (including you, in the future) to understand what the code does. NJ_MINWAGE &lt;- 5.05 Later, it will be easier to understand wageAfter &lt; NJ_MINWAGE without any comments than it would be to understand wageAfter &lt; 5.05. In the latter case you’d have to remember that the new NJ minimum wage was 5.05 and that’s why you were using that value. Using 5.05 in your code, instead of assigning it to an object called NJ_MINWAGE, is an example of a magic number; try to avoid them. Note that the variable location has multiple values: PA and four regions of NJ. So we’ll add a state variable to the data. minwage %&gt;% count(location) #&gt; # A tibble: 5 x 2 #&gt; location n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 centralNJ 45 #&gt; 2 northNJ 146 #&gt; 3 PA 67 #&gt; 4 shoreNJ 33 #&gt; 5 southNJ 67 We can extract the state from the final two characters of the location variable using thestringr function str_sub: minwage &lt;- mutate(minwage, state = str_sub(location, -2L)) Alternatively, since &quot;PA&quot; is the only value that an observation in Pennsylvania takes in location, and since all other observations are in New Jersey: minwage &lt;- mutate(minwage, state = if_else(location == &quot;PA&quot;, &quot;PA&quot;, &quot;NJ&quot;)) Let’s confirm that the restaurants followed the law: minwage %&gt;% group_by(state) %&gt;% summarise(prop_after = mean(wageAfter &lt; NJ_MINWAGE), prop_Before = mean(wageBefore &lt; NJ_MINWAGE)) #&gt; # A tibble: 2 x 3 #&gt; state prop_after prop_Before #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.00344 0.911 #&gt; 2 PA 0.955 0.940 Create a variable for the proportion of full-time employees in NJ and PA after the increase: minwage &lt;- minwage %&gt;% mutate(totalAfter = fullAfter + partAfter, fullPropAfter = fullAfter / totalAfter) Now calculate the average proportion of full-time employees for each state: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state #&gt; # A tibble: 2 x 2 #&gt; state fullPropAfter #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ 0.320 #&gt; 2 PA 0.272 We could compute the difference in means between NJ and PA by (filter(full_prop_by_state, state == &quot;NJ&quot;)[[&quot;fullPropAfter&quot;]] - filter(full_prop_by_state, state == &quot;PA&quot;)[[&quot;fullPropAfter&quot;]]) #&gt; [1] 0.0481 or spread(full_prop_by_state, state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 1 x 3 #&gt; NJ PA diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.320 0.272 0.0481 2.4.1 Confounding Bias We can calculate the proportion of each chain out of all fast-food restaurants in each state: chains_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% count(chain) %&gt;% mutate(prop = n / sum(n)) We can easily compare these using a dot-plot: ggplot(chains_by_state, aes(x = chain, y = prop, colour = state)) + geom_point() + coord_flip() In the QSS text, only Burger King restaurants are compared. However, dplyr makes comparing all restaurants not much more complicated than comparing two. All we have to do is change the group_by statement we used previously so that we group by chain restaurants and states: full_prop_by_state_chain &lt;- minwage %&gt;% group_by(state, chain) %&gt;% summarise(fullPropAfter = mean(fullPropAfter)) full_prop_by_state_chain #&gt; # A tibble: 8 x 3 #&gt; # Groups: state [?] #&gt; state chain fullPropAfter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 NJ burgerking 0.358 #&gt; 2 NJ kfc 0.328 #&gt; 3 NJ roys 0.283 #&gt; 4 NJ wendys 0.260 #&gt; 5 PA burgerking 0.321 #&gt; 6 PA kfc 0.236 #&gt; # ... with 2 more rows We can plot and compare the proportions easily in this format. In general, ordering categorical variables alphabetically is useless, so we’ll order the chains by the average of the NJ and PA fullPropAfter, using fct_reorder function: ggplot(full_prop_by_state_chain, aes(x = forcats::fct_reorder(chain, fullPropAfter), y = fullPropAfter, colour = state)) + geom_point() + coord_flip() + labs(x = &quot;chains&quot;) To calculate the difference between states in the proportion of full-time employees after the change: full_prop_by_state_chain %&gt;% spread(state, fullPropAfter) %&gt;% mutate(diff = NJ - PA) #&gt; # A tibble: 4 x 4 #&gt; chain NJ PA diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 burgerking 0.358 0.321 0.0364 #&gt; 2 kfc 0.328 0.236 0.0918 #&gt; 3 roys 0.283 0.213 0.0697 #&gt; 4 wendys 0.260 0.248 0.0117 2.4.2 Before and After and Difference-in-Difference Designs To compute the estimates in the before and after design first create an additional variable for the proportion of full-time employees before the minimum wage increase. minwage &lt;- minwage %&gt;% mutate(totalBefore = fullBefore + partBefore, fullPropBefore = fullBefore / totalBefore) The before-and-after analysis is the difference between the full-time employment before and after the minimum wage law passed looking only at NJ: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) #&gt; diff #&gt; 1 0.0239 The difference-in-differences design uses the difference in the before-and-after differences for each state. minwage %&gt;% group_by(state) %&gt;% summarise(diff = mean(fullPropAfter) - mean(fullPropBefore)) %&gt;% spread(state, diff) %&gt;% mutate(diff_in_diff = NJ - PA) #&gt; # A tibble: 1 x 3 #&gt; NJ PA diff_in_diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0239 -0.0377 0.0616 Let’s create a single dataset with the mean values of each state before and after to visually look at each of these designs: full_prop_by_state &lt;- minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(fullPropAfter, fullPropBefore), mean) %&gt;% gather(period, fullProp, -state) %&gt;% mutate(period = recode(period, fullPropAfter = 1, fullPropBefore = 0)) full_prop_by_state #&gt; # A tibble: 4 x 3 #&gt; state period fullProp #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 1.00 0.320 #&gt; 2 PA 1.00 0.272 #&gt; 3 NJ 0 0.297 #&gt; 4 PA 0 0.310 Now plot this new dataset: ggplot(full_prop_by_state, aes(x = period, y = fullProp, colour = state)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(0, 1), labels = c(&quot;Before&quot;, &quot;After&quot;)) 2.5 Descriptive Statistics for a Single Variable To calculate the summary for the variables wageBefore and wageAfter for New Jersey only: minwage %&gt;% filter(state == &quot;NJ&quot;) %&gt;% select(wageBefore, wageAfter) %&gt;% summary() #&gt; wageBefore wageAfter #&gt; Min. :4.25 Min. :5.00 #&gt; 1st Qu.:4.25 1st Qu.:5.05 #&gt; Median :4.50 Median :5.05 #&gt; Mean :4.61 Mean :5.08 #&gt; 3rd Qu.:4.87 3rd Qu.:5.05 #&gt; Max. :5.75 Max. :5.75 We calculate the interquartile range for each state’s wages after the passage of the law using the same grouped summarize as we used before: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter = IQR(wageAfter), wageBefore = IQR(wageBefore)) #&gt; # A tibble: 2 x 3 #&gt; state wageAfter wageBefore #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0 0.620 #&gt; 2 PA 0.575 0.750 Calculate the variance and standard deviation of wageAfter and wageBefore for each state: minwage %&gt;% group_by(state) %&gt;% summarise(wageAfter_sd = sd(wageAfter), wageAfter_var = var(wageAfter), wageBefore_sd = sd(wageBefore), wageBefore_var = var(wageBefore)) #&gt; # A tibble: 2 x 5 #&gt; state wageAfter_sd wageAfter_var wageBefore_sd wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.0112 0.343 0.118 #&gt; 2 PA 0.359 0.129 0.358 0.128 Here we can see again how using summarise_at allows for more compact code to specify variables and summary statistics that would be the case using just summarise: minwage %&gt;% group_by(state) %&gt;% summarise_at(vars(wageAfter, wageBefore), funs(sd, var)) #&gt; # A tibble: 2 x 5 #&gt; state wageAfter_sd wageBefore_sd wageAfter_var wageBefore_var #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NJ 0.106 0.343 0.0112 0.118 #&gt; 2 PA 0.359 0.358 0.129 0.128 3 Measurement Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;broom&quot;) library(&quot;tidyr&quot;) 3.1 Measuring Civilian Victimization during Wartime data(&quot;afghan&quot;, package = &quot;qss&quot;) Summarize the variables of interest afghan %&gt;% select(age, educ.years, employed, income) %&gt;% summary() #&gt; age educ.years employed income #&gt; Min. :15.0 Min. : 0 Min. :0.000 Length:2754 #&gt; 1st Qu.:22.0 1st Qu.: 0 1st Qu.:0.000 Class :character #&gt; Median :30.0 Median : 1 Median :1.000 Mode :character #&gt; Mean :32.4 Mean : 4 Mean :0.583 #&gt; 3rd Qu.:40.0 3rd Qu.: 8 3rd Qu.:1.000 #&gt; Max. :80.0 Max. :18 Max. :1.000 Loading data with either data() orread_csv() does not convert strings to factors by default; see below with income. To get a summary of the different levels, either convert it to a factor (see R4DS Ch 15), or use count(): count(afghan, income) #&gt; # A tibble: 6 x 2 #&gt; income n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 10,001-20,000 616 #&gt; 2 2,001-10,000 1420 #&gt; 3 20,001-30,000 93 #&gt; 4 less than 2,000 457 #&gt; 5 over 30,000 14 #&gt; 6 &lt;NA&gt; 154 Use count to calculate the proportion of respondents who answer that they were harmed by the ISAF or the Taliban (violent.exp.ISAF and violent.exp.taliban, respectively): afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 9 x 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.483 #&gt; 2 0 1 354 0.129 #&gt; 3 0 NA 22 0.00799 #&gt; 4 1 0 475 0.172 #&gt; 5 1 1 526 0.191 #&gt; 6 1 NA 22 0.00799 #&gt; # ... with 3 more rows We need to use ungroup() in order to ensure that sum(n) sums over the entire dataset as opposed to only within categories of violent.exp.ISAF. Unlike prop.table(), the code above does not drop missing values. We can drop those values by adding filter() and !is.na() to test for missing values in those variables: afghan %&gt;% filter(!is.na(violent.exp.ISAF), !is.na(violent.exp.taliban)) %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 4 x 4 #&gt; violent.exp.ISAF violent.exp.taliban n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 1330 0.495 #&gt; 2 0 1 354 0.132 #&gt; 3 1 0 475 0.177 #&gt; 4 1 1 526 0.196 3.2 Handling Missing Data in R We already observed the issues with NA values in calculating the proportion answering the “experienced violence” questions. You can filter rows with specific variables having missing values using filter() as shown above. head(afghan$income, n = 10) #&gt; [1] &quot;2,001-10,000&quot; &quot;2,001-10,000&quot; &quot;2,001-10,000&quot; &quot;2,001-10,000&quot; #&gt; [5] &quot;2,001-10,000&quot; NA &quot;10,001-20,000&quot; &quot;2,001-10,000&quot; #&gt; [9] &quot;2,001-10,000&quot; NA head(is.na(afghan$income), n = 10) #&gt; [1] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE Counts and proportion of missing values of income: summarise(afghan, n_missing = sum(is.na(income)), p_missing = mean(is.na(income))) #&gt; n_missing p_missing #&gt; 1 154 0.0559 Mean, and other functions, do not by default exclude missing values. Use na.rm = TRUE in these cases. x &lt;- c(1, 2, 3, NA) mean(x) #&gt; [1] NA mean(x, na.rm = TRUE) #&gt; [1] 2 Table of proportions of individuals harmed by the ISAF and Taliban that includes missing (NA) values: violent_exp_prop &lt;- afghan %&gt;% group_by(violent.exp.ISAF, violent.exp.taliban) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) %&gt;% select(-n) violent_exp_prop #&gt; # A tibble: 9 x 3 #&gt; violent.exp.ISAF violent.exp.taliban prop #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 0.483 #&gt; 2 0 1 0.129 #&gt; 3 0 NA 0.00799 #&gt; 4 1 0 0.172 #&gt; 5 1 1 0.191 #&gt; 6 1 NA 0.00799 #&gt; # ... with 3 more rows The data frame above can be reorganized so that rows are ISAF and the columns are Taliban as follows: violent_exp_prop %&gt;% spread(violent.exp.taliban, prop) #&gt; # A tibble: 3 x 4 #&gt; violent.exp.ISAF `0` `1` `&lt;NA&gt;` #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.483 0.129 0.00799 #&gt; 2 1 0.172 0.191 0.00799 #&gt; 3 NA 0.00254 0.00290 0.00363 drop_na is an alternative to na.omit that allows for removing missing values, drop_na(afghan) Tip There are multiple types of missing values. NA # logical #&gt; [1] NA NA_integer_ # integer #&gt; [1] NA NA_real_ # double #&gt; [1] NA NA_character_ # character #&gt; [1] NA In many cases, this distinction does not matter since many functions will coerce these missing values to the correct vector type. However, you will need to use these in some tidyverse functions that require the outputs to be the same type, e.g. map() and most of the other purrr functions, and if_else(). The code below produces an error, since the TRUE case returns an integer value (x is an integer), but the FALSE case does not specify the type of NA. x &lt;- 1:5 class(x) #&gt; [1] &quot;integer&quot; if_else(x &lt; 3, x, NA) #&gt; Error: `false` must be type integer, not logical So instead of NA, use NA_integer_: if_else(x &lt; 3, x, NA_integer_) #&gt; [1] 1 2 NA NA NA 3.3 Visualizing the Univariate Distribution 3.3.1 Barplot afghan &lt;- afghan %&gt;% mutate(violent.exp.ISAF.fct = fct_explicit_na(fct_recode(factor(violent.exp.ISAF), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the ISAF&quot;) afghan &lt;- afghan %&gt;% mutate(violent.exp.taliban.fct = fct_explicit_na(fct_recode(factor(violent.exp.taliban), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;)) ggplot(afghan, aes(x = violent.exp.ISAF.fct, y = ..prop.., group = 1)) + geom_bar() + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization by the Taliban&quot;) Instead of creating two separate box-plots, create a single plot facetted by ISAF and Taliban: select(afghan, violent.exp.ISAF, violent.exp.taliban) %&gt;% gather(variable, value) %&gt;% mutate(value = fct_explicit_na(fct_recode(factor(value), Harm = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), &quot;No response&quot;), variable = recode(variable, violent.exp.ISAF = &quot;ISAF&quot;, violent.exp.taliban = &quot;Taliban&quot;)) %&gt;% ggplot(aes(x = value, y = ..prop.., group = 1)) + geom_bar() + facet_wrap(~ variable, ncol = 1) + xlab(&quot;Response category&quot;) + ylab(&quot;Proportion of respondents&quot;) + ggtitle(&quot;Civilian Victimization&quot;) This plot could improved by plotting the two values simultaneously to be able to better compare them. This will require creating a data frame that has the following columns: perpetrator (ISAF, Taliban) and response (No Harm, Harm, No response). violent_exp &lt;- afghan %&gt;% select(violent.exp.ISAF, violent.exp.taliban) %&gt;% gather(perpetrator, response) %&gt;% mutate(perpetrator = str_replace(perpetrator, &quot;violent\\\\.exp\\\\.&quot;, &quot;&quot;), perpetrator = str_replace(perpetrator, &quot;taliban&quot;, &quot;Taliban&quot;), response = fct_recode(factor(response), &quot;Harm&quot; = &quot;1&quot;, &quot;No Harm&quot; = &quot;0&quot;), response = fct_explicit_na(response, &quot;No response&quot;), response = fct_relevel(response, c(&quot;No response&quot;, &quot;No Harm&quot;)) ) %&gt;% count(perpetrator, response) %&gt;% mutate(prop = n / sum(n)) ggplot(violent_exp, aes(x = prop, y = response, color = perpetrator)) + geom_point() + scale_color_manual(values = c(ISAF = &quot;green&quot;, Taliban = &quot;black&quot;)) Black was chosen for the Taliban, and Green for ISAF because they are the colors of their respective flags. 3.3.2 Histogram See the documentation for geom_histogram. ggplot(afghan, aes(x = age, y = ..density..)) + geom_histogram(binwidth = 5, boundary = 0) + scale_x_continuous(breaks = seq(20, 80, by = 10)) + labs(title = &quot;Distribution of respondent&#39;s age&quot;, y = &quot;Age&quot;, x = &quot;Density&quot;) ggplot(afghan, aes(x = educ.years, y = ..density..)) + geom_histogram(binwidth = 1, center = 0) + geom_vline(xintercept = median(afghan$educ.years), color = &quot;white&quot;, size = 2) + annotate(&quot;text&quot;, x = median(afghan$educ.years), y = 0.2, label = &quot;median&quot;, hjust = 0) + labs(title = &quot;Distribution of respondent&#39;s education&quot;, x = &quot;Years of education&quot;, y = &quot;Density&quot;) There are several alternatives to the histogram. Density plots (geom_density): dens_plot &lt;- ggplot(afghan, aes(x = age)) + geom_density() + scale_x_continuous(breaks = seq(20, 80, by = 10)) + labs(title = &quot;Distribution of respondent&#39;s age&quot;, y = &quot;Age&quot;, x = &quot;Density&quot;) dens_plot which can be combined with a geom_rug to create a rug plot, which puts small lines on the axis to represent the value of each observation. It can be combined with a scatter or density plot to add extra detail. Adjust the alpha to modify the color transparency of the rug and address overplotting. dens_plot + geom_rug(alpha = .2) Frequency polygons (geom_freqpoly): See R for Data Science EDA. ggplot(afghan, aes(x = age)) + geom_freqpoly() + scale_x_continuous(breaks = seq(20, 80, by = 10)) + labs(title = &quot;Distribution of respondent&#39;s age&quot;, y = &quot;Age&quot;, x = &quot;Density&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.3.3 Boxplot See the documentation for geom_boxplot. ggplot(afghan, aes(x = 1, y = age)) + geom_boxplot() + coord_flip() + labs(y = &quot;Age&quot;, x = &quot;&quot;, title = &quot;Distribution of Age&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_boxplot() + coord_flip() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;, title = &quot;Education by Province&quot;) Helmand and Uruzgan have much lower levels of education than the other provinces, and also report higher levels of violence. afghan %&gt;% group_by(province) %&gt;% summarise(educ.years = mean(educ.years, na.rm = TRUE), violent.exp.taliban = mean(violent.exp.taliban, na.rm = TRUE), violent.exp.ISAF = mean(violent.exp.ISAF, na.rm = TRUE)) %&gt;% arrange(educ.years) #&gt; # A tibble: 5 x 4 #&gt; province educ.years violent.exp.taliban violent.exp.ISAF #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 1.04 0.455 0.496 #&gt; 2 Helmand 1.60 0.504 0.541 #&gt; 3 Khost 5.79 0.233 0.242 #&gt; 4 Kunar 5.93 0.303 0.399 #&gt; 5 Logar 6.70 0.0802 0.144 An alternatives to the traditional boxplot: The Tufte boxplot: library(&quot;ggthemes&quot;) ggplot(afghan, aes(y = educ.years, x = province)) + geom_tufteboxplot() + coord_flip() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;, title = &quot;Education by Province&quot;) Dot plot with jitter and adjusted alpha to avoid overplotting: ggplot(afghan, aes(y = educ.years, x = province)) + geom_point(position = position_jitter(width = 0.25, height = 0), alpha = .2) + coord_flip() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;, title = &quot;Education by Province&quot;) A violin plot: ggplot(afghan, aes(y = educ.years, x = province)) + geom_violin() + coord_flip() + labs(x = &quot;Province&quot;, y = &quot;Years of education&quot;, title = &quot;Education by Province&quot;) 3.3.4 Printing and saving graphics Use the function rdoc (&quot;ggplot2&quot;, &quot;ggsave&quot;) to save ggplot2 graphics. Also, R Markdown files have their own means of creating and saving plots created by code-chunks. 3.4 Survey Sampling 3.4.1 The Role of Randomization data(&quot;afghan.village&quot;, package = &quot;qss&quot;) Box-plots of altitude ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = altitude)) + geom_boxplot() + labs(y = &quot;Altitude (meter)&quot;, x = &quot;&quot;) + coord_flip() Box plots log-population values of sampled and non-sampled ggplot(afghan.village, aes(x = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), y = log(population))) + geom_boxplot() + labs(y = &quot;log(population)&quot;, x = &quot;&quot;) + coord_flip() You can also compare these distributions by plotting their densities: ggplot(afghan.village, aes(colour = factor(village.surveyed, labels = c(&quot;sampled&quot;, &quot;non-sampled&quot;)), x = log(population))) + geom_density() + geom_rug() + labs(x = &quot;log(population)&quot;, colour = &quot;&quot;) 3.4.2 Non-response and other sources of bias Calculate the rates of item non-response by province to the question about civilian victimization by ISAF and Taliban forces (violent.exp.ISAF and violent.exp.taliban): afghan %&gt;% group_by(province) %&gt;% summarise(ISAF = mean(is.na(violent.exp.ISAF)), taliban = mean(is.na(violent.exp.taliban))) %&gt;% arrange(-ISAF) #&gt; # A tibble: 5 x 3 #&gt; province ISAF taliban #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Uruzgan 0.0207 0.0620 #&gt; 2 Helmand 0.0164 0.0304 #&gt; 3 Khost 0.00476 0.00635 #&gt; 4 Kunar 0 0 #&gt; 5 Logar 0 0 Calculate the proportion who support the ISAF using the difference in means between the ISAF and control groups: (mean(filter(afghan, list.group == &quot;ISAF&quot;)$list.response) - mean(filter(afghan, list.group == &quot;control&quot;)$list.response)) #&gt; [1] 0.049 To calculate the table responses to the list experiment in the control, ISAF, and Taliban groups: afghan %&gt;% group_by(list.response, list.group) %&gt;% count() %&gt;% glimpse() %&gt;% spread(list.group, n, fill = 0) #&gt; Observations: 12 #&gt; Variables: 3 #&gt; $ list.response &lt;int&gt; 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4 #&gt; $ list.group &lt;chr&gt; &quot;control&quot;, &quot;ISAF&quot;, &quot;control&quot;, &quot;ISAF&quot;, &quot;taliban&quot;,... #&gt; $ n &lt;int&gt; 188, 174, 265, 278, 433, 265, 260, 287, 200, 182... #&gt; # A tibble: 5 x 4 #&gt; # Groups: list.response [5] #&gt; list.response control ISAF taliban #&gt; * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 188 174 0 #&gt; 2 1 265 278 433 #&gt; 3 2 265 260 287 #&gt; 4 3 200 182 198 #&gt; 5 4 0 24.0 0 3.5 Measuring Political Polarization data(&quot;congress&quot;, package = &quot;qss&quot;) glimpse(congress) #&gt; Observations: 14,552 #&gt; Variables: 7 #&gt; $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8... #&gt; $ district &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 98, 98, 1, 2, 3, 4, 5, ... #&gt; $ state &lt;chr&gt; &quot;USA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;ALABAMA&quot;, &quot;A... #&gt; $ party &lt;chr&gt; &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Demo... #&gt; $ name &lt;chr&gt; &quot;TRUMAN&quot;, &quot;BOYKIN F.&quot;, &quot;GRANT G.&quot;, &quot;ANDREWS G.&quot;, &quot;... #&gt; $ dwnom1 &lt;dbl&gt; -0.276, -0.026, -0.042, -0.008, -0.082, -0.170, -0.12... #&gt; $ dwnom2 &lt;dbl&gt; 0.016, 0.796, 0.999, 1.005, 1.066, 0.870, 0.990, 0.89... q &lt;- congress %&gt;% filter(congress %in% c(80, 112), party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-1.5, 1.5)) q However, since there are colors associated with Democrats (blue) and Republicans (blue), we should use them rather than the defaults. There’s some evidence that using semantically-resonant colors can help decoding data visualizations (See Lin, et al. 2013). Since I’ll reuse the scale several times, I’ll save it in a variable. scale_colour_parties &lt;- scale_colour_manual(values = c(Democrat = &quot;blue&quot;, Republican = &quot;red&quot;, Other = &quot;green&quot;)) q + scale_colour_parties congress %&gt;% ggplot(aes(x = dwnom1, y = dwnom2, colour = party)) + geom_point() + facet_wrap(~ congress) + coord_fixed() + scale_y_continuous(&quot;racial liberalism/conservatism&quot;, limits = c(-2, 2)) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;, limits = c(-2, 2)) #scale_colour_parties congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% ggplot(aes(x = congress, y = dwnom1, colour = fct_reorder2(party, congress, dwnom1))) + geom_line() + scale_colour_parties + labs(y = &quot;DW-NOMINATE score (1st Dimension)&quot;, x = &quot;Congress&quot;, colour = &quot;Party&quot;) Alternatively, you can plot the mean DW-Nominate scores for each party and congress over time. This plot uses color for parties and lets the points and labels for the first and last congresses (80 and 112) to convey progress through time. party_means &lt;- congress %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% group_by(party, congress) %&gt;% summarise(dwnom1 = mean(dwnom1), dwnom2 = mean(dwnom2)) party_endpoints &lt;- party_means %&gt;% filter(congress %in% c(min(congress), max(congress))) %&gt;% mutate(label = str_c(party, congress, sep = &quot; - &quot;)) ggplot(party_means, aes(x = dwnom1, y = dwnom2, color = party, group = party)) + geom_point() + geom_path() + ggrepel::geom_text_repel(data = party_endpoints, mapping = aes(label = congress), color = &quot;black&quot;) + scale_y_continuous(&quot;racial liberalism/conservatism&quot;) + scale_x_continuous(&quot;economic liberalism/conservatism&quot;) + scale_colour_parties 3.5.1 Correlation Let’s plot the Gini coefficient data(&quot;USGini&quot;, package = &quot;qss&quot;) ggplot(USGini, aes(x = year, y = gini)) + geom_point() + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Gini coefficient&quot;) + ggtitle(&quot;Income Inequality&quot;) To calculate a measure of party polarization take the code used in the plot of Republican and Democratic party median ideal points and adapt it to calculate the difference in the party medians: party_polarization &lt;- congress %&gt;% group_by(congress, party) %&gt;% summarise(dwnom1 = mean(dwnom1)) %&gt;% filter(party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;)) %&gt;% spread(party, dwnom1) %&gt;% mutate(polarization = Republican - Democrat) party_polarization #&gt; # A tibble: 33 x 4 #&gt; # Groups: congress [33] #&gt; congress Democrat Republican polarization #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 80 -0.146 0.276 0.421 #&gt; 2 81 -0.195 0.264 0.459 #&gt; 3 82 -0.180 0.265 0.445 #&gt; 4 83 -0.181 0.261 0.442 #&gt; 5 84 -0.209 0.261 0.471 #&gt; 6 85 -0.214 0.250 0.464 #&gt; # ... with 27 more rows ggplot(party_polarization, aes(x = congress, y = polarization)) + geom_point() + geom_line() + ggtitle(&quot;Political Polarization&quot;) + labs(x = &quot;Year&quot;, y = &quot;Republican median − Democratic median&quot;) 3.5.2 Quantile-Quantile Plot congress %&gt;% filter(congress == 112, party %in% c(&quot;Republican&quot;, &quot;Democrat&quot;)) %&gt;% ggplot(aes(x = dwnom2, y = ..density..)) + geom_histogram(binwidth = .2) + facet_grid(party ~ .) + labs(x = &quot;racial liberalism/conservatism dimension&quot;) The package ggplot2 includes a function stat_qq which can be used to create qq-plots but it is more suited to comparing a sample distribution with a theoretical distribution, usually the normal one. However, we can calculate one by hand, which may give more insight into exactly what the qq-plot is doing. party_qtiles &lt;- tibble( probs = seq(0, 1, by = 0.01), Democrat = quantile(filter(congress, congress == 112, party == &quot;Democrat&quot;)$dwnom2, probs = probs), Republican = quantile(filter(congress, congress == 112, party == &quot;Republican&quot;)$dwnom2, probs = probs) ) party_qtiles #&gt; # A tibble: 101 x 3 #&gt; probs Democrat Republican #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 -0.925 -1.38 #&gt; 2 0.0100 -0.672 -0.720 #&gt; 3 0.0200 -0.619 -0.566 #&gt; 4 0.0300 -0.593 -0.526 #&gt; 5 0.0400 -0.567 -0.468 #&gt; 6 0.0500 -0.560 -0.436 #&gt; # ... with 95 more rows The plot looks different than the one in the text since the x- and y-scales are in the original values instead of z-scores (see the next section). party_qtiles %&gt;% ggplot(aes(x = Democrat, y = Republican)) + geom_point() + geom_abline() + coord_fixed() 3.6 Clustering 3.6.1 Matrices While matrices are great for numerical computations, such as when you are implementing algorithms, generally keeping data in data frames is more convenient for data wrangling. See R for Data Science chapter Vectors. 3.6.2 Lists See R for Data Science chapters Vectors and Iteration, as well as the purrr package for more powerful methods of computing on lists. 3.6.3 k-means algorithms Calculate the clusters by the 80th and 112th congresses: k80two.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 2, nstart = 5) Add the cluster ids to data sets: congress80 &lt;- congress %&gt;% filter(congress == 80) %&gt;% mutate(cluster2 = factor(k80two.out$cluster)) We will also create a data sets with the cluster centroids. These are in the centers element of the cluster object. k80two.out$centers #&gt; dwnom1 dwnom2 #&gt; 1 -0.0484 0.783 #&gt; 2 0.1468 -0.339 To make it easier to use with ggplot2, we need to convert this to a data frame. The tidy function from the broom package: k80two.clusters &lt;- tidy(k80two.out) k80two.clusters #&gt; x1 x2 size withinss cluster #&gt; 1 -0.0484 0.783 135 10.9 1 #&gt; 2 0.1468 -0.339 311 54.9 2 Plot the ideal points and clusters: ggplot() + geom_point(data = congress80, aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80two.clusters, mapping = aes(x = x1, y = x2)) congress80 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; # A tibble: 5 x 3 #&gt; # Groups: party, cluster2 [5] #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 132 #&gt; 2 Democrat 2 62 #&gt; 3 Other 2 2 #&gt; 4 Republican 1 3 #&gt; 5 Republican 2 247 And now we can repeat these steps for the 112th congress: k112two.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 2, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112two.out$cluster)) k112two.clusters &lt;- tidy(k112two.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112two.clusters, mapping = aes(x = x1, y = x2)) Number of observations from each party in each cluster: congress112 %&gt;% group_by(party, cluster2) %&gt;% count() #&gt; # A tibble: 3 x 3 #&gt; # Groups: party, cluster2 [3] #&gt; party cluster2 n #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 Democrat 1 200 #&gt; 2 Republican 1 1 #&gt; 3 Republican 2 242 Now repeat the same with four clusters on the 80th congress: k80four.out &lt;- kmeans(select(filter(congress, congress == 80), dwnom1, dwnom2), centers = 4, nstart = 5) congress80 &lt;- filter(congress, congress == 80) %&gt;% mutate(cluster2 = factor(k80four.out$cluster)) k80four.clusters &lt;- tidy(k80four.out) ggplot() + geom_point(data = congress80, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k80four.clusters, mapping = aes(x = x1, y = x2), size = 3) and on the 112th congress: k112four.out &lt;- kmeans(select(filter(congress, congress == 112), dwnom1, dwnom2), centers = 4, nstart = 5) congress112 &lt;- filter(congress, congress == 112) %&gt;% mutate(cluster2 = factor(k112four.out$cluster)) k112four.clusters &lt;- tidy(k112four.out) ggplot() + geom_point(data = congress112, mapping = aes(x = dwnom1, y = dwnom2, colour = cluster2)) + geom_point(data = k112four.clusters, mapping = aes(x = x1, y = x2), size = 3) 4 Prediction Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) The packages modelr and broom are used to wrangle the results of linear regressions, library(&quot;broom&quot;) library(&quot;modelr&quot;) 4.1 Predicting Election Outcomes 4.1.1 Loops in R RStudio provides many features to help debugging, which will be useful in for loops and function: see this article for an example. values &lt;- c(2, 3, 6) n &lt;- length(values) results &lt;- rep(NA, n) for (i in 1:n) { results[i] &lt;- values[i] * 2 cat(values[i], &quot;times 2 is equal to&quot;, results[i], &quot;\\n&quot;) } #&gt; 2 times 2 is equal to 4 #&gt; 3 times 2 is equal to 6 #&gt; 6 times 2 is equal to 12 Note that the above code uses the for loop for pedagogical purposes only, this could have simply been written results &lt;- values * 2 results #&gt; [1] 4 6 12 In general, avoid using for loops when there is a vectorized function. But sticking with the for loop, there are several things that could be improved. Avoid using the idiom 1:n in for loops. To see why, look what happens when values are empty: values &lt;- c() n &lt;- length(values) results &lt;- rep(NA, n) for (i in 1:n) { cat(&quot;i = &quot;, i, &quot;\\n&quot;) results[i] &lt;- values[i] * 2 cat(values[i], &quot;times 2 is equal to&quot;, results[i], &quot;\\n&quot;) } #&gt; i = 1 #&gt; times 2 is equal to NA #&gt; i = 0 #&gt; times 2 is equal to Instead of not running a loop, as you would expect, it runs two loops, where i = 1, then i = 0. This edge case occurs more than you may think, especially if you are writing functions where you don’t know the length of the vector is ex ante. The way to avoid this is to use either rdoc(&quot;base&quot;, &quot;seq_len&quot;) or rdoc(&quot;base&quot;, &quot;seq_along&quot;), which will handle 0-length vectors correctly. values &lt;- c() n &lt;- length(values) results &lt;- rep(NA, n) for (i in seq_along(values)) { results[i] &lt;- values[i] * 2 } print(results) #&gt; logical(0) or values &lt;- c() n &lt;- length(values) results &lt;- rep(NA, n) for (i in seq_len(n)) { results[i] &lt;- values[i] * 2 } print(results) #&gt; logical(0) Also, note that the the result is logical(0). That’s because the NA missing value has class logical, and thus rep(NA, ...) returns a logical vector. It is better style to initialize the vector with the same data type that you will be using, results &lt;- rep(NA_real_, length(values)) results #&gt; numeric(0) class(results) #&gt; [1] &quot;numeric&quot; Often loops can be rewritten to use a map function. Read the R for Data Science chapter Iteration before proceeding. To do so, we first write a function that will be applied to each element of the vector. When converting from a for loop to a function, this is usually simply the body of the for loop, though you may need to add arguments for any variables defined outside the body of the for loop. In this case, mult_by_two &lt;- function(x) { x * 2 } We can now test that this function works on different values: mult_by_two(0) #&gt; [1] 0 mult_by_two(2.5) #&gt; [1] 5 mult_by_two(-3) #&gt; [1] -6 At this point, we could replace the body of the for loop with this function: values &lt;- c(2, 4, 6) n &lt;- length(values) results &lt;- rep(NA, n) for (i in seq_len(n)) { results[i] &lt;- mult_by_two(values[i]) } print(results) #&gt; [1] 4 8 12 This can be useful if the body of a for loop is many lines long. However, this loop is still unwieldy code. We have to remember to define an empty vector results that is the same size as values to hold the results, and then correctly loop over all the values. We already saw how these steps have possibilities for errors. Functionals like map, apply a function to each element of a vector. results &lt;- map(values, mult_by_two) results #&gt; [[1]] #&gt; [1] 4 #&gt; #&gt; [[2]] #&gt; [1] 8 #&gt; #&gt; [[3]] #&gt; [1] 12 The values of each element are correct, but map returns a list vector, not a numeric vector like we may have been expecting. If we want a numeric vector, use map_dbl, results &lt;- map_dbl(values, mult_by_two) results #&gt; [1] 4 8 12 Also, instead of explicitly defining a function, like mult_by_two, we could have instead used an anonymous function with the functional. An anonymous function is a function that is not assigned to a name. results &lt;- map_dbl(values, function(x) x * 2) results #&gt; [1] 4 8 12 The various purrr functions also will interpret formulas as functions where .x and .y are interpreted as (up to) two arguments. results &lt;- map_dbl(values, ~ .x * 2) results #&gt; [1] 4 8 12 This is for parsimony and convenience; in the background, these functions are creating anonymous functions from the given formula. QSS discusses several debugging strategies. The functional approach lends itself to easier debugging because the function can be tested with input values independently of the loop. 4.1.2 General Conditional Statements in R See the R for Data Science section Conditional Execution for a more complete discussion of conditional execution. If you are using conditional statements to assign values for data frame, see the dplyr functions if_else, recode, and case_when The following code which uses a for loop, values &lt;- 1:5 n &lt;- length(values) results &lt;- rep(NA_real_, n) for (i in seq_len(n)) { x &lt;- values[i] r &lt;- x %% 2 if (r == 0) { cat(x, &quot;is even and I will perform addition&quot;, x, &quot; + &quot;, x, &quot;\\n&quot;) results[i] &lt;- x + x } else { cat(x, &quot;is even and I will perform multiplication&quot;, x, &quot; * &quot;, x, &quot;\\n&quot;) results[i] &lt;- x * x } } #&gt; 1 is even and I will perform multiplication 1 * 1 #&gt; 2 is even and I will perform addition 2 + 2 #&gt; 3 is even and I will perform multiplication 3 * 3 #&gt; 4 is even and I will perform addition 4 + 4 #&gt; 5 is even and I will perform multiplication 5 * 5 results #&gt; [1] 1 4 9 8 25 could be rewritten to use if_else, if_else(values %% 2 == 0, values + values, values * values) #&gt; [1] 1 4 9 8 25 or using the map_dbl functional with a named function, myfunc &lt;- function(x) { if (x %% 2 == 0) { x + x } else { x * x } } map_dbl(values, myfunc) #&gt; [1] 1 4 9 8 25 or map_dbl with an anonymous function, map_dbl(values, function(x) { if (x %% 2 == 0) { x + x } else { x * x } }) #&gt; [1] 1 4 9 8 25 4.1.3 Poll Predictions Load the election polls by state for the 2008 US Presidential election, data(&quot;polls08&quot;, package = &quot;qss&quot;) glimpse(polls08) #&gt; Observations: 1,332 #&gt; Variables: 5 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;,... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;SurveyUSA-2&quot;, &quot;Ca... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 34, 37, 36, 36, 3... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 47, 55, 51, 49, 5... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, 2008-10-06, 2008... and the election results, data(&quot;pres08&quot;, package = &quot;qss&quot;) glimpse(pres08) #&gt; Observations: 51 #&gt; Variables: 5 #&gt; $ state.name &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo... #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE... #&gt; $ Obama &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36,... #&gt; $ McCain &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, ... #&gt; $ EV &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, ... Compute Obama’s margin in polls and final election polls08 &lt;- polls08 %&gt;% mutate(margin = Obama - McCain) pres08 &lt;- pres08 %&gt;% mutate(margin = Obama - McCain) To work with dates, the R package lubridate makes wrangling them much easier. See the R for Data Science chapter Dates and Times. The function ymd will convert character strings like year-month-day and more into dates, as long as the order is (year, month, day). See dmy, mdy, and others for other ways to convert strings to dates. x &lt;- ymd(&quot;2008-11-04&quot;) y &lt;- ymd(&quot;2008/9/1&quot;) x - y #&gt; Time difference of 64 days However, note that in polls08, the date middate is already a date object, class(polls08$middate) #&gt; [1] &quot;Date&quot; The function read_csv by default will check character vectors to see if they have patterns that appear to be dates, and if so, will parse those columns as dates. We’ll create a variable for election day ELECTION_DAY &lt;- ymd(&quot;2008-11-04&quot;) and add a new column to poll08 with the days to the election polls08 &lt;- mutate(polls08, ELECTION_DAY - middate) Although the code in the chapter uses a for loop, there is no reason to do so. We can accomplish the same task by merging the election results data to the polling data by state. polls_w_results &lt;- left_join(polls08, select(pres08, state, elec_margin = margin), by = &quot;state&quot;) %&gt;% mutate(error = elec_margin - margin) glimpse(polls_w_results) #&gt; Observations: 1,332 #&gt; Variables: 9 #&gt; $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;... #&gt; $ Pollster &lt;chr&gt; &quot;SurveyUSA-2&quot;, &quot;Capital Survey-2&quot;, &quot;S... #&gt; $ Obama &lt;int&gt; 36, 34, 35, 35, 39, 34, 36, 25, 35, 3... #&gt; $ McCain &lt;int&gt; 61, 54, 62, 55, 60, 64, 58, 52, 55, 4... #&gt; $ middate &lt;date&gt; 2008-10-27, 2008-10-15, 2008-10-08, ... #&gt; $ margin &lt;int&gt; -25, -20, -27, -20, -21, -30, -22, -2... #&gt; $ `ELECTION_DAY - middate` &lt;time&gt; 8 days, 20 days, 27 days, 29 days, 4... #&gt; $ elec_margin &lt;int&gt; -21, -21, -21, -21, -21, -21, -21, -2... #&gt; $ error &lt;int&gt; 4, -1, 6, -1, 0, 9, 1, 6, -1, -8, -3,... To get the last poll in each state, arrange and filter on middate last_polls &lt;- polls_w_results %&gt;% arrange(state, desc(middate)) %&gt;% group_by(state) %&gt;% slice(1) last_polls #&gt; # A tibble: 51 x 9 #&gt; # Groups: state [51] #&gt; state Pollster Obama McCain middate margin `ELECT… elec… error #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;time&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 AK Research 2000-3 39 58 2008-10-29 -19 6 -21 - 2 #&gt; 2 AL SurveyUSA-2 36 61 2008-10-27 -25 8 -21 4 #&gt; 3 AR ARG-4 44 51 2008-10-29 - 7 6 -20 -13 #&gt; 4 AZ ARG-3 46 50 2008-10-29 - 4 6 - 9 - 5 #&gt; 5 CA SurveyUSA-3 60 36 2008-10-30 24 5 24 0 #&gt; 6 CO ARG-3 52 45 2008-10-29 7 6 9 2 #&gt; # ... with 45 more rows Challenge: Instead of using the last poll, use the average of polls in the last week? Last month? How do the margins on the polls change over the election period? To simplify things for later, let’s define a function rmse which calculates the root mean squared error, as defined in the book. See the R for Data Science chapter Functions for more on writing functions. rmse &lt;- function(actual, pred) { sqrt(mean( (actual - pred) ^ 2)) } Now we can use rmse() to calculate the RMSE for all the final polls: rmse(last_polls$margin, last_polls$elec_margin) #&gt; [1] 5.88 Or since we already have a variable error, sqrt(mean(last_polls$error ^ 2)) #&gt; [1] 5.88 The mean prediction error is mean(last_polls$error) #&gt; [1] 1.08 This is slightly different than what is in the book due to the difference in the poll used as the final poll; many states have many polls on the last day. I’ll choose bin widths of 1%, since that is fairly interpretable: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 1, boundary = 0) The text uses bin widths of 5%: ggplot(last_polls, aes(x = error)) + geom_histogram(binwidth = 5, boundary = 0) Challenge: What other ways could you visualize the results? How would you show all states? What about plotting the absolute or squared errors instead of the errors? Challenge: What happens to prediction error if you average polls? Consider averaging back over time? What happens if you take the averages of the state poll average and average of all polls - does that improve prediction? To create a scatter plots using the state abbreviations instead of points use geom_text instead of geom_point. ggplot(last_polls, aes(x = margin, y = elec_margin, label = state)) + geom_abline(color = &quot;white&quot;, size = 2) + geom_hline(yintercept = 0, color = &quot;gray&quot;, size = 2) + geom_vline(xintercept = 0, color = &quot;gray&quot;, size = 2) + geom_text() + coord_fixed() + labs(x = &quot;Poll Results&quot;, y = &quot;Actual Election Results&quot;) We can create a confusion matrix as follows. Create a new column classification which shows how the poll’s classification was related to the actual election outcome (“true positive”, “false positive”, “true negative”, “false negative”). If there were two outcomes, then we would use the function. But with more than two outcomes, it is easier to use the dplyr function . last_polls &lt;- last_polls %&gt;% ungroup() %&gt;% mutate(classification = case_when( (.$margin &gt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;true positive&quot;, (.$margin &gt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;false positive&quot;, (.$margin &lt; 0 &amp; .$elec_margin &lt; 0) ~ &quot;true negative&quot;, (.$margin &lt; 0 &amp; .$elec_margin &gt; 0) ~ &quot;false negative&quot; )) You need to use . to refer to the data frame when using case_when() within mutate(). Also, we needed to first use in order to remove the grouping variable so mutate() will work. Now simply count the number of polls in each category of classification: last_polls %&gt;% group_by(classification) %&gt;% count() #&gt; # A tibble: 4 x 2 #&gt; # Groups: classification [4] #&gt; classification n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 false negative 2 #&gt; 2 false positive 1 #&gt; 3 true negative 21 #&gt; 4 true positive 27 Which states were incorrectly predicted by the polls, and what was their margins? last_polls %&gt;% filter(classification %in% c(&quot;false positive&quot;, &quot;false negative&quot;)) %&gt;% select(state, margin, elec_margin, classification) %&gt;% arrange(desc(elec_margin)) #&gt; # A tibble: 3 x 4 #&gt; state margin elec_margin classification #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 IN -5 1 false negative #&gt; 2 NC -1 1 false negative #&gt; 3 MO 1 -1 false positive What was the difference in the poll prediction of electoral votes and actual electoral votes? We hadn’t included the variable EV when we first merged, but that’s no problem, we’ll just merge again in order to grab that variable: last_polls %&gt;% left_join(select(pres08, state, EV), by = &quot;state&quot;) %&gt;% summarise(EV_pred = sum( (margin &gt; 0) * EV), EV_actual = sum( (elec_margin &gt; 0) * EV)) #&gt; # A tibble: 1 x 2 #&gt; EV_pred EV_actual #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 349 364 data(&quot;pollsUS08&quot;, package = &quot;qss&quot;) pollsUS08 &lt;- mutate(pollsUS08, DaysToElection = ELECTION_DAY - middate) We’ll produce the seven-day averages slightly differently than the method used in the text. For all dates in the data, we’ll calculate the moving average. The code presented in QSS uses a for loop similar to the following: all_dates &lt;- seq(min(polls08$middate), ELECTION_DAY, by = &quot;days&quot;) # Number of poll days to use POLL_DAYS &lt;- 7 pop_vote_avg &lt;- vector(length(all_dates), mode = &quot;list&quot;) for (i in seq_along(all_dates)) { date &lt;- all_dates[i] # summarise the seven day week_data &lt;- filter(polls08, as.integer(middate - date) &lt;= 0, as.integer(middate - date) &gt; - POLL_DAYS) %&gt;% summarise(Obama = mean(Obama, na.rm = TRUE), McCain = mean(McCain, na.rm = TRUE)) # add date for the observation week_data$date &lt;- date pop_vote_avg[[i]] &lt;- week_data } pop_vote_avg &lt;- bind_rows(pop_vote_avg) Write a function which takes a date, and calculates the days (set the default to 7 days) moving average using the dataset .data: poll_ma &lt;- function(date, .data, days = 7) { filter(.data, as.integer(middate - date) &lt;= 0, as.integer(middate - date) &gt; - !!days) %&gt;% summarise(Obama = mean(Obama, na.rm = TRUE), McCain = mean(McCain, na.rm = TRUE)) %&gt;% mutate(date = !!date) } The code above uses !!. This tells filter that days refers to a variable days in the calling environment, and not a column named days in the data frame. In this case, there wouldn’t be any ambiguities since there is not a column named days, but in general there can be ambiguities in the dplyr functions as to whether the names refer to columns in the data frame or variables in the environment calling the function. Read Programming with dplyr for an in-depth discussion of this. This returns a one row data frame with the moving average for McCain and Obama on Nov 1, 2008. poll_ma(as.Date(&quot;2008-11-01&quot;), polls08) #&gt; Obama McCain date #&gt; 1 49.1 45.4 2008-11-01 Since we made days an argument to the function we could easily change the code to calculate other moving averages, poll_ma(as.Date(&quot;2008-11-01&quot;), polls08, days = 3) #&gt; Obama McCain date #&gt; 1 50.6 45.4 2008-11-01 Now use a functional to execute that function with all dates for which we want moving averages. The function poll_ma returns a data frame, and our ideal output is a data frame that stacks those data frames row-wise. So we will use the map_df function, map_df(all_dates, poll_ma, polls08) Note that the other arguments for poll_ma are placed after the name of the function as additional arguments to map_df. It is easier to plot this if the data are tidy, with Obama and McCain as categories of a column candidate. pop_vote_avg_tidy &lt;- pop_vote_avg %&gt;% gather(candidate, share, -date, na.rm = TRUE) head(pop_vote_avg_tidy) #&gt; date candidate share #&gt; 1 2008-01-01 Obama 46.5 #&gt; 2 2008-01-02 Obama 46.5 #&gt; 3 2008-01-03 Obama 46.5 #&gt; 4 2008-01-04 Obama 46.5 #&gt; 5 2008-01-05 Obama 46.5 #&gt; 6 2008-01-06 Obama 46.5 ggplot(pop_vote_avg_tidy, aes(x = date, y = share, colour = fct_reorder2(candidate, date, share))) + geom_point() + geom_line() + scale_colour_manual(&quot;Candidate&quot;, values = c(Obama = &quot;blue&quot;, McCain = &quot;red&quot;)) Challenge read R for Data Science chapter Iteration and use the function map_df to create the object poll_vote_avg as above instead of a for loop. The 7-day average is similar to the simple method used by Real Clear Politics. The RCP average is simply the average of all polls in their data for the last seven days. Sites like 538 and the Huffpost Pollster, on the other hand, also use what amounts to averaging polls, but using more sophisticated statistical methods to assign different weights to different polls. Challenge Why do we need to use different polls for the popular vote data? Why not simply average all the state polls? What would you have to do? Would the overall popular vote be useful in predicting state-level polling, or vice-versa? How would you use them? 4.2 Linear Regression 4.2.1 Facial Appearance and Election Outcomes Load the face dataset: data(&quot;face&quot;, package = &quot;qss&quot;) Add Democrat and Republican vote shares, and the difference in shares: face &lt;- mutate(face, d.share = d.votes / (d.votes + r.votes), r.share = r.votes / (d.votes + r.votes), diff.share = d.share - r.share) Plot facial competence vs. vote share: ggplot(face, aes(x = d.comp, y = diff.share, colour = w.party)) + geom_ref_line(h = 0) + geom_point() + scale_colour_manual(&quot;Winning\\nParty&quot;, values = c(D = &quot;blue&quot;, R = &quot;red&quot;)) + labs(x = &quot;Competence scores for Democrats&quot;, y = &quot;Democratic margin in vote share&quot;) 4.2.2 Correlation and Scatter Plots cor(face$d.comp, face$diff.share) #&gt; [1] 0.433 4.2.3 Least Squares Run the linear regression fit &lt;- lm(diff.share ~ d.comp, data = face) fit #&gt; #&gt; Call: #&gt; lm(formula = diff.share ~ d.comp, data = face) #&gt; #&gt; Coefficients: #&gt; (Intercept) d.comp #&gt; -0.312 0.660 There are many functions to get data out of the lm model. In addition to these, the broom package provides three functions: glance, tidy, and augment that always return data frames. The function glance returns a one-row data-frame summary of the model, glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.187 0.18 0.266 27 8.85e-07 2 -10.5 27 35.3 #&gt; deviance df.residual #&gt; 1 8.31 117 The function tidy returns a data frame in which each row is a coefficient, tidy(fit) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) -0.312 0.066 -4.73 6.24e-06 #&gt; 2 d.comp 0.660 0.127 5.19 8.85e-07 The function augment returns the original data with fitted values, residuals, and other observation level stats from the model appended to it. augment(fit) %&gt;% head() #&gt; diff.share d.comp .fitted .se.fit .resid .hat .sigma .cooksd #&gt; 1 0.2101 0.565 0.0606 0.0266 0.1495 0.00996 0.267 0.001600 #&gt; 2 0.1194 0.342 -0.0864 0.0302 0.2059 0.01286 0.267 0.003938 #&gt; 3 0.0499 0.612 0.0922 0.0295 -0.0423 0.01229 0.268 0.000158 #&gt; 4 0.1965 0.542 0.0454 0.0256 0.1511 0.00922 0.267 0.001510 #&gt; 5 0.4958 0.680 0.1370 0.0351 0.3588 0.01737 0.266 0.016307 #&gt; 6 -0.3495 0.321 -0.1006 0.0319 -0.2490 0.01433 0.267 0.006436 #&gt; .std.resid #&gt; 1 0.564 #&gt; 2 0.778 #&gt; 3 -0.160 #&gt; 4 0.570 #&gt; 5 1.358 #&gt; 6 -0.941 We can plot the results of the bivariate linear regression as follows: ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_ref_line(v = mean(face$d.comp)) + geom_ref_line(h = mean(face$diff.share)) + geom_abline(slope = coef(fit)[&quot;d.comp&quot;], intercept = coef(fit)[&quot;(Intercept)&quot;], colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = 0.9, y = mean(face$diff.share) + 0.05, label = &quot;Mean of Y&quot;, color = &quot;blue&quot;, vjust = 0) + annotate(&quot;text&quot;, y = -0.9, x = mean(face$d.comp), label = &quot;Mean of X&quot;, color = &quot;blue&quot;, hjust = 0) + scale_y_continuous(&quot;Democratic margin in vote shares&quot;, breaks = seq(-1, 1, by = 0.5), limits = c(-1, 1)) + scale_x_continuous(&quot;Democratic margin in vote shares&quot;, breaks = seq(0, 1, by = 0.2), limits = c(0, 1)) + ggtitle(&quot;Facial compotence and vote share&quot;) A more general way to plot the predictions of the model against the data is to use the methods described in Ch 23.3.3 of R4DS. Create an evenly spaced grid of values of d.comp, and add predictions of the model to it. grid &lt;- face %&gt;% data_grid(d.comp) %&gt;% add_predictions(fit) head(grid) #&gt; # A tibble: 6 x 2 #&gt; d.comp pred #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0640 -0.270 #&gt; 2 0.0847 -0.256 #&gt; 3 0.0893 -0.253 #&gt; 4 0.115 -0.237 #&gt; 5 0.115 -0.236 #&gt; 6 0.164 -0.204 Now we can plot the regression line and the original data just like any other plot. ggplot() + geom_point(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_line(data = grid, mapping = aes(x = d.comp, y = pred), colour = &quot;red&quot;) This method is more complicated than the geom_abline method for a bivariate regression, but will work for more complicated models, while the geom_abline method won’t. Note that geom_smooth can be used to add a regression line to a data-set. ggplot(data = face, mapping = aes(x = d.comp, y = diff.share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The argument method = &quot;lm&quot; specifies that the function lm is to be used to generate fitted values. It is equivalent to running the regression lm(y ~ x) and plotting the regression line, where y and x are the aesthetics specified by the mappings. The argument se = FALSE tells the function not to plot the confidence interval of the regression (discussed later). 4.2.4 Regression towards the mean 4.2.5 Merging Data Sets in R See the R for Data Science chapter Relational data. data(&quot;pres12&quot;, package = &quot;qss&quot;) To join both data frames full_join(pres08, pres12, by = &quot;state&quot;) However, since there are duplicate names, .x and .y are appended. Challenge What would happen if by = &quot;state&quot; was dropped? To avoid the duplicate names, or change them, you can rename before merging, full_join(select(pres08, state, Obama_08 = Obama, McCain_08 = McCain, EV_08 = EV), select(pres12, state, Obama_12 = Obama, Romney_12 = Romney, EV_12 = EV), by = &quot;state&quot;) or use the suffix argument to full_join pres &lt;- full_join(pres08, pres12, by = &quot;state&quot;, suffix = c(&quot;_08&quot;, &quot;_12&quot;)) head(pres) #&gt; state.name state Obama_08 McCain EV_08 margin Obama_12 Romney EV_12 #&gt; 1 Alabama AL 39 60 9 -21 38 61 9 #&gt; 2 Alaska AK 38 59 3 -21 41 55 3 #&gt; 3 Arizona AZ 45 54 10 -9 45 54 11 #&gt; 4 Arkansas AR 39 59 6 -20 37 61 6 #&gt; 5 California CA 61 37 55 24 60 37 55 #&gt; 6 Colorado CO 54 45 9 9 51 46 9 Challenge Would you consider this data tidy? How would you make it tidy? The dplyr equivalent functions for cbind is bind_cols. pres &lt;- pres %&gt;% mutate(Obama2008_z = as.numeric(scale(Obama_08)), Obama2012_z = as.numeric(scale(Obama_12))) Likewise, bind_cols concatenates data frames by row. We need to use the as.numeric function because scale() always returns a matrix. Omitting as.numeric() would not produce an error in the code chunk above, since the columns of a data frame can be matrices, but it would produce errors in some of the following code if it were omitted. Scatter plot of states with vote shares in 2008 and 2012 ggplot(pres, aes(x = Obama2008_z, y = Obama2012_z, label = state)) + geom_abline(colour = &quot;white&quot;, size = 2) + geom_text() + coord_fixed() + scale_x_continuous(&quot;Obama&#39;s standardized vote share in 2008&quot;, limits = c(-4, 4)) + scale_y_continuous(&quot;Obama&#39;s standardized vote share in 2012&quot;, limits = c(-4, 4)) To calculate the bottom and top quartiles pres %&gt;% filter(Obama2008_z &lt; quantile(Obama2008_z, 0.25)) %&gt;% summarise(improve = mean(Obama2012_z &gt; Obama2008_z)) #&gt; improve #&gt; 1 0.583 pres %&gt;% filter(Obama2008_z &lt; quantile(Obama2008_z, 0.75)) %&gt;% summarise(improve = mean(Obama2012_z &gt; Obama2008_z)) #&gt; improve #&gt; 1 0.5 Challenge: Why is it important to standardize the vote shares? 4.2.6 Model Fit data(&quot;florida&quot;, package = &quot;qss&quot;) fit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida) fit2 #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida) #&gt; #&gt; Coefficients: #&gt; (Intercept) Perot96 #&gt; 1.3458 0.0359 Extract \\(R ^ 2\\) from the results of summary, summary(fit2)$r.squared #&gt; [1] 0.513 Alternatively, can get the R squared value from the data frame glance returns: glance(fit2) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.513 0.506 316 68.5 9.47e-12 2 -480 966 972 #&gt; deviance df.residual #&gt; 1 6506118 65 We can add predictions and residuals to the original data frame using the modelr functions add_residuals and add_predictions florida &lt;- florida %&gt;% add_predictions(fit2) %&gt;% add_residuals(fit2) glimpse(florida) #&gt; Observations: 67 #&gt; Variables: 9 #&gt; $ county &lt;chr&gt; &quot;Alachua&quot;, &quot;Baker&quot;, &quot;Bay&quot;, &quot;Bradford&quot;, &quot;Brevard&quot;, &quot;... #&gt; $ Clinton96 &lt;int&gt; 40144, 2273, 17020, 3356, 80416, 320736, 1794, 2712... #&gt; $ Dole96 &lt;int&gt; 25303, 3684, 28290, 4038, 87980, 142834, 1717, 2783... #&gt; $ Perot96 &lt;int&gt; 8072, 667, 5922, 819, 25249, 38964, 630, 7783, 7244... #&gt; $ Bush00 &lt;int&gt; 34124, 5610, 38637, 5414, 115185, 177323, 2873, 354... #&gt; $ Gore00 &lt;int&gt; 47365, 2392, 18850, 3075, 97318, 386561, 2155, 2964... #&gt; $ Buchanan00 &lt;int&gt; 263, 73, 248, 65, 570, 788, 90, 182, 270, 186, 122,... #&gt; $ pred &lt;dbl&gt; 291.3, 25.3, 214.0, 30.8, 908.2, 1400.7, 24.0, 280.... #&gt; $ resid &lt;dbl&gt; -2.83e+01, 4.77e+01, 3.40e+01, 3.42e+01, -3.38e+02,... There are now two new columns in florida, pred with the fitted values (predictions), and resid with the residuals. Use fit2_augment to create a residual plot: fit2_resid_plot &lt;- ggplot(florida, aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) fit2_resid_plot Note, we use the function geom_refline to add a reference line at 0. Let’s add some labels to points, who is that outlier? fit2_resid_plot + geom_label(aes(label = county)) The outlier county is “Palm Beach” arrange(florida) %&gt;% arrange(desc(abs(resid))) %&gt;% select(county, resid) %&gt;% head() #&gt; county resid #&gt; 1 PalmBeach 2302 #&gt; 2 Broward -613 #&gt; 3 Lee -357 #&gt; 4 Brevard -338 #&gt; 5 Miami-Dade -329 #&gt; 6 Pinellas -317 Data without Palm Beach florida_pb &lt;- filter(florida, county != &quot;PalmBeach&quot;) fit3 &lt;- lm(Buchanan00 ~ Perot96, data = florida_pb) fit3 #&gt; #&gt; Call: #&gt; lm(formula = Buchanan00 ~ Perot96, data = florida_pb) #&gt; #&gt; Coefficients: #&gt; (Intercept) Perot96 #&gt; 45.8419 0.0244 \\(R^2\\) or coefficient of determination glance(fit3) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; 1 0.851 0.849 87.7 366 3.61e-28 2 -388 782 788 #&gt; deviance df.residual #&gt; 1 492803 64 florida_pb %&gt;% add_residuals(fit3) %&gt;% add_predictions(fit3) %&gt;% ggplot(aes(x = pred, y = resid)) + geom_ref_line(h = 0) + geom_point() + ylim(-750, 2500) + xlim(0, 1500) + labs(x = &quot;Fitted values&quot;, y = &quot;residuals&quot;) Create predictions for both models using data_grid and gather_predictions: florida_grid &lt;- florida %&gt;% data_grid(Perot96) %&gt;% gather_predictions(fit2, fit3) %&gt;% mutate(model = fct_recode(model, &quot;Regression\\n with Palm Beach&quot; = &quot;fit2&quot;, &quot;Regression\\n without Palm Beach&quot; = &quot;fit3&quot;)) Note this is an example of using non-syntactic column names in a tibble, as discussed in Chapter 10 of R for data science. ggplot() + geom_point(data = florida, mapping = aes(x = Perot96, y = Buchanan00)) + geom_line(data = florida_grid, mapping = aes(x = Perot96, y = pred, colour = model)) + geom_label(data = filter(florida, county == &quot;PalmBeach&quot;), mapping = aes(x = Perot96, y = Buchanan00, label = county), vjust = &quot;top&quot;, hjust = &quot;right&quot;) + geom_text(data = tibble(label = unique(florida_grid$model), x = c(20000, 31000), y = c(1000, 300)), mapping = aes(x = x, y = y, label = label, colour = label)) + labs(x = &quot;Perot&#39;s Vote in 1996&quot;, y = &quot;Buchanan&#39;s Votes in 1996&quot;) + theme(legend.position = &quot;none&quot;) See Graphics for communication in R for Data Science on labels and annotations in plots. 4.3 Regression and Causation 4.3.1 Randomized Experiments Load data data(&quot;women&quot;, package = &quot;qss&quot;) Proportion of female politicians in reserved GP vs. unreserved GP women %&gt;% group_by(reserved) %&gt;% summarise(prop_female = mean(female)) #&gt; # A tibble: 2 x 2 #&gt; reserved prop_female #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0.0748 #&gt; 2 1 1.00 The diff in means estimator: # drinking water facilities # irrigation facilities mean(women$irrigation[women$reserved == 1]) - mean(women$irrigation[women$reserved == 0]) #&gt; [1] -0.369 Mean values of irrigation and water in reserved and non-reserved districts. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) #&gt; # A tibble: 2 x 3 #&gt; reserved irrigation water #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 3.39 14.7 #&gt; 2 1 3.02 24.0 The difference between the two groups can be calculated with the function diff, which calculates the difference between subsequent observations. This works as long as we are careful about which group is first or second. women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% summarise(diff_irrigation = diff(irrigation), diff_water = diff(water)) #&gt; # A tibble: 1 x 2 #&gt; diff_irrigation diff_water #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.369 9.25 The other way uses tidyr spread and gather, women %&gt;% group_by(reserved) %&gt;% summarise(irrigation = mean(irrigation), water = mean(water)) %&gt;% gather(variable, value, -reserved) %&gt;% spread(reserved, value) %&gt;% mutate(diff = `1` - `0`) #&gt; # A tibble: 2 x 4 #&gt; variable `0` `1` diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 irrigation 3.39 3.02 -0.369 #&gt; 2 water 14.7 24.0 9.25 Now each row is an outcome variable of interest, and there are columns for the treatment (1) and control (0) groups, and the difference (diff). lm(water ~ reserved, data = women) #&gt; #&gt; Call: #&gt; lm(formula = water ~ reserved, data = women) #&gt; #&gt; Coefficients: #&gt; (Intercept) reserved #&gt; 14.74 9.25 lm(irrigation ~ reserved, data = women) #&gt; #&gt; Call: #&gt; lm(formula = irrigation ~ reserved, data = women) #&gt; #&gt; Coefficients: #&gt; (Intercept) reserved #&gt; 3.388 -0.369 4.3.2 Regression with multiple predictors data(&quot;social&quot;, package = &quot;qss&quot;) glimpse(social) #&gt; Observations: 305,866 #&gt; Variables: 6 #&gt; $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;mal... #&gt; $ yearofbirth &lt;int&gt; 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 19... #&gt; $ primary2004 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,... #&gt; $ messages &lt;chr&gt; &quot;Civic Duty&quot;, &quot;Civic Duty&quot;, &quot;Hawthorne&quot;, &quot;Hawthorn... #&gt; $ primary2006 &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,... #&gt; $ hhsize &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 1,... levels(social$messages) #&gt; NULL fit &lt;- lm(primary2006 ~ messages, data = social) fit #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ messages, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesControl messagesHawthorne #&gt; 0.31454 -0.01790 0.00784 #&gt; messagesNeighbors #&gt; 0.06341 Create indicator variables for each message: social &lt;- social %&gt;% mutate(Control = as.integer(messages == &quot;Control&quot;), Hawthorne = as.integer(messages == &quot;Hawthorne&quot;), Neighbors = as.integer(messages == &quot;Neighbors&quot;)) alternatively, create these using a for loop. This is easier to understand and less prone to typos: for (i in unique(social$messages)) { social[[i]] &lt;- as.integer(social[[&quot;messages&quot;]] == i) } We created a variable for each level of messages even though we will exclude one of them. lm(primary2006 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ Control + Hawthorne + Neighbors, data = social) #&gt; #&gt; Coefficients: #&gt; (Intercept) Control Hawthorne Neighbors #&gt; 0.31454 -0.01790 0.00784 0.06341 Create predictions for each unique value of messages unique_messages &lt;- data_grid(social, messages) %&gt;% add_predictions(fit) unique_messages #&gt; # A tibble: 4 x 2 #&gt; messages pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Compare to the sample averages social %&gt;% group_by(messages) %&gt;% summarise(mean(primary2006)) #&gt; # A tibble: 4 x 2 #&gt; messages `mean(primary2006)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 #&gt; 2 Control 0.297 #&gt; 3 Hawthorne 0.322 #&gt; 4 Neighbors 0.378 Linear regression without intercept. fit.noint &lt;- lm(primary2006 ~ -1 + messages, data = social) fit.noint #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ -1 + messages, data = social) #&gt; #&gt; Coefficients: #&gt; messagesCivic Duty messagesControl messagesHawthorne #&gt; 0.315 0.297 0.322 #&gt; messagesNeighbors #&gt; 0.378 Calculating the regression average effect is also easier if we make the control group the first level so all regression coefficients are comparisons to it. Use fct_relevel to make “Control” fit.control &lt;- mutate(social, messages = fct_relevel(messages, &quot;Control&quot;)) %&gt;% lm(primary2006 ~ messages, data = .) fit.control #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ messages, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) messagesCivic Duty messagesHawthorne #&gt; 0.2966 0.0179 0.0257 #&gt; messagesNeighbors #&gt; 0.0813 Difference in means social %&gt;% group_by(messages) %&gt;% summarise(primary2006 = mean(primary2006)) %&gt;% mutate(Control = primary2006[messages == &quot;Control&quot;], diff = primary2006 - Control) #&gt; # A tibble: 4 x 4 #&gt; messages primary2006 Control diff #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Civic Duty 0.315 0.297 0.0179 #&gt; 2 Control 0.297 0.297 0 #&gt; 3 Hawthorne 0.322 0.297 0.0257 #&gt; 4 Neighbors 0.378 0.297 0.0813 Adjusted R-squared is included in the output of broom::glance() glance(fit) #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC #&gt; 1 0.00328 0.00327 0.463 336 1.06e-217 4 -198247 396504 #&gt; BIC deviance df.residual #&gt; 1 396557 65468 305862 glance(fit)[[&quot;adj.r.squared&quot;]] #&gt; [1] 0.00327 4.3.3 Heterogeneous Treatment Effects Average treatment effect (ate) among those who voted in 2004 primary ate &lt;- social %&gt;% group_by(primary2004, messages) %&gt;% summarise(primary2006 = mean(primary2006)) %&gt;% spread(messages, primary2006) %&gt;% mutate(ate_Neighbors = Neighbors - Control) %&gt;% select(primary2004, Neighbors, Control, ate_Neighbors) ate #&gt; # A tibble: 2 x 4 #&gt; # Groups: primary2004 [2] #&gt; primary2004 Neighbors Control ate_Neighbors #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.306 0.237 0.0693 #&gt; 2 1 0.482 0.386 0.0965 Difference in ATE in 2004 voters and non-voters diff(ate$ate_Neighbors) #&gt; [1] 0.0272 social.neighbor &lt;- social %&gt;% filter( (messages == &quot;Control&quot;) | (messages == &quot;Neighbors&quot;)) fit.int &lt;- lm(primary2006 ~ primary2004 + messages + primary2004:messages, data = social.neighbor) fit.int #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ primary2004 + messages + primary2004:messages, #&gt; data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 lm(primary2006 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ primary2004 * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) primary2004 #&gt; 0.2371 0.1487 #&gt; messagesNeighbors primary2004:messagesNeighbors #&gt; 0.0693 0.0272 social.neighbor &lt;- social.neighbor %&gt;% mutate(age = 2008 - yearofbirth) summary(social.neighbor$age) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 22.0 43.0 52.0 51.8 61.0 108.0 fit.age &lt;- lm(primary2006 ~ age * messages, data = social.neighbor) fit.age #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ age * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) age messagesNeighbors #&gt; 0.089477 0.003998 0.048573 #&gt; age:messagesNeighbors #&gt; 0.000628 Calculate average treatment effects ate.age &lt;- crossing(age = seq(from = 25, to = 85, by = 20), messages = c(&quot;Neighbors&quot;, &quot;Control&quot;)) %&gt;% add_predictions(fit.age) %&gt;% spread(messages, pred) %&gt;% mutate(diff = Neighbors - Control) ate.age #&gt; # A tibble: 4 x 4 #&gt; age Control Neighbors diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 25.0 0.189 0.254 0.0643 #&gt; 2 45.0 0.269 0.346 0.0768 #&gt; 3 65.0 0.349 0.439 0.0894 #&gt; 4 85.0 0.429 0.531 0.102 You can use poly function to calculate polynomials instead of adding each term, age + I(age ^ 2). Though note that the coefficients will be be different since by default poly calculates orthogonal polynomials instead of the natural (raw) polynomials. However, you really shouldn’t interpret the coefficients directly anyways, so this should matter. fit.age2 &lt;- lm(primary2006 ~ poly(age, 2) * messages, data = social.neighbor) fit.age2 #&gt; #&gt; Call: #&gt; lm(formula = primary2006 ~ poly(age, 2) * messages, data = social.neighbor) #&gt; #&gt; Coefficients: #&gt; (Intercept) poly(age, 2)1 #&gt; 0.2966 27.6665 #&gt; poly(age, 2)2 messagesNeighbors #&gt; -10.2832 0.0816 #&gt; poly(age, 2)1:messagesNeighbors poly(age, 2)2:messagesNeighbors #&gt; 4.5820 -5.5124 Create a data frame of combinations of ages and messages using data_grid, which means that we only need to specify the variables, and not the specific values, y.hat &lt;- data_grid(social.neighbor, age, messages) %&gt;% add_predictions(fit.age2) ggplot(y.hat, aes(x = age, y = pred, colour = str_c(messages, &quot; condition&quot;))) + geom_line() + labs(colour = &quot;&quot;, y = &quot;Predicted turnout rates&quot;) + theme(legend.position = &quot;bottom&quot;) y.hat %&gt;% spread(messages, pred) %&gt;% mutate(ate = Neighbors - Control) %&gt;% filter(age &gt; 20, age &lt; 90) %&gt;% ggplot(aes(x = age, y = ate)) + geom_line() + labs(y = &quot;Estimated average treatment effect&quot;, x = &quot;Age&quot;) + ylim(0, 0.1) 4.3.4 Regression Discontinuity Design data(&quot;MPs&quot;, package = &quot;qss&quot;) MPs_labour &lt;- filter(MPs, party == &quot;labour&quot;) MPs_tory &lt;- filter(MPs, party == &quot;tory&quot;) labour_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &lt; 0)) labour_fit2 &lt;- lm(ln.net ~ margin, MPs_labour, margin &gt; 0) tory_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &gt; 0)) Use to generate a grid for predictions. y1_labour &lt;- filter(MPs_labour, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit1) y2_labour &lt;- filter(MPs_labour, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(labour_fit2) y1_tory &lt;- filter(MPs_tory, margin &lt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit1) y2_tory &lt;- filter(MPs_tory, margin &gt; 0) %&gt;% data_grid(margin) %&gt;% add_predictions(tory_fit2) Tory politicians ggplot() + geom_ref_line(v = 0) + geom_point(data = MPs_tory, mapping = aes(x = margin, y = ln.net)) + geom_line(data = y1_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;, size = 1.5) + geom_line(data = y2_tory, mapping = aes(x = margin, y = pred), colour = &quot;red&quot;, size = 1.5) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;, title = &quot;labour&quot;) We can actually produce this plot easily without running the regressions, by using geom_smooth: ggplot(mutate(MPs, winner = (margin &gt; 0)), aes(x = margin, y = ln.net)) + geom_ref_line(v = 0) + geom_point() + geom_smooth(method = lm, se = FALSE, mapping = aes(group = winner)) + facet_grid(party ~ .) + labs(x = &quot;margin of victory&quot;, y = &quot;log net wealth at death&quot;) In the previous code, I didn’t directly compute the the average net wealth at 0, so I’ll need to do that here. I’ll use gather_predictions to add predictions for multiple models: spread_predictions(data_frame(margin = 0), tory_fit1, tory_fit2) %&gt;% mutate(rd_est = tory_fit2 - tory_fit1) #&gt; # A tibble: 1 x 4 #&gt; margin tory_fit1 tory_fit2 rd_est #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 12.5 13.2 0.650 tory_fit3 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit4 &lt;- lm(margin.pre ~ margin, data = filter(MPs_tory, margin &gt; 0)) (filter(tidy(tory_fit4), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]] - filter(tidy(tory_fit3), term == &quot;(Intercept)&quot;)[[&quot;estimate&quot;]]) #&gt; [1] -0.0173 5 Discovery The idea of tidy data and the common feature of tidyverse packages is that data should be stored in data frames with certain conventions. This works well with naturally tabular data, the type which has been common in social science applications. But there are other domains in which other data structures are more appropriate because they more naturally model the data or processes, or for computational reasons. The three applications in this chapter: text, networks, and spatial data are examples where the tidy data structure is less of an advantage. I will still rely on ggplot2 for plotting, and use tidy verse compatible packages where appropriate. Textual data: tidytext Network data: igraph for network computation, as in the chapter. But several ggplot22 extension packages for plotting the networks. Spatial data: ggplot2 has some built-in support for maps. The map package provides map data. See the R for Data Science section 12.7 Non-tidy data and this post on Non-tidy data by Jeff Leek for more on non-tidy data. 5.1 Textual data Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) library(&quot;modelr&quot;) library(&quot;tm&quot;) library(&quot;SnowballC&quot;) library(&quot;tidytext&quot;) library(&quot;wordcloud&quot;) This section will primarily use the tidytext package. It is a relatively new package. The tm and quanteda (by Ken Benoit) packages are more established and use the document-term matrix format as described in the QSS chapter. The tidytext package stores everything in a data frame; this may be less efficient than the other packages, but has the benefit of being able to easily take advantage of the tidyverse ecosystem. If your corpus is not too large, this shouldn’t be an issue. See Tidy Text Mining with R for a full introduction to using tidytext. In tidy data, each row is an observation and each column is a variable. In the tidytext package, documents are stored as data frames with one-term-per-row. We can cast data into the tidytext format either from the Corpus object, or, after processing, from the document-term matrix object. DIR_SOURCE &lt;- system.file(&quot;extdata/federalist&quot;, package = &quot;qss&quot;) corpus_raw &lt;- VCorpus(DirSource(directory = DIR_SOURCE, pattern = &quot;fp&quot;)) corpus_raw #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 85 Use the function tidy to convert the to a data frame with one row per document. corpus_tidy &lt;- tidy(corpus_raw, &quot;corpus&quot;) corpus_tidy #&gt; # A tibble: 85 x 8 #&gt; author datetimestamp description heading id lang… orig… text #&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 NA 2018-01-13 13:32:27 NA NA fp01.… en NA AFTER… #&gt; 2 NA 2018-01-13 13:32:27 NA NA fp02.… en NA &quot;WHEN… #&gt; 3 NA 2018-01-13 13:32:27 NA NA fp03.… en NA IT IS… #&gt; 4 NA 2018-01-13 13:32:27 NA NA fp04.… en NA &quot;MY L… #&gt; 5 NA 2018-01-13 13:32:27 NA NA fp05.… en NA &quot;QUEE… #&gt; 6 NA 2018-01-13 13:32:27 NA NA fp06.… en NA &quot;THE … #&gt; # ... with 79 more rows The text column contains the text of the documents themselves. Since most of the metadata columns are either missings or irrelevant for our purposes, we’ll delete those columns, keeping only the document (id) and text columns. corpus_tidy &lt;- select(corpus_tidy, id, text) Also, we want to extract the essay number and use that as the document id rather than its file name. corpus_tidy &lt;- mutate(corpus_tidy, document = as.integer(str_extract(id, &quot;\\\\d+&quot;))) %&gt;% select(-id) The function tokenizes the document texts: tokens &lt;- corpus_tidy %&gt;% # tokenizes into words and stems them unnest_tokens(word, text, token = &quot;word_stems&quot;) %&gt;% # remove any numbers in the strings mutate(word = str_replace_all(word, &quot;\\\\d+&quot;, &quot;&quot;)) %&gt;% # drop any empty strings filter(word != &quot;&quot;) tokens #&gt; # A tibble: 202,089 x 2 #&gt; document word #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 after #&gt; 2 1 an #&gt; 3 1 unequivoc #&gt; 4 1 experi #&gt; 5 1 of #&gt; 6 1 the #&gt; # ... with 2.021e+05 more rows The unnest_tokens function uses the tokenizers package to tokenize the text. By default, it uses the function which removes punctuation, and lowercases the words. I set the tokenizer to to stem the word, using the SnowballC package. We can remove stop-words with an anti_join on the dataset stop_words data(&quot;stop_words&quot;, package = &quot;tidytext&quot;) tokens &lt;- anti_join(tokens, stop_words, by = &quot;word&quot;) 5.1.1 Document-Term Matrix In tokens there is one observation for each token (word) in the each document. This is almost equivalent to a document-term matrix. For a document-term matrix we need documents, and terms as the keys for the data and a column with the number of times the term appeared in the document. dtm &lt;- count(tokens, document, word) head(dtm) #&gt; # A tibble: 6 x 3 #&gt; document word n #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 abl 1 #&gt; 2 1 absurd 1 #&gt; 3 1 accid 1 #&gt; 4 1 accord 1 #&gt; 5 1 acknowledg 1 #&gt; 6 1 act 1 5.1.2 Topic Discovery Plot the word-clouds for essays 12 and 24: filter(dtm, document == 12) %&gt;% { wordcloud(.$word, .$n, max.words = 20) } filter(dtm, document == 24) %&gt;% { wordcloud(.$word, .$n, max.words = 20) } Use the function bind_tf_idf to add a column with the tf-idf to the data frame. dtm &lt;- bind_tf_idf(dtm, word, document, n) dtm #&gt; # A tibble: 38,847 x 6 #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 abl 1 0.00145 0.705 0.00102 #&gt; 2 1 absurd 1 0.00145 1.73 0.00251 #&gt; 3 1 accid 1 0.00145 3.75 0.00543 #&gt; 4 1 accord 1 0.00145 0.754 0.00109 #&gt; 5 1 acknowledg 1 0.00145 1.55 0.00225 #&gt; 6 1 act 1 0.00145 0.400 0.000579 #&gt; # ... with 3.884e+04 more rows The 10 most important words for Paper No. 12 are dtm %&gt;% filter(document == 12) %&gt;% top_n(10, tf_idf) #&gt; # A tibble: 10 x 6 #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 12 cent 2 0.00199 4.44 0.00884 #&gt; 2 12 coast 3 0.00299 3.75 0.0112 #&gt; 3 12 commerc 8 0.00796 1.11 0.00884 #&gt; 4 12 contraband 3 0.00299 4.44 0.0133 #&gt; 5 12 excis 5 0.00498 2.65 0.0132 #&gt; 6 12 gallon 2 0.00199 4.44 0.00884 #&gt; # ... with 4 more rows and for Paper No. 24, dtm %&gt;% filter(document == 24) %&gt;% top_n(10, tf_idf) #&gt; # A tibble: 10 x 6 #&gt; document word n tf idf tf_idf #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 24 armi 7 0.00858 1.26 0.0108 #&gt; 2 24 arsenal 2 0.00245 3.75 0.00919 #&gt; 3 24 dock 3 0.00368 4.44 0.0163 #&gt; 4 24 frontier 3 0.00368 2.83 0.0104 #&gt; 5 24 garrison 6 0.00735 2.83 0.0208 #&gt; 6 24 nearer 2 0.00245 3.34 0.00820 #&gt; # ... with 4 more rows The slightly different results from the book are due to tokenization differences. Subset those documents known to have been written by Hamilton. HAMILTON_ESSAYS &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) dtm_hamilton &lt;- filter(dtm, document %in% HAMILTON_ESSAYS) The kmeans function expects the input to be rows for observations and columns for each variable: in our case that would be documents as rows, and words as columns, with the tf-idf as the cell values. We could use spread to do this, but that would be a large matrix. CLUSTERS &lt;- 4 km_out &lt;- kmeans(cast_dtm(dtm_hamilton, document, word, tf_idf), centers = CLUSTERS, nstart = 10) km_out$iter #&gt; [1] 3 Data frame with the unique terms used by Hamilton. I extract these from the column names of the DTM after cast_dtm to ensure that the order is the same as the k-means results. hamilton_words &lt;- tibble(word = colnames(cast_dtm(dtm_hamilton, document, word, tf_idf))) The centers of the clusters is a cluster x word matrix. We want to transpose it and then append columns to hamilton_words so the location of each word in the cluster is listed. dim(km_out$centers) #&gt; [1] 4 3850 hamilton_words &lt;- bind_cols(hamilton_words, as_tibble(t(km_out$centers))) hamilton_words #&gt; # A tibble: 3,850 x 5 #&gt; word `1` `2` `3` `4` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 abl 0.000939 0.000743 0 0 #&gt; 2 absurd 0 0.000517 0 0.000882 #&gt; 3 accid 0 0.000202 0 0 #&gt; 4 accord 0 0.000399 0 0.000852 #&gt; 5 acknowledg 0 0.000388 0 0.000473 #&gt; 6 act 0 0.000560 0.00176 0.000631 #&gt; # ... with 3,844 more rows To find the top 10 words in each centroid, we use top_n with group_by: top_words_cluster &lt;- gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) We can print them out using a for loop for (i in 1:CLUSTERS) { cat(&quot;CLUSTER &quot;, i, &quot;: &quot;, str_c(filter(top_words_cluster, cluster == i)$word, collapse = &quot;, &quot;), &quot;\\n\\n&quot;) } #&gt; CLUSTER 1 : presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc #&gt; #&gt; CLUSTER 2 : offic, presid, tax, land, revenu, armi, militia, senat, taxat, claus #&gt; #&gt; CLUSTER 3 : sedit, guilt, chief, clemenc, impun, plead, crime, pardon, treason, conniv #&gt; #&gt; CLUSTER 4 : court, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, impeach, appel This is alternative code that prints out a table: gather(hamilton_words, cluster, value, -word) %&gt;% group_by(cluster) %&gt;% top_n(10, value) %&gt;% summarise(top_words = str_c(word, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster top_words 1 presid, appoint, senat, claus, expir, fill, recess, session, unfound, vacanc 2 offic, presid, tax, land, revenu, armi, militia, senat, taxat, claus 3 sedit, guilt, chief, clemenc, impun, plead, crime, pardon, treason, conniv 4 court, jurisdict, inferior, suprem, trial, tribun, cogniz, juri, impeach, appel Or to print out the documents in each cluster, enframe(km_out$cluster, &quot;document&quot;, &quot;cluster&quot;) %&gt;% group_by(cluster) %&gt;% summarise(documents = str_c(document, collapse = &quot;, &quot;)) %&gt;% knitr::kable() cluster documents 1 67 2 1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60, 61, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 84, 85 3 74 4 65, 81, 82, 83 5.1.3 Authorship Prediction We’ll create a data-frame with the known MADISON_ESSAYS &lt;- c(10, 14, 37:48, 58) JAY_ESSAYS &lt;- c(2:5, 64) known_essays &lt;- bind_rows(tibble(document = MADISON_ESSAYS, author = &quot;Madison&quot;), tibble(document = HAMILTON_ESSAYS, author = &quot;Hamilton&quot;), tibble(document = JAY_ESSAYS, author = &quot;Jay&quot;)) STYLE_WORDS &lt;- tibble(word = c(&quot;although&quot;, &quot;always&quot;, &quot;commonly&quot;, &quot;consequently&quot;, &quot;considerable&quot;, &quot;enough&quot;, &quot;there&quot;, &quot;upon&quot;, &quot;while&quot;, &quot;whilst&quot;)) hm_tfm &lt;- unnest_tokens(corpus_tidy, word, text) %&gt;% count(document, word) %&gt;% # term freq per 1000 words group_by(document) %&gt;% mutate(count = n / sum(n) * 1000) %&gt;% select(-n) %&gt;% inner_join(STYLE_WORDS, by = &quot;word&quot;) %&gt;% # merge known essays left_join(known_essays, by = &quot;document&quot;) %&gt;% # make wide with each word a column # fill empty values with 0 spread(word, count, fill = 0) Calculate average usage by each author of each word hm_tfm %&gt;% # remove docs with no author filter(!is.na(author)) %&gt;% # convert back to long (tidy) format to make it easier to summarize gather(word, count, -document, -author) %&gt;% # calculate averge document word usage by author group_by(author, word) %&gt;% summarise(avg_count = mean(count)) %&gt;% spread(author, avg_count) %&gt;% knitr::kable() word Hamilton Jay Madison although 0.012 0.543 0.206 always 0.522 0.929 0.154 commonly 0.184 0.129 0.000 consequently 0.018 0.469 0.344 considerable 0.377 0.081 0.123 enough 0.274 0.000 0.000 there 3.065 0.954 0.849 upon 3.054 0.112 0.152 while 0.255 0.192 0.000 whilst 0.005 0.000 0.292 author_data &lt;- hm_tfm %&gt;% ungroup() %&gt;% filter(is.na(author) | author != &quot;Jay&quot;) %&gt;% mutate(author2 = case_when(.$author == &quot;Hamilton&quot; ~ 1, .$author == &quot;Madison&quot; ~ -1, TRUE ~ NA_real_)) hm_fit &lt;- lm(author2 ~ upon + there + consequently + whilst, data = author_data) hm_fit #&gt; #&gt; Call: #&gt; lm(formula = author2 ~ upon + there + consequently + whilst, #&gt; data = author_data) #&gt; #&gt; Coefficients: #&gt; (Intercept) upon there consequently whilst #&gt; -0.195 0.229 0.127 -0.644 -0.984 author_data &lt;- author_data %&gt;% add_predictions(hm_fit) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;)) sd(author_data$pred) #&gt; [1] 0.79 These coefficients are a little different, probably due to differences in the tokenization procedure, and in particular, the document size normalization. 5.1.4 Cross-Validation tidyverse: For cross-validation, I rely on the modelr package function RDoc(&quot;modelr::crossv_kfold&quot;). See the tutorial Cross validation of linear regression with modelr for more on using modelr for cross validation or k-fold cross-validation with modelr and broom. In sample, this regression perfectly predicts the authorship of the documents with known authors. author_data %&gt;% filter(!is.na(author)) %&gt;% group_by(author) %&gt;% summarise(`Proportion Correct` = mean(author == pred_author)) #&gt; # A tibble: 2 x 2 #&gt; author `Proportion Correct` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1.00 #&gt; 2 Madison 1.00 Create the cross-validation data-sets using . As in the chapter, I will use a leave-one-out cross-validation, which is a k-fold cross-validation where k is the number of observations. To simplify this, I define the crossv_loo function that runs crossv_kfold with k = nrow(data). crossv_loo &lt;- function(data, id = &quot;.id&quot;) { modelr::crossv_kfold(data, k = nrow(data), id = id) } # leave one out cross-validation object cv &lt;- author_data %&gt;% filter(!is.na(author)) %&gt;% crossv_loo() Now estimate the model for each training dataset models &lt;- purrr::map(cv$train, ~ lm(author2 ~ upon + there + consequently + whilst, data = ., model = FALSE)) Note that I use purrr::map to ensure that the correct map() function is used since the maps package also defines a map. Now calculate the test performance on the held out observation, test &lt;- map2_df(models, cv$test, function(mod, test) { add_predictions(as.data.frame(test), mod) %&gt;% mutate(pred_author = if_else(pred &gt;= 0, &quot;Hamilton&quot;, &quot;Madison&quot;), correct = (pred_author == author)) }) test %&gt;% group_by(author) %&gt;% summarise(mean(correct)) #&gt; # A tibble: 2 x 2 #&gt; author `mean(correct)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Hamilton 1.00 #&gt; 2 Madison 0.786 When adding prediction with add_predictions it added predictions for missing values as well. Table of authorship of disputed papers author_data %&gt;% filter(is.na(author)) %&gt;% select(document, pred, pred_author) %&gt;% knitr::kable() document pred pred_author 18 -0.360 Madison 19 -0.587 Madison 20 -0.055 Madison 49 -0.966 Madison 50 -0.003 Madison 51 -1.520 Madison 52 -0.195 Madison 53 -0.506 Madison 54 -0.521 Madison 55 0.094 Hamilton 56 -0.550 Madison 57 -1.221 Madison 62 -0.946 Madison 63 -0.184 Madison disputed_essays &lt;- filter(author_data, is.na(author))$document ggplot(mutate(author_data, author = fct_explicit_na(factor(author), &quot;Disputed&quot;)), aes(y = document, x = pred, colour = author, shape = author)) + geom_ref_line(v = 0) + geom_point() + scale_y_continuous(breaks = seq(10, 80, by = 10), minor_breaks = seq(5, 80, by = 5)) + scale_color_manual(values = c(&quot;Madison&quot; = &quot;blue&quot;, &quot;Hamilton&quot; = &quot;red&quot;, &quot;Disputed&quot; = &quot;black&quot;)) + scale_shape_manual(values = c(&quot;Madison&quot; = 16, &quot;Hamilton&quot; = 15, &quot;Disputed&quot; = 17)) + labs(colour = &quot;Author&quot;, shape = &quot;Author&quot;, y = &quot;Federalist Papers&quot;, x = &quot;Predicted values&quot;) 5.2 Network data The igraph, sna, and network packages are the best in class. See the Social Network Analysis section of the Social Sciences Task View. See this tutorial by Katherin Ognyanova, Static and dynamic network visualization with R, for a good overview of network visualization with those packages in R. There are several packages that plot networks in ggplot2. ggnetwork ggraph geomnet GGally functions ggnet, ggnet2, and ggnetworkmap. ggCompNet compares the speed of various network plotting packages in R. See this presentation for an overview of some of those packages for data visualization. Examples: Network Visualization Examples with the ggplot2 Package Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) library(&quot;igraph&quot;) library(&quot;intergraph&quot;) library(&quot;GGally&quot;) 5.2.1 Twitter Following Network data(&quot;twitter.following&quot;, package = &quot;qss&quot;) data(&quot;twitter.senator&quot;, package = &quot;qss&quot;) Since the names twitter.following and twitter.senator are verbose, we’ll simplify future code by copying their values to variables named twitter and senator, respectively. twitter &lt;- twitter.following senator &lt;- twitter.senator Simply use the function since twitter consists of edges (a link from a senator to another). Since graph_from_edgelist expects a matrix, convert the data frame to a matrix using . twitter_adj &lt;- graph_from_edgelist(as.matrix(twitter)) Add in- and out-degree variables to the senator data frame: senator &lt;- mutate(senator, indegree = igraph::degree(twitter_adj, mode = &quot;in&quot;), outdegree = igraph::degree(twitter_adj, mode = &quot;out&quot;)) Now find the senators with the 3 greatest in-degrees arrange(senator, desc(indegree)) %&gt;% slice(1:3) %&gt;% select(name, party, state, indegree, outdegree) #&gt; # A tibble: 3 x 5 #&gt; name party state indegree outdegree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tom Cotton R AR 64.0 15.0 #&gt; 2 Richard J. Durbin D IL 60.0 87.0 #&gt; 3 John Barrasso R WY 58.0 79.0 or using the function: top_n(senator, 3, indegree) %&gt;% arrange(desc(indegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; name party state indegree outdegree #&gt; 1 Tom Cotton R AR 64 15 #&gt; 2 Richard J. Durbin D IL 60 87 #&gt; 3 John Barrasso R WY 58 79 #&gt; 4 Joe Donnelly D IN 58 9 #&gt; 5 Orrin G. Hatch R UT 58 50 The top_n function catches that three senators are tied for 3rd highest outdegree, whereas the simply sorting and slicing cannot. And we can find the senators with the three highest out-degrees similarly, top_n(senator, 3, outdegree) %&gt;% arrange(desc(outdegree)) %&gt;% select(name, party, state, indegree, outdegree) #&gt; name party state indegree outdegree #&gt; 1 Thad Cochran R MS 55 89 #&gt; 2 Steve Daines R MT 30 88 #&gt; 3 John McCain R AZ 41 88 #&gt; 4 Joe Manchin, III D WV 43 88 # Define scales to reuse for the plots scale_colour_parties &lt;- scale_colour_manual(&quot;Party&quot;, values = c(R = &quot;red&quot;, D = &quot;blue&quot;, I = &quot;green&quot;)) scale_shape_parties &lt;- scale_shape_manual(&quot;Party&quot;, values = c(R = 16, D = 17, I = 4)) senator %&gt;% mutate(closeness_in = igraph::closeness(twitter_adj, mode = &quot;in&quot;), closeness_out = igraph::closeness(twitter_adj, mode = &quot;out&quot;)) %&gt;% ggplot(aes(x = closeness_in, y = closeness_out, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + labs(main = &quot;Closeness&quot;, x = &quot;Incoming path&quot;, y = &quot;Outgoing path&quot;) What does the reference line indicate? What does that say about senators twitter networks? senator %&gt;% mutate(betweenness_dir = igraph::betweenness(twitter_adj, directed = TRUE), betweenness_undir = igraph::betweenness(twitter_adj, directed = FALSE)) %&gt;% ggplot(aes(x = betweenness_dir, y = betweenness_undir, colour = party, shape = party)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + scale_colour_parties + scale_shape_parties + labs(main = &quot;Betweenness&quot;, x = &quot;Directed&quot;, y = &quot;Undirected&quot;) We’ve covered three different methods of calculating the importance of a node in a network: degree, closeness, and centrality. But what do they mean? What’s the “best” measure of importance? The answer to the the former is “it depends on the question”. There are probably other papers out there on this, but Borgatti (2005) is a good discussion: Borgatti, Stephen. 2005. “Centrality and Network Flow”. Social Networks. DOI Add and plot page-rank: senator &lt;- mutate(senator, page_rank = page_rank(twitter_adj)[[&quot;vector&quot;]]) ggnet(twitter_adj, mode = &quot;target&quot;) 5.3 Spatial Data Some resources on plotting spatial data in R: ggplot2 has several map-related functions borders fortify.map map_data ggmap allows ggplot to us a map from Google Maps, OpenStreet Maps or similar as a background for the plot. David Kahle and Hadley Wickham. 2013. ggmap: Spatial Visualization with ggplot2. Journal of Statistical Software Github dkahle/ggmamp tmap is not built on ggplot2 but uses a ggplot2-like API for network data. leaflet is an R interface to a popular javascript mapping library. Here are few tutorials on plotting spatial data in ggplot2: Making Maps with R Plotting Data on a World Map Introduction to Spatial Data and ggplot2 Prerequisites library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;forcats&quot;) library(&quot;modelr&quot;) library(&quot;ggrepel&quot;) 5.3.1 Spatial Data in R data(&quot;us.cities&quot;, package = &quot;maps&quot;) glimpse(us.cities) #&gt; Observations: 1,005 #&gt; Variables: 6 #&gt; $ name &lt;chr&gt; &quot;Abilene TX&quot;, &quot;Akron OH&quot;, &quot;Alameda CA&quot;, &quot;Albany GA... #&gt; $ country.etc &lt;chr&gt; &quot;TX&quot;, &quot;OH&quot;, &quot;CA&quot;, &quot;GA&quot;, &quot;NY&quot;, &quot;OR&quot;, &quot;NM&quot;, &quot;LA&quot;, &quot;V... #&gt; $ pop &lt;int&gt; 113888, 206634, 70069, 75510, 93576, 45535, 494962... #&gt; $ lat &lt;dbl&gt; 32.5, 41.1, 37.8, 31.6, 42.7, 44.6, 35.1, 31.3, 38... #&gt; $ long &lt;dbl&gt; -99.7, -81.5, -122.3, -84.2, -73.8, -123.1, -106.6... #&gt; $ capital &lt;int&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... usa_map &lt;- map_data(&quot;usa&quot;) #&gt; #&gt; Attaching package: &#39;maps&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; map capitals &lt;- filter(us.cities, capital == 2, !country.etc %in% c(&quot;HI&quot;, &quot;AK&quot;)) ggplot() + geom_map(map = usa_map) + borders(database = &quot;usa&quot;) + geom_point(aes(x = long, y = lat, size = pop), data = capitals) + # scale size area ensures: 0 = no area scale_size_area() + coord_quickmap() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;US State Capitals&quot;, size = &quot;Population&quot;) cal_cities &lt;- filter(us.cities, country.etc == &quot;CA&quot;) %&gt;% top_n(7, pop) ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;) + geom_point(aes(x = long, y = lat), data = cal_cities) + geom_text_repel(aes(x = long, y = lat, label = name), data = cal_cities) + coord_quickmap() + theme_minimal() + labs(x = &quot;&quot;, y = &quot;&quot;) 5.3.2 Colors in R For more resources on using colors in R R4DS chapter Graphics for Communication ggplot2 book Chapter “Scales” Jenny Bryan Using colors in R Achim Zeileis, Kurt Hornik, Paul Murrell (2009). Escaping RGBland: Selecting Colors for Statistical Graphics. Computational Statistics &amp; Data Analysis DOI colorspace vignette Maureen Stone Choosing Colors for Data Visualization ColorBrewer A website with a variety of palettes, primarily designed for maps, but also useful in data viz. Stephen Few Practical Rules for Using Color in Charts Why Should Engineers and Scientists by Worried About Color? A Better Default Colormap for Matplotlib A SciPy 2015 talk that describes how the viridis was created. Evaluation of Artery Visualizations for Heart Disease Diagnosis Using the wrong color scale can be deadly … literally. The python package matplotlib has a good discussion of colormaps. Peter Kovesi Good Color Maps: How to Design Them. See the viridis, ggthemes, dichromat, and pals packages for color palettes. Use scale_identity for the color and alpha scales since the values of the variables are the values of the scale itself (the color names, and the alpha values). ggplot(tibble(x = rep(1:4, each = 2), y = x + rep(c(0, 0.2), times = 2), colour = rep(c(&quot;black&quot;, &quot;red&quot;), each = 4), alpha = c(1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5)), aes(x = x, y = y, colour = colour, alpha = alpha)) + geom_point(size = 15) + scale_color_identity() + scale_alpha_identity() + theme_bw() + theme(panel.grid = element_blank()) 5.3.3 United States Presidential Elections data(&quot;pres08&quot;, package = &quot;qss&quot;) pres08 &lt;- pres08 %&gt;% mutate(Dem = Obama / (Obama + McCain), Rep = McCain / (Obama + McCain)) ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = &quot;blue&quot;) + coord_quickmap() + theme_void() cal_color &lt;- filter(pres08, state == &quot;CA&quot;) %&gt;% { rgb(red = .$Rep, green = 0, blue = .$Dem) } ggplot() + borders(database = &quot;state&quot;, regions = &quot;California&quot;, fill = cal_color) + coord_quickmap() + theme_void() # America as red and blue states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ( (pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = ifelse(pres08$Rep[i] &gt; pres08$Dem[i], &quot;red&quot;, &quot;blue&quot;), fill = TRUE, add = TRUE) } } ## America as purple states map(database = &quot;state&quot;) # create a map for (i in 1:nrow(pres08)) { if ( (pres08$state[i] != &quot;HI&quot;) &amp; (pres08$state[i] != &quot;AK&quot;) &amp; (pres08$state[i] != &quot;DC&quot;)) { map(database = &quot;state&quot;, regions = pres08$state.name[i], col = rgb(red = pres08$Rep[i], blue = pres08$Dem[i], green = 0), fill = TRUE, add = TRUE) } } states &lt;- map_data(&quot;state&quot;) %&gt;% left_join(mutate(pres08, state.name = str_to_lower(state.name)), by = c(&quot;region&quot; = &quot;state.name&quot;)) %&gt;% # drops DC filter(!is.na(EV)) %&gt;% mutate(party = if_else(Dem &gt; Rep, &quot;Dem&quot;, &quot;Rep&quot;), color = map2_chr(Dem, Rep, ~ rgb(blue = .x, red = .y, green = 0))) ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = party)) + coord_quickmap() + scale_fill_manual(values = c(&quot;Rep&quot; = &quot;red&quot;, &quot;Dem&quot; = &quot;blue&quot;)) + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) For plotting the purple states, I use since the color column contains the RGB values to use in the plot: ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = color)) + coord_quickmap() + scale_fill_identity() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) However, plotting purple states is not a good data visualization. Even though the colors are a proportional mixture of red and blue, human visual perception doesn’t work that way. The proportion of the democratic vote is best thought of a diverging scale with 0.5 is midpoint. And since the Democratic Party is associated with the color blue and the Republican Party is associated with the color red. The Color Brewer palette RdBu is an example: ggplot(states) + geom_polygon(aes(group = group, x = long, y = lat, fill = Dem)) + scale_fill_distiller(&quot;% Obama&quot;, direction = 1, limits = c(0, 1), type = &quot;div&quot;, palette = &quot;RdBu&quot;) + coord_quickmap() + theme_void() + labs(x = &quot;&quot;, y = &quot;&quot;) 5.3.4 Expansion of Walmart We don’t need to do the direct mapping since data(&quot;walmart&quot;, package = &quot;qss&quot;) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = mutate(walmart, size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)), alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + guides(color = guide_legend(override.aes = list(alpha = 1))) + theme_void() We don’t need to worry about colors since ggplot handles that. I use guides to so that the colors or not transparent in the legend (see R for Data Science chapterGraphics for communication). To make a plot showing all Walmart stores opened up through that year, I write a function, that takes the year and dataset as parameters. Since I am calling the function for its side effect (printing the plot) rather than the value it returns, I use the walk function rather than map. See R for Data Science, Chapter 21.8: Walk for more information. map_walmart &lt;- function(year, .data) { .data &lt;- filter(.data, opendate &lt; make_date(year, 1, 1)) %&gt;% mutate(size = if_else(type == &quot;DistributionCenter&quot;, 2, 1)) ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, size = size), data = .data, alpha = 1 / 3) + coord_quickmap() + scale_size_identity() + guides(color = guide_legend(override.aes = list(alpha = 1))) + theme_void() + ggtitle(year) } years &lt;- c(1975, 1985, 1995, 2005) walk(years, ~ print(map_walmart(.x, walmart))) 5.3.5 Animation in R For easy animation with ggplot2, use the gganimate package. Note that the gganimate package is not on CRAN, so you have to install it with the devtools package: install.packages(&quot;cowplot&quot;) devtools::install_github(&quot;dgrtwo/animate&quot;) library(&quot;gganimate&quot;) An animation is a series of frames. The gganimate package works by adding a frame aesthetic to ggplots, and function will animate the plot. I use frame = year(opendate) to have the animation use each year as a frame, and cumulative = TRUE so that the previous years are shown. walmart_animated &lt;- ggplot() + borders(database = &quot;state&quot;) + geom_point(aes(x = long, y = lat, colour = type, fill = type, frame = year(opendate), cumulative = TRUE), data = walmart) + coord_quickmap() + theme_void() gganimate(walmart_animated) 6 Probability Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;stringr&quot;) library(&quot;broom&quot;) 6.1 Probability 6.1.1 Frequentist vs. Bayesian 6.1.2 Definition and Axioms 6.1.3 Permutations birthday &lt;- function(k) { logdenom &lt;- k * log(365) + lfactorial(365 - k) lognumer &lt;- lfactorial(365) pr &lt;- 1 - exp(lognumer - logdenom) pr } bday &lt;- tibble(k = 1:50, pr = birthday(k)) ggplot(bday, aes(x = k, y = pr)) + geom_hline(yintercept = 0.5, colour = &quot;white&quot;, size = 2) + geom_line() + geom_point() + scale_y_continuous(str_c(&quot;Probability that at least two&quot;, &quot;people have the same birthday&quot;, sep = &quot;\\n&quot;), limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + labs(x = &quot;Number of people&quot;) Note: The logarithm is used for numerical stability. Basically, “floating-point” numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues. See “Falling Into the Floating Point Trap” in The R Inferno for a summary of floating point numbers. See these John Fox posts 1 2 for an example of numerical stability gone wrong. Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/. 6.1.4 Sampling without replacement Instead of using a for loop, we could do the simulations using a functional as described in R for Data Science chapter “Iterations”. Define the function sim_bdays which randomly samples k birthdays, and returns TRUE if there are any duplicate birthdays, and FALSE if there are none. sim_bdays &lt;- function(k) { days &lt;- sample(1:365, k, replace = TRUE) length(unique(days)) &lt; k } We can test the code for k = 10 birthdays. sim_bdays(10) #&gt; [1] FALSE Since the function is randomly sampling birthdays, running it multiple times will produce different answers. One helpful feature of a functional style of writing code vs. a for loop is that the function encapsulates the code and allows you to test that it works for different inputs before repeating it for many inputs. It is more difficult to debug functions that produce random outputs, but some sanity checks are that the function: returns a logical vector of length one (TRUE or FALSE) always returns FALSE when k = 1 since there can never be a duplicates with one person always returns TRUE when k &gt; 365 by the pidgeonhole principle. sim_bdays(1) #&gt; [1] FALSE sim_bdays(366) #&gt; [1] TRUE Set the parameters for 1,000 simulations, and 23 individuals. We use map_lgl since sim_bdays returns a logical value (TRUE, FALSE): sims &lt;- 1000 k &lt;- 23 map_lgl(seq_len(sims), ~ sim_bdays(k)) %&gt;% mean() #&gt; [1] 0.475 An alternative way of running this is using the rerun and using flatten to turn the output to a numeric vector: rerun(sims, sim_bdays(k)) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.477 6.1.5 Combinations The function for \\(84\\choose{6}\\) is: choose(84, 6) #&gt; [1] 4.06e+08 However, due to the the larges values that the binomial coefficient, it is almost always better to use the log of the binomial coefficient, \\(\\log{84\\choose{6}}\\), lchoose(84, 6) #&gt; [1] 19.8 6.2 Conditional Probability 6.2.1 Conditional, Marginal, and Joint Probabilities Load Florida voting data from the qss package: data(FLVoters, package = &quot;qss&quot;) dim(FLVoters) #&gt; [1] 10000 6 glimpse(FLVoters) #&gt; Observations: 10,000 #&gt; Variables: 6 #&gt; $ surname &lt;chr&gt; &quot;PIEDRA&quot;, &quot;LYNCH&quot;, &quot;CHESTER&quot;, &quot;LATHROP&quot;, &quot;HUMMEL&quot;, &quot;CH... #&gt; $ county &lt;int&gt; 115, 115, 115, 115, 115, 115, 115, 115, 1, 1, 115, 115... #&gt; $ VTD &lt;int&gt; 66, 13, 103, 80, 8, 55, 84, 48, 41, 39, 26, 45, 11, 48... #&gt; $ age &lt;int&gt; 58, 51, 63, 54, 77, 49, 77, 34, 56, 60, 44, 45, 80, 83... #&gt; $ gender &lt;chr&gt; &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;,... #&gt; $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, NA, &quot;white&quot;, &quot;white&quot;, &quot;white&quot;, &quot;whit... FLVoters &lt;- FLVoters %&gt;% na.omit() dim(FLVoters) #&gt; [1] 9113 6 Note the difference between glimpse() and dim() - what is different in how they handle NA observations? Instead of using prop.base, we calculate the probabilities with a data frame. Calculate the marginal probabilities of each race: margin_race &lt;- FLVoters %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) margin_race #&gt; # A tibble: 6 x 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 175 0.0192 #&gt; 2 black 1194 0.131 #&gt; 3 hispanic 1192 0.131 #&gt; 4 native 29 0.00318 #&gt; 5 other 310 0.0340 #&gt; 6 white 6213 0.682 Calculate the marginal probabilities of each gender: margin_gender &lt;- FLVoters %&gt;% count(gender) %&gt;% mutate(prop = n / sum(n)) margin_gender #&gt; # A tibble: 2 x 3 #&gt; gender n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f 4883 0.536 #&gt; 2 m 4230 0.464 FLVoters %&gt;% filter(gender == &quot;f&quot;) %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 6 x 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 83 0.0170 #&gt; 2 black 678 0.139 #&gt; 3 hispanic 666 0.136 #&gt; 4 native 17 0.00348 #&gt; 5 other 158 0.0324 #&gt; 6 white 3281 0.672 joint_p &lt;- FLVoters %&gt;% count(gender, race) %&gt;% mutate(prop = n / sum(n)) joint_p #&gt; # A tibble: 12 x 4 #&gt; gender race n prop #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f asian 83 0.00911 #&gt; 2 f black 678 0.0744 #&gt; 3 f hispanic 666 0.0731 #&gt; 4 f native 17 0.00187 #&gt; 5 f other 158 0.0173 #&gt; 6 f white 3281 0.360 #&gt; # ... with 6 more rows We can convert the data frame to have gender as columns: joint_p %&gt;% select(-n) %&gt;% spread(gender, prop) #&gt; # A tibble: 6 x 3 #&gt; race f m #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian 0.00911 0.0101 #&gt; 2 black 0.0744 0.0566 #&gt; 3 hispanic 0.0731 0.0577 #&gt; 4 native 0.00187 0.00132 #&gt; 5 other 0.0173 0.0167 #&gt; 6 white 0.360 0.322 Sum over race: joint_p %&gt;% group_by(race) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 6 x 2 #&gt; race prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 asian 0.0192 #&gt; 2 black 0.131 #&gt; 3 hispanic 0.131 #&gt; 4 native 0.00318 #&gt; 5 other 0.0340 #&gt; 6 white 0.682 Sum over gender: joint_p %&gt;% group_by(gender) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 2 x 2 #&gt; gender prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 f 0.536 #&gt; 2 m 0.464 FLVoters &lt;- FLVoters %&gt;% mutate(age_group = cut(age, c(0, 20, 40, 60, Inf), right = TRUE, labels = c(&quot;&lt;= 20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;&gt; 60&quot;))) joint3 &lt;- FLVoters %&gt;% count(race, age_group, gender) %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) joint3 #&gt; # A tibble: 47 x 5 #&gt; race age_group gender n prop #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian &lt;= 20 f 1 0.000110 #&gt; 2 asian &lt;= 20 m 2 0.000219 #&gt; 3 asian 20-40 f 24 0.00263 #&gt; 4 asian 20-40 m 26 0.00285 #&gt; 5 asian 40-60 f 38 0.00417 #&gt; 6 asian 40-60 m 47 0.00516 #&gt; # ... with 41 more rows Marginal probabilities by age groups margin_age &lt;- FLVoters %&gt;% count(age_group) %&gt;% mutate(prop = n / sum(n)) margin_age #&gt; # A tibble: 4 x 3 #&gt; age_group n prop #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 161 0.0177 #&gt; 2 20-40 2469 0.271 #&gt; 3 40-60 3285 0.360 #&gt; 4 &gt; 60 3198 0.351 Calculate the probabilities that each group is in a given age group, and show \\(P(\\text{black} \\land \\text{female} \\land \\text{age} &gt; 60)\\): (Note: the symbol \\(\\land\\) is the logical symbol for ‘and’, implying the joint probability.) left_join(joint3, select(margin_age, age_group, margin_age = prop), by = &quot;age_group&quot;) %&gt;% mutate(prob_age_group = prop / margin_age) %&gt;% filter(race == &quot;black&quot;, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;) %&gt;% select(race, age_group, gender, prob_age_group) #&gt; # A tibble: 1 x 4 #&gt; race age_group gender prob_age_group #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black &gt; 60 f 0.0538 Two-way joint probability table for age group and gender joint2 &lt;- FLVoters %&gt;% count(age_group, gender) %&gt;% ungroup() %&gt;% mutate(prob_age_gender = n / sum(n)) joint2 #&gt; # A tibble: 8 x 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 f 88 0.00966 #&gt; 2 &lt;= 20 m 73 0.00801 #&gt; 3 20-40 f 1304 0.143 #&gt; 4 20-40 m 1165 0.128 #&gt; 5 40-60 f 1730 0.190 #&gt; 6 40-60 m 1555 0.171 #&gt; # ... with 2 more rows The joint probability \\(P(\\text{age} &gt; 60 \\land \\text{female})\\), joint2 %&gt;% filter(age_group == &quot;&gt; 60&quot;, gender == &quot;f&quot;) #&gt; # A tibble: 1 x 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f 1761 0.193 The conditional probabilities \\(P(\\text{race } | \\text{ gender, age})\\), condprob_race &lt;- left_join(joint3, select(joint2, -n), by = c(&quot;age_group&quot;, &quot;gender&quot;)) %&gt;% mutate(prob_race = prop / prob_age_gender) %&gt;% arrange(age_group, gender) %&gt;% select(age_group, gender, race, prob_race) Each row is the \\(P(\\text{race } | \\text{ age group} \\land \\text{gender})\\), so \\(P(\\text{black } | \\text{ female} \\land \\text{age} &gt; 60)\\), filter(condprob_race, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;, race == &quot;black&quot;) #&gt; # A tibble: 1 x 4 #&gt; age_group gender race prob_race #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f black 0.0977 6.2.2 Independence Create a table with the products of margins of race and age. Using the function crossing to create a tibble with all combinations of race and gender and the independent prob. race_gender_indep &lt;- crossing(select(margin_race, race, prob_race = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(prob_indep = prob_race * prob_gender) %&gt;% left_join(select(joint_p, gender, race, prob = prop), by = c(&quot;gender&quot;, &quot;race&quot;)) %&gt;% select(race, gender, everything()) race_gender_indep #&gt; # A tibble: 12 x 6 #&gt; race gender prob_race prob_gender prob_indep prob #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian f 0.0192 0.536 0.0103 0.00911 #&gt; 2 asian m 0.0192 0.464 0.00891 0.0101 #&gt; 3 black f 0.131 0.536 0.0702 0.0744 #&gt; 4 black m 0.131 0.464 0.0608 0.0566 #&gt; 5 hispanic f 0.131 0.536 0.0701 0.0731 #&gt; 6 hispanic m 0.131 0.464 0.0607 0.0577 #&gt; # ... with 6 more rows ggplot(race_gender_indep, aes(x = prob_indep, y = prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(. ~ gender) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(x = expression(P(&quot;race&quot;) * P(&quot;gender&quot;)), y = expression(P(&quot;race and gender&quot;))) While the original code only calculates joint-independence value for values of age &gt; 60, and female, this calculates the joint probabilities for all combinations of the three variables, and facets by age and gender. joint_indep &lt;- crossing(select(margin_race, race, prob_race = prop), select(margin_age, age_group, prob_age = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(indep_prob = prob_race * prob_age * prob_gender) %&gt;% left_join(select(joint3, race, age_group, gender, prob = prop), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% replace_na(list(prob = 0)) ggplot(joint_indep, aes(x = prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age and gender)&quot;, y = &quot;P(race) * P(age) * P(gender)&quot;, title = &quot;Joint Independence&quot;) While code in QSS only calculates the conditional independence given female, the following code calculates conditional independence for all values of gender: cond_gender &lt;- left_join(select(joint3, race, age_group, gender, joint_prob = prop), select(margin_gender, gender, prob_gender = prop), by = c(&quot;gender&quot;)) %&gt;% mutate(cond_prob = joint_prob / prob_gender) Calculate the conditional distribution \\(\\Pr(\\text{race} | \\text{gender})\\): prob_race_gender &lt;- left_join(select(joint_p, race, gender, prob_race_gender = prop), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_race = prob_race_gender / prob_gender) Calculate the conditional distribution \\(\\Pr(\\text{age} | \\text{gender})\\): prob_age_gender &lt;- left_join(select(joint2, age_group, gender, prob_age_gender), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_age = prob_age_gender / prob_gender) # indep prob of race and age indep_cond_gender &lt;- full_join(select(prob_race_gender, race, gender, prob_race), select(prob_age_gender, age_group, gender, prob_age), by = &quot;gender&quot;) %&gt;% mutate(indep_prob = prob_race * prob_age) inner_join(select(indep_cond_gender, race, age_group, gender, indep_prob), select(cond_gender, race, age_group, gender, cond_prob), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% ggplot(aes(x = cond_prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age | gender)&quot;, y = &quot;P(race | gender) * P(age | gender)&quot;, title = &quot;Marginal independence&quot;) Monty-hall problem The for loop approach in QSS is valid code, but here we provide a more functional approach to solving the problem. We will define a function to choose a door, repeat the function multiple times while storing the results in a data frame, and then summarize that data frame. First, create a function for a single iteration. This returns a single logical value: choose_door &lt;- function(.iter) { # what&#39;s behind each door door: # Why is it okay that this is fixed? doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) # User randomly chooses a door first &lt;- sample(1:3, 1) # randomly choose the door that Monty Hall reveals remain &lt;- doors[-first] monty &lt;- sample( (1:2)[remain == &quot;goat&quot;], size = 1) # did the contestant win? tibble(.iter = .iter, result = remain[-monty] == &quot;car&quot;) } Now use map_df to run choose_door multiple times, and then summarize the results: sims &lt;- 1000 map_df(seq_len(sims), choose_door) %&gt;% summarise(win_pct = mean(result)) #&gt; # A tibble: 1 x 1 #&gt; win_pct #&gt; &lt;dbl&gt; #&gt; 1 0.670 6.2.3 Bayes’ Rule 6.2.4 Predicting Race Using Surname and Residence Location Start with the Census names files: data(&quot;cnames&quot;, package = &quot;qss&quot;) glimpse(cnames) #&gt; Observations: 151,671 #&gt; Variables: 7 #&gt; $ surname &lt;chr&gt; &quot;SMITH&quot;, &quot;JOHNSON&quot;, &quot;WILLIAMS&quot;, &quot;BROWN&quot;, &quot;JONES&quot;, ... #&gt; $ count &lt;int&gt; 2376206, 1857160, 1534042, 1380145, 1362755, 11278... #&gt; $ pctwhite &lt;dbl&gt; 73.34, 61.55, 48.52, 60.72, 57.69, 85.80, 64.73, 6... #&gt; $ pctblack &lt;dbl&gt; 22.22, 33.80, 46.72, 34.54, 37.73, 10.41, 30.77, 0... #&gt; $ pctapi &lt;dbl&gt; 0.40, 0.42, 0.37, 0.41, 0.35, 0.42, 0.40, 1.43, 0.... #&gt; $ pcthispanic &lt;dbl&gt; 1.56, 1.50, 1.60, 1.64, 1.44, 1.43, 1.58, 90.82, 9... #&gt; $ pctothers &lt;dbl&gt; 2.48, 2.73, 2.79, 2.69, 2.79, 1.94, 2.52, 1.09, 0.... For each surname, cnames contains variables with the probability that it belongs to an individual of a given race (pctwhite, pctblack, …). We want to find the most-likely race for a given surname, by finding the race with the maximum proportion. Instead of dealing with multiple variables, it is easier to use max on a single variable, so we will rearrange the data to in order to use a grouped summarize, and then merge the new variable back to the original data sets. We will also remove the ‘pct’ prefix from each of the variable names. Calculate the most likely race for each name: most_likely_race &lt;- cnames %&gt;% select(-count) %&gt;% gather(race_pred, pct, -surname) %&gt;% # remove pct_ prefix from variable names mutate(race_pred = str_replace(race_pred, &quot;^pct&quot;, &quot;&quot;)) %&gt;% # # group by surname group_by(surname) %&gt;% # select obs with the largest percentage filter(row_number(desc(pct)) == 1L) %&gt;% # Ungroup to avoid errors later ungroup %&gt;% # # don&#39;t need pct anymore select(-pct) %&gt;% mutate(race_pred = recode(race_pred, asian = &quot;api&quot;, other = &quot;others&quot;)) Merge the data frame with the most likely race for each surname to the the original cnames data frame: cnames &lt;- cnames %&gt;% left_join(most_likely_race, by = &quot;surname&quot;) Instead of using match, use inner_join to merge the surnames to FLVoters: FLVoters &lt;- FLVoters %&gt;% inner_join(cnames, by = &quot;surname&quot;) dim(FLVoters) #&gt; [1] 8022 14 FLVoters also includes a “native” category that the surname dataset does not. FLVoters &lt;- FLVoters %&gt;% mutate(race2 = fct_recode(race, other = &quot;native&quot;)) Check that the levels of race and race_pred are the same: FLVoters %&gt;% count(race2) #&gt; # A tibble: 5 x 2 #&gt; race2 n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 asian 140 #&gt; 2 black 1078 #&gt; 3 hispanic 1023 #&gt; 4 other 277 #&gt; 5 white 5504 FLVoters %&gt;% count(race_pred) #&gt; # A tibble: 5 x 2 #&gt; race_pred n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 api 120 #&gt; 2 black 258 #&gt; 3 hispanic 1121 #&gt; 4 others 7 #&gt; 5 white 6516 Now we can calculate True positive rate for all races FLVoters %&gt;% group_by(race2) %&gt;% summarise(tp = mean(race2 == race_pred)) %&gt;% arrange(desc(tp)) #&gt; # A tibble: 5 x 2 #&gt; race2 tp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 white 0.950 #&gt; 2 hispanic 0.847 #&gt; 3 black 0.160 #&gt; 4 asian 0 #&gt; 5 other 0 and the False discovery rate for all races, FLVoters %&gt;% group_by(race_pred) %&gt;% summarise(fp = mean(race2 != race_pred)) %&gt;% arrange(desc(fp)) #&gt; # A tibble: 5 x 2 #&gt; race_pred fp #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 api 1.00 #&gt; 2 others 1.00 #&gt; 3 black 0.329 #&gt; 4 hispanic 0.227 #&gt; 5 white 0.197 Now add residence data using the FLCensus data included in the data(&quot;FLCensus&quot;, package = &quot;qss&quot;) \\(P(\\text{race})\\) in Florida: race.prop &lt;- FLCensus %&gt;% select(total.pop, white, black, api, hispanic, others) %&gt;% gather(race, pct, -total.pop) %&gt;% group_by(race) %&gt;% summarise(mean = weighted.mean(pct, weights = total.pop)) %&gt;% arrange(desc(mean)) race.prop #&gt; # A tibble: 5 x 2 #&gt; race mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 white 0.605 #&gt; 2 hispanic 0.213 #&gt; 3 black 0.139 #&gt; 4 api 0.0219 #&gt; 5 others 0.0214 6.2.5 Predicting Election Outcomes with Uncertainty Load the pres08 data from the qss package. data(&quot;pres08&quot;, package = &quot;qss&quot;) Add a column p which contains Obama’s vote share of the major parties: pres08 &lt;- pres08 %&gt;% mutate(p = Obama / (Obama + McCain)) Write a function to simulate the elections. df is the data frame (pres08) with the state, EV, and p columns. n_draws is the size of the binomial distribution to draw from. .id is the simulation number. sim_election &lt;- function(.id, df, n_draws = 1000) { # For each state randomly sample mutate(df, draws = rbinom(n(), n_draws, p)) %&gt;% filter(draws &gt; (n_draws / 2)) %&gt;% summarise(EV = sum(EV), .id = .id) } Now simulate the election 10,000 times: sims &lt;- 10000 sim_results &lt;- map_df(seq_len(sims), ~ sim_election(.x, pres08, n_draws = 1000)) In the 2008 election, Obama received 364 electoral votes ELECTION_EV &lt;- 364 And plot them, ggplot(sim_results, aes(x = EV, y = ..density..)) + geom_histogram(binwidth = 10, fill = &quot;gray30&quot;) + geom_vline(xintercept = ELECTION_EV, colour = &quot;black&quot;, size = 2) + labs(x = &quot;Electoral Votes&quot;, y = &quot;density&quot;) Simulation mean, variance, and standard deviations: sim_results %&gt;% select(EV) %&gt;% summarise_all(funs(mean, var, sd)) #&gt; mean var sd #&gt; 1 352 271 16.5 Theoretical probabilities from a binomial distribution: # we cannot use n, because mutate will look for n() first. n_draws &lt;- 1000 pres08 %&gt;% mutate(pb = pbinom(n_draws / 2, size = n_draws, prob = p, lower.tail = FALSE)) %&gt;% summarise(mean = sum(pb * EV), V = sum(pb * (1 - pb) * EV ^ 2), sd = sqrt(V)) #&gt; mean V sd #&gt; 1 352 269 16.4 6.3 Random Variables and Probability Distributions 6.3.1 Bernoulli and Uniform Distributions Uniform distribution functions: dunif(0.5, min = 0, max = 1) #&gt; [1] 1 punif(1, min = -2, max = 2) #&gt; [1] 0.75 Sample from a uniform distribution, and convert to a probability: sims &lt;- 1000 p &lt;- 0.5 # Sample of size sims from Bernoulli distribution with probability p y &lt;- as.integer(runif(sims, min = 0, max = 1) &lt;= p) # mean probability mean(y) #&gt; [1] 0.501 6.3.2 Binomial distribution dbinom(2, size = 3, prob = 0.5) #&gt; [1] 0.375 pbinom(1, 3, 0.5) #&gt; [1] 0.5 voters &lt;- c(1000, 10000, 100000) dbinom(voters / 2, size = voters, prob = 0.5) #&gt; [1] 0.02523 0.00798 0.00252 6.3.3 Normal distribution pnorm(1) - pnorm(-1) #&gt; [1] 0.683 pnorm(2) - pnorm(-2) #&gt; [1] 0.954 Write a function to calculate the area of a normal distribution \\(\\pm \\sigma\\) normal_pm_sigma &lt;- function(x, mu = 0, sd = 1) { (pnorm(mu + sd * x, mean = mu, sd = sd) - pnorm(mu - sd * x, mean = mu, sd = sd)) } normal_pm_sigma(1) #&gt; [1] 0.683 normal_pm_sigma(2) #&gt; [1] 0.954 normal_pm_sigma(1, mu = 5, sd = 2) #&gt; [1] 0.683 normal_pm_sigma(2, mu = 5, sd = 2) #&gt; [1] 0.954 data(&quot;pres08&quot;, package = &quot;qss&quot;) data(&quot;pres12&quot;, package = &quot;qss&quot;) To join both data frames pres &lt;- full_join(select(pres08, state, Obama_2008 = Obama, McCain_2008 = McCain, EV_2008 = EV), select(pres12, state, Obama_2012 = Obama, Romney_2012 = Romney, EV_2012 = EV), by = &quot;state&quot;) %&gt;% mutate(Obama_2008_z = as.numeric(scale(Obama_2008)), Obama_2012_z = as.numeric(scale(Obama_2012))) fit1 &lt;- lm(Obama_2012_z ~ -1 + Obama_2008_z, data = pres) Plot the residuals and compare them to a normal distribution: err &lt;- tibble(err = resid(fit1)) %&gt;% # z-score of residuals mutate(err_std = err / sd(err)) ggplot(err, aes(x = err_std)) + geom_histogram(mapping = aes(y = ..density..), binwidth = 1, boundary = 0) + stat_function(geom = &quot;line&quot;, fun = dnorm) + scale_x_continuous(&quot;Standardized residuals&quot;, breaks = -3:3, limits = c(-3, 3)) ggplot(err, aes(sample = err_std)) + geom_abline(intercept = 0, slope = 1, color = &quot;white&quot;, size = 2) + geom_qq() + coord_fixed(ratio = 1) + scale_y_continuous(&quot;Sample quantiles&quot;, limits = c(-3, 3)) + scale_x_continuous(&quot;Theoretical quantiles&quot;, limits = c(-3, 3)) Alternatively, you can use the augment function from broom which returns the residuals for each observation in the .resid column. augment(fit1) %&gt;% mutate(.resid_z = .resid / sd(.resid)) %&gt;% ggplot(aes(x = .resid_z)) + geom_histogram(mapping = aes(y = ..density..), binwidth = 1, boundary = 0) + stat_function(geom = &quot;line&quot;, fun = dnorm) + scale_x_continuous(&quot;Standardized residuals&quot;, breaks = -3:3, limits = c(-3, 3)) Obama’s vote shares in 2008 and 2012. Standard deviation of errors: err_sd &lt;- sd(resid(fit1)) Probability of having a larger vote in California in 2012 than in 2008? CA_2008 &lt;- filter(pres, state == &quot;CA&quot;)$Obama_2008_z The predicted value of 2012 vote share can be calculated manually with \\(\\hat{beta} \\times x\\), CA_mean_2012 &lt;- CA_2008 * coef(fit1)[&quot;Obama_2008_z&quot;] CA_mean_2012 #&gt; Obama_2008_z #&gt; 0.858 or calculated using the predict function with the newdata argument providing any specific values to calculate? predict(fit1, newdata = tibble(Obama_2008_z = CA_2008)) #&gt; 1 #&gt; 0.858 Now calculate pnorm(CA_2008, mean = CA_mean_2012, sd = err_sd, lower.tail = FALSE) #&gt; [1] 0.468 We can generalize the previous code to calculate the probability that Obama would exceed his 2008 vote to all states: pres_gt_2008 &lt;- augment(fit1) %&gt;% mutate(p_greater = pnorm(Obama_2008_z, mean = .fitted, sd = err_sd, lower.tail = FALSE)) Plotting these results, we can observe regression to the mean. States with larger (smaller) 2008 vote shares had a lower (higher) probability that the 2012 vote share will exceed them. ggplot(pres_gt_2008, aes(x = Obama_2008_z, y = p_greater)) + modelr::geom_ref_line(h = 0.5) + modelr::geom_ref_line(v = 0) + geom_point() + labs(x = &quot;Standardized vote share of Obama (2008)&quot;, y = &quot;Pr. 2012 vote share greater than 2008&quot;) 6.3.4 Expectation and Variance Theoretical and actual sample variable of a set of Bernoulli draws: p &lt;- 0.5 # theretical variance p * (1 - p) #&gt; [1] 0.25 # a sample y &lt;- sample(c(0, 1), size = 1000, replace = TRUE, prob = c(p, 1 - p)) # the sample variance of that sample var(y) #&gt; [1] 0.25 6.3.5 Predicting Election Outcomes with Uncertainty Load 2008 Presidential Election data: data(&quot;pres08&quot;, package = &quot;qss&quot;) pres08 &lt;- mutate(pres08, p = Obama / (Obama + McCain)) sim_election &lt;- function(x, n = 1000) { n_states &lt;- nrow(x) # samples the number of votes for Obama mutate(x, draws = rbinom(n_states, size = !!n, prob = p), obama_EV = EV * (draws &gt; (!!n / 2))) %&gt;% pluck(&quot;obama_EV&quot;) %&gt;% sum() } This function returns the electoral votes for Obama for a single simulation: sim_election(pres08) #&gt; [1] 349 Run this simulation sims times, saving the electoral votes of Obama in each simulation: sims &lt;- 10000 Obama_EV_sims &lt;- map_dbl(seq_len(sims), ~ sim_election(pres08)) Obama’s actual electoral value OBAMA_EV &lt;- 364 library(&quot;glue&quot;) ggplot(tibble(Obama_EV = Obama_EV_sims), aes(x = Obama_EV, y = ..density..)) + geom_histogram(binwidth = 10, boundary = 0, fill = &quot;gray60&quot;) + geom_vline(xintercept = OBAMA_EV, colour = &quot;black&quot;, size = 2) + annotate(&quot;text&quot;, x = OBAMA_EV + 2, y = 0.02, label = glue(&quot;Obama&#39;s 2008 EV = {OBAMA_EV}&quot;), hjust = 0) + scale_x_continuous(&quot;Obama&#39;s Electoral College Votes&quot;, breaks = seq(300, 400, by = 20)) + scale_y_continuous(&quot;Density&quot;) + labs(title = &quot;Prediction of election outcomes&quot;) Summarize the simulations: summary(Obama_EV_sims) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 278 340 353 352 364 401 Compare theoretical and simulation means: # simulation EV mean(Obama_EV_sims) #&gt; [1] 352 # theoretical n &lt;- 1000 pres08 %&gt;% mutate(Obama_EV = EV * pbinom(n / 2, size = n, prob = p, lower.tail = FALSE)) %&gt;% summarise(Obama_EV = sum(Obama_EV)) #&gt; Obama_EV #&gt; 1 352 Compare theoretical and simulation variances: # simulation variance var(Obama_EV_sims) #&gt; [1] 273 # theoretical variance Obama_EV_var &lt;- pres08 %&gt;% mutate(pb = pbinom(n / 2, size = n, prob = p, lower.tail = FALSE), EV_var = pb * (1 - pb) * EV ^ 2) %&gt;% summarise(EV_var = sum(EV_var)) %&gt;% pluck(&quot;EV_var&quot;) Obama_EV_var #&gt; [1] 269 and standard deviations # sim sd(Obama_EV_sims) #&gt; [1] 16.5 # theoretical sqrt(Obama_EV_var) #&gt; [1] 16.4 6.4 Large Sample Theorems 6.4.1 Law of Large Numbers Put the simulation number, x and mean in a tibble: sims &lt;- 1000 p &lt;- 0.2 size &lt;- 10 lln_binom &lt;- tibble( n = seq_len(sims), x = rbinom(sims, prob = p, size = size), mean = cumsum(x) / n, distrib = str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;)) lln_unif &lt;- tibble(n = seq_len(sims), x = runif(sims), mean = cumsum(x) / n, distrib = str_c(&quot;Uniform(0, 1)&quot;)) true_means &lt;- tribble(~distrib, ~mean, &quot;Uniform(0, 1)&quot;, 0.5, str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;), size * p) ggplot() + geom_hline(aes(yintercept = mean), data = true_means, colour = &quot;white&quot;, size = 2) + geom_line(aes(x = n, y = mean), data = bind_rows(lln_binom, lln_unif)) + facet_grid(distrib ~ ., scales = &quot;free_y&quot;) + labs(x = &quot;Sample Size&quot;, y = &quot;Sample Mean&quot;) 6.4.2 Central Limit Theorem The population mean of the binomial distribution is \\(\\mu = p n\\) and the variance is \\(\\sigma^2 = p (1 - p) n\\). sims &lt;- 1000 n_samp &lt;- 1000 Write functions to calculate the mean of a binomial distribution with size, size, and probability, p, binom_mean &lt;- function(size, p) { size * p } variance of a binomial distribution, binom_var &lt;- function(size, p) { size * p * (1 - p) } Write a function that takes n_samp samples from a binomial distribution with size, size, and probability of success, p, and returns a data frame with the z-score of the sample distribution: sim_binom_clt &lt;- function(n_samp, size, p) { x &lt;- rbinom(n_samp, prob = p, size = size) z &lt;- (mean(x) - binom_mean(size, p)) / sqrt(binom_var(size, p) / n_samp) tibble(distrib = str_c(&quot;Binomial(&quot;, p, &quot;, &quot;, size, &quot;)&quot;), z = z) } For the uniform distribution, we need a function to calculate the mean of a uniform distribution, unif_mean &lt;- function(min, max) { 0.5 * (min + max) } variance of a uniform distribution, unif_var &lt;- function(min, max) { (1 / 12) * (max - min) ^ 2 } and a function to perform the CLT simulation, sim_unif_clt &lt;- function(n_samp, min = 0, max = 1) { x &lt;- runif(n_samp, min = min, max = max) z &lt;- (mean(x) - unif_mean(min, max)) / sqrt(unif_var(min, max) / n_samp) tibble(distrib = str_c(&quot;Uniform(&quot;, min, &quot;, &quot;, max, &quot;)&quot;), z = z) } Since we will calculate this for n_samp = 1000 and n_samp, we might as well write a function for it. clt_plot &lt;- function(n_samp) { bind_rows(map_df(seq_len(sims), ~ sim_binom_clt(n_samp, size, p)), map_df(seq_len(sims), ~ sim_unif_clt(n_samp))) %&gt;% ggplot(aes(x = z)) + geom_density() + geom_rug() + stat_function(fun = dnorm, colour = &quot;red&quot;) + facet_grid(distrib ~ .) + ggtitle(str_c(&quot;Sample size = &quot;, n_samp)) } clt_plot(1000) clt_plot(100) 7 Uncertainty Prerequisites We will use these package in this chapter: library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;lubridate&quot;) library(&quot;stringr&quot;) library(&quot;modelr&quot;) library(&quot;broom&quot;) library(&quot;purrr&quot;) 7.1 Estimation 7.1.1 Unbiasedness and Consistency In these simulations, we draw a sample of size n from normal distributions with means mu0 and mu1 and standard deviations sd0 and sd1 respectively, n &lt;- 100 mu0 &lt;- 0 sd0 &lt;- 1 mu1 &lt;- 1 sd1 &lt;- 1 smpl &lt;- tibble(id = seq_len(n), # Y if not treated Y0 = rnorm(n, mean = mu0, sd = sd0), # Y if treated Y1 = rnorm(n, mean = mu1, sd = sd1), # individual treatment effects tau = Y1 - Y0) The SATE is: SATE &lt;- mean(smpl[[&quot;tau&quot;]]) SATE #&gt; [1] 0.952 Simulations of RCTs. Write a function that takes the sample as an input smpl, then randomly assigns the treatment to each individual: sim_treat &lt;- function(smpl) { n &lt;- nrow(smpl) SATE &lt;- mean(smpl[[&quot;tau&quot;]]) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(Y_obs = mean(Y_obs)) %&gt;% spread(treat, Y_obs) %&gt;% rename(Y1_mean = `1`, Y0_mean = `0`) %&gt;% mutate(diff_mean = Y1_mean - Y0_mean, est_error = diff_mean - SATE) } This returns a data frame with columns: Y0_mean (mean \\(Y\\) for observations not receiving the treatment), Y1_mean (mean \\(Y\\) for observations receiving the treatment), diff (difference in mean values between the two groups), and est_error (difference between the estimated difference and the known SATE): sim_treat(smpl) #&gt; # A tibble: 1 x 4 #&gt; Y0_mean Y1_mean diff_mean est_error #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.0570 0.875 0.932 -0.0207 Rerun this function sims times: sims &lt;- 5000 sate_sims &lt;- map_df(seq_len(sims), ~ sim_treat(smpl)) summary(sate_sims[[&quot;est_error&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.484 -0.087 -0.002 -0.001 0.085 0.414 Simulate the PATE: PATE &lt;- mu1 - mu0 sim_pate &lt;- function(n, mu0, mu1, sd0, sd1) { smpl &lt;- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0), Y1 = rnorm(n, mean = mu1, sd = sd1), tau = Y1 - Y0) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(Y_obs = mean(Y_obs)) %&gt;% spread(treat, Y_obs) %&gt;% rename(Y1_mean = `1`, Y0_mean = `0`) %&gt;% mutate(diff_mean = Y1_mean - Y0_mean, est_error = diff_mean - PATE) } Example of one simulation sim_pate(n, mu0, mu1, sd0, sd1) #&gt; # A tibble: 1 x 4 #&gt; Y0_mean Y1_mean diff_mean est_error #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.109 0.925 1.03 0.0338 pate_sims &lt;- map_df(seq_len(sims), ~ sim_pate(n, mu0, mu1, sd0, sd1)) summary(pate_sims[[&quot;est_error&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.677 -0.135 -0.004 -0.003 0.129 0.853 7.1.2 Standard Error Plot of the sampling distribution of the difference-in-means estimator: ggplot(pate_sims, aes(x = diff_mean, y = ..density..)) + geom_histogram(binwidth = 0.01, boundary = 1) + geom_vline(xintercept = PATE, colour = &quot;white&quot;, size = 2) + ggtitle(&quot;Sampling distribution&quot;) + labs(x = &quot;Difference-in-means estimator&quot;) sd(pate_sims[[&quot;diff_mean&quot;]]) #&gt; [1] 0.196 Simulate PATE with a standard error: sim_pate_se &lt;- function(n, mu0, mu1, sd0, sd1) { # PATE - difference in means PATE &lt;- mu1 - mu0 # sample smpl &lt;- tibble(Y0 = rnorm(n, mean = mu0, sd = sd0), Y1 = rnorm(n, mean = mu1, sd = sd1), tau = Y1 - Y0) # indexes of obs receiving treatment idx &lt;- sample(seq_len(n), floor(nrow(smpl) / 2), replace = FALSE) # treat variable are those receiving treatment, else 0 smpl[[&quot;treat&quot;]] &lt;- as.integer(seq_len(nrow(smpl)) %in% idx) # sample smpl %&gt;% mutate(Y_obs = if_else(treat == 1L, Y1, Y0)) %&gt;% group_by(treat) %&gt;% summarise(mean = mean(Y_obs), var = var(Y_obs), nobs = n()) %&gt;% summarise(diff_mean = diff(mean), se = sqrt(sum(var / nobs)), est_error = diff_mean - PATE) } Run a single simulation: sim_pate_se(n, mu0, mu1, sd0, sd1) #&gt; # A tibble: 1 x 3 #&gt; diff_mean se est_error #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1.05 0.190 0.0539 Run sims simulations: sims &lt;- 5000 pate_sims_se &lt;- map_df(seq_len(sims), ~ sim_pate_se(n, mu0, mu1, sd0, sd1)) Standard deviation of difference-in-means sd(pate_sims_se[[&quot;diff_mean&quot;]]) #&gt; [1] 0.199 Mean of standard errors, mean(pate_sims_se[[&quot;se&quot;]]) #&gt; [1] 0.2 7.1.3 Confidence Intervals Calculate a \\(p\\%\\) confidence interval for the binomial distribution: # Sample size n &lt;- 1000 # point estimate x_bar &lt;- 0.6 # standard error se &lt;- sqrt(x_bar * (1 - x_bar) / n) # Desired Confidence levels levels &lt;- c(0.99, 0.95, 0.90) tibble(level = levels) %&gt;% mutate( ci_lower = x_bar - qnorm(1 - (1 - level) / 2) * se, ci_upper = x_bar + qnorm(1 - (1 - level) / 2) * se ) #&gt; # A tibble: 3 x 3 #&gt; level ci_lower ci_upper #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.990 0.560 0.640 #&gt; 2 0.950 0.570 0.630 #&gt; 3 0.900 0.575 0.625 Calculate the coverage ratio of the 95% confidence interval in the PATE simulations. level &lt;- 0.95 pate_sims_se %&gt;% mutate(ci_lower = diff_mean - qnorm(1 - (1 - level) / 2) * se, ci_upper = diff_mean + qnorm(1 - (1 - level) / 2) * se, includes_pate = PATE &gt; ci_lower &amp; PATE &lt; ci_upper) %&gt;% summarise(coverage = mean(includes_pate)) #&gt; # A tibble: 1 x 1 #&gt; coverage #&gt; &lt;dbl&gt; #&gt; 1 0.948 To do this for multiple levels encapsulate the above code in a function with arguments .data (the data frame) and the confidence level, level: pate_sims_coverage &lt;- function(.data, level = 0.95) { mutate(.data, ci_lower = diff_mean - qnorm(1 - (1 - level) / 2) * se, ci_upper = diff_mean + qnorm(1 - (1 - level) / 2) * se, includes_pate = PATE &gt; ci_lower &amp; PATE &lt; ci_upper) %&gt;% summarise(coverage = mean(includes_pate)) } pate_sims_coverage(pate_sims_se, 0.95) #&gt; # A tibble: 1 x 1 #&gt; coverage #&gt; &lt;dbl&gt; #&gt; 1 0.948 pate_sims_coverage(pate_sims_se, 0.99) #&gt; # A tibble: 1 x 1 #&gt; coverage #&gt; &lt;dbl&gt; #&gt; 1 0.989 pate_sims_coverage(pate_sims_se, 0.90) #&gt; # A tibble: 1 x 1 #&gt; coverage #&gt; &lt;dbl&gt; #&gt; 1 0.898 p &lt;- 0.6 n &lt;- 10 alpha &lt;- 0.05 sims &lt;- 5000 Define a function that samples from a Bernoulli distribution, calculates its standard error, and returns a logical value as to whether it contains the true value: binom_ci_contains &lt;- function(n, p, alpha = 0.05) { x &lt;- rbinom(n, size = 1, prob = p) x_bar &lt;- mean(x) se &lt;- sqrt(x_bar * (1 - x_bar) / n) ci_lower &lt;- x_bar - qnorm(1 - alpha / 2) * se ci_upper &lt;- x_bar + qnorm(1 - alpha / 2) * se (ci_lower &lt;= p) &amp; (p &lt;= ci_upper) } We can run this once for given sample size: n &lt;- 10 binom_ci_contains(n, p) #&gt; [1] FALSE Using map_df we can rerun it sims times and calculate the coverage proportion: mean(map_lgl(seq_len(sims), ~ binom_ci_contains(n, p))) #&gt; [1] 0.905 Encapsulate the above code in a function that calculates the coverage of a confidence interval with size, n, and success probability, p: binom_ci_coverage &lt;- function(n, p, sims) { mean(map_lgl(seq_len(sims), ~ binom_ci_contains(n, p))) } Use binom_ci_coverage to calculate CI coverage for multiple values of the sample size: tibble(n = c(10L, 100L, 1000L)) %&gt;% mutate(coverage = map_dbl(n, binom_ci_coverage, p = !!p, sims = !!sims)) #&gt; # A tibble: 3 x 2 #&gt; n coverage #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 10 0.900 #&gt; 2 100 0.943 #&gt; 3 1000 0.948 7.1.4 Margin of Error and Sample Size Calculation in Polls Write a function to calculate the sample size needed for a given proportion. moe_pop_prop &lt;- function(MoE) { tibble(p = seq(from = 0.01, to = 0.99, by = 0.01), n = 1.96 ^ 2 * p * (1 - p) / MoE ^ 2, MoE = MoE) } moe_pop_prop(0.01) #&gt; # A tibble: 99 x 3 #&gt; p n MoE #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0100 380 0.0100 #&gt; 2 0.0200 753 0.0100 #&gt; 3 0.0300 1118 0.0100 #&gt; 4 0.0400 1475 0.0100 #&gt; 5 0.0500 1825 0.0100 #&gt; 6 0.0600 2167 0.0100 #&gt; # ... with 93 more rows Then use map_df to call this function for different margins of error, and return the entire thing as a data frame with columns: n, p, and MoE. MoE &lt;- c(0.01, 0.03, 0.05) props &lt;- map_df(MoE, moe_pop_prop) Since its a data frame, its easy to plot with ggplot: ggplot(props, aes(x = p, y = n, colour = factor(MoE))) + geom_line() + labs(colour = &quot;margin of error&quot;, x = &quot;population proportion&quot;, y = &quot;sample size&quot;) + theme(legend.position = &quot;bottom&quot;) read_csv already recognizes the date columns, so we don’t need to convert them. The 2008 election was on Nov 11, 2008, so we’ll store that in a variable. ELECTION_DATE &lt;- ymd(20081104) Load the final vote shares, data(&quot;pres08&quot;, package = &quot;qss&quot;) and polling data data(&quot;polls08&quot;, package = &quot;qss&quot;) We need to add an additional column to the polls08 data frame which contains the number of days until the election: polls08 &lt;- polls08 %&gt;% mutate(DaysToElection = as.integer(ELECTION_DATE - middate)) For each state calculate the mean of the latest polls, poll_pred &lt;- polls08 %&gt;% group_by(state) %&gt;% # latest polls in the state filter(DaysToElection == min(DaysToElection)) %&gt;% # take mean of latest polls and convert from 0-100 to 0-1 summarise(Obama = mean(Obama) / 100) # Add confidence itervals # sample size sample_size &lt;- 1000 # confidence level alpha &lt;- 0.05 poll_pred &lt;- poll_pred %&gt;% mutate(se = sqrt(Obama * (1 - Obama) / sample_size), ci_lwr = Obama + qnorm(alpha / 2) * se, ci_upr = Obama + qnorm(1 - alpha / 2) * se) # Add actual outcome poll_pred &lt;- left_join(poll_pred, select(pres08, state, actual = Obama), by = &quot;state&quot;) %&gt;% mutate(actual = actual / 100, covers = (ci_lwr &lt;= actual) &amp; (actual &lt;= ci_upr)) poll_pred #&gt; # A tibble: 51 x 7 #&gt; state Obama se ci_lwr ci_upr actual covers #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 AK 0.390 0.0154 0.360 0.420 0.380 T #&gt; 2 AL 0.360 0.0152 0.330 0.390 0.390 F #&gt; 3 AR 0.440 0.0157 0.409 0.471 0.390 F #&gt; 4 AZ 0.465 0.0158 0.434 0.496 0.450 T #&gt; 5 CA 0.600 0.0155 0.570 0.630 0.610 T #&gt; 6 CO 0.520 0.0158 0.489 0.551 0.540 T #&gt; # ... with 45 more rows In the plot, color the point ranges by whether they include the election day outcome. ggplot(poll_pred, aes(x = actual, y = Obama, ymin = ci_lwr, ymax = ci_upr, colour = covers)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_pointrange() + scale_y_continuous(&quot;Poll prediction&quot;, limits = c(0, 1)) + scale_x_continuous(&quot;Obama&#39;s vote share&quot;, limits = c(0, 1)) + scale_colour_discrete(&quot;CI includes result?&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) Proportion of polls with confidence intervals that include the election outcome? poll_pred %&gt;% summarise(mean(covers)) #&gt; # A tibble: 1 x 1 #&gt; `mean(covers)` #&gt; &lt;dbl&gt; #&gt; 1 0.588 poll_pred &lt;- poll_pred %&gt;% # calc bias mutate(bias = Obama - actual) %&gt;% # bias corrected prediction, se, and CI mutate(Obama_bc = Obama - mean(bias), se_bc = sqrt(Obama_bc * (1 - Obama_bc) / sample_size), ci_lwr_bc = Obama_bc + qnorm(alpha / 2) * se_bc, ci_upr_bc = Obama_bc + qnorm(1 - alpha / 2) * se_bc, covers_bc = (ci_lwr_bc &lt;= actual) &amp; (actual &lt;= ci_upr_bc)) poll_pred %&gt;% summarise(mean(covers_bc)) #&gt; # A tibble: 1 x 1 #&gt; `mean(covers_bc)` #&gt; &lt;dbl&gt; #&gt; 1 0.765 7.1.5 Analysis of Randomized Controlled Trials Load the STAR data from the qss package, data(&quot;STAR&quot;, package = &quot;qss&quot;) Add meaningful labels to the classtype variable: STAR &lt;- STAR %&gt;% mutate(classtype = factor(classtype, labels = c(&quot;small class&quot;, &quot;regular class&quot;, &quot;regular class with aid&quot;))) Summarize scores by classroom type: classtype_means &lt;- STAR %&gt;% group_by(classtype) %&gt;% summarise(g4reading = mean(g4reading, na.rm = TRUE)) Plot the distribution of scores by classroom type: classtypes_used &lt;- c(&quot;small class&quot;, &quot;regular class&quot;) ggplot(filter(STAR, classtype %in% classtypes_used, !is.na(g4reading)), aes(x = g4reading, y = ..density..)) + geom_histogram(binwidth = 20) + geom_vline(data = filter(classtype_means, classtype %in% classtypes_used), mapping = aes(xintercept = g4reading), colour = &quot;white&quot;, size = 2) + facet_grid(classtype ~ .) + labs(x = &quot;Fourth grade reading score&quot;, y = &quot;Density&quot;) alpha &lt;- 0.05 star_estimates &lt;- STAR %&gt;% filter(!is.na(g4reading)) %&gt;% group_by(classtype) %&gt;% summarise(n = n(), est = mean(g4reading), se = sd(g4reading) / sqrt(n)) %&gt;% mutate(lwr = est + qnorm(alpha / 2) * se, upr = est + qnorm(1 - alpha / 2) * se) star_estimates #&gt; # A tibble: 3 x 6 #&gt; classtype n est se lwr upr #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 small class 726 723 1.91 720 727 #&gt; 2 regular class 836 720 1.84 716 723 #&gt; 3 regular class with aid 791 721 1.86 717 724 star_estimates %&gt;% filter(classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;)) %&gt;% # ensure that it is ordered small then regular arrange(desc(classtype)) %&gt;% summarise( se = sqrt(sum(se ^ 2)), est = diff(est) ) %&gt;% mutate(ci_lwr = est + qnorm(alpha / 2) * se, ci_up = est + qnorm(1 - alpha / 2) * se) #&gt; # A tibble: 1 x 4 #&gt; se est ci_lwr ci_up #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2.65 3.50 -1.70 8.70 Use we could use spread and gather: star_ate &lt;- star_estimates %&gt;% filter(classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;)) %&gt;% mutate(classtype = fct_recode(factor(classtype), &quot;small&quot; = &quot;small class&quot;, &quot;regular&quot; = &quot;regular class&quot;)) %&gt;% select(classtype, est, se) %&gt;% gather(stat, value, -classtype) %&gt;% unite(variable, stat, classtype) %&gt;% spread(variable, value) %&gt;% mutate(ate_est = est_small - est_regular, ate_se = sqrt(se_small ^ 2 + se_regular ^ 2), ci_lwr = ate_est + qnorm(alpha / 2) * ate_se, ci_upr = ate_est + qnorm(1 - alpha / 2) * ate_se) star_ate #&gt; # A tibble: 1 x 8 #&gt; est_regular est_small se_regular se_small ate_est ate_se ci_lwr ci_upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 720 723 1.84 1.91 3.50 2.65 -1.70 8.70 7.1.6 Analysis Based on Student’s t-Distribution Use filter to subset. t.test(filter(STAR, classtype == &quot;small class&quot;)$g4reading, filter(STAR, classtype == &quot;regular class&quot;)$g4reading) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: filter(STAR, classtype == &quot;small class&quot;)$g4reading and filter(STAR, classtype == &quot;regular class&quot;)$g4reading #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 723 720 The function t.test can also take a formula as its first parameter. t.test(g4reading ~ classtype, data = filter(STAR, classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;))) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: g4reading by classtype #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean in group small class mean in group regular class #&gt; 723 720 7.2 Hypothesis Testing 7.2.1 Tea-Testing Experiment # Number of cups of tea cups &lt;- 4 # Number guessed correctly k &lt;- c(0, seq_len(cups)) true &lt;- tibble(correct = k * 2, n = choose(cups, k) * choose(cups, cups - k)) %&gt;% mutate(prob = n / sum(n)) true #&gt; # A tibble: 5 x 3 #&gt; correct n prob #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 1.00 0.0143 #&gt; 2 2.00 16.0 0.229 #&gt; 3 4.00 36.0 0.514 #&gt; 4 6.00 16.0 0.229 #&gt; 5 8.00 1.00 0.0143 sims &lt;- 1000 guess &lt;- tibble(guess = c(&quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;, &quot;M&quot;, &quot;T&quot;, &quot;T&quot;, &quot;M&quot;)) randomize_tea &lt;- function(df) { # randomize the order of teas assignment &lt;- sample_frac(df, 1) %&gt;% rename(actual = guess) bind_cols(df, assignment) %&gt;% summarise(correct = sum(guess == actual)) } approx &lt;- map_df(seq_len(sims), ~ randomize_tea(guess)) %&gt;% count(correct) %&gt;% mutate(prob = n / sum(n)) left_join(select(approx, correct, prob_sim = prob), select(true, correct, prob_exact = prob), by = &quot;correct&quot;) %&gt;% mutate(diff = prob_sim - prob_exact) #&gt; # A tibble: 5 x 4 #&gt; correct prob_sim prob_exact diff #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.0110 0.0143 -0.00329 #&gt; 2 2.00 0.234 0.229 0.00543 #&gt; 3 4.00 0.486 0.514 -0.0283 #&gt; 4 6.00 0.253 0.229 0.0244 #&gt; 5 8.00 0.0160 0.0143 0.00171 7.2.2 The General Framework The test functions like fisher.test do not work particularly well with data frames, and expect vectors or matrices as input, so tidyverse functions are less directly applicable # all guesses correct x &lt;- tribble(~Guess, ~Truth, ~Number, &quot;Milk&quot;, &quot;Milk&quot;, 4L, &quot;Milk&quot;, &quot;Tea&quot;, 0L, &quot;Tea&quot;, &quot;Milk&quot;, 0L, &quot;Tea&quot;, &quot;Tea&quot;, 4L) x #&gt; # A tibble: 4 x 3 #&gt; Guess Truth Number #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Milk Milk 4 #&gt; 2 Milk Tea 0 #&gt; 3 Tea Milk 0 #&gt; 4 Tea Tea 4 # 6 correct guesses y &lt;- x %&gt;% mutate(Number = c(3L, 1L, 1L, 3L)) y #&gt; # A tibble: 4 x 3 #&gt; Guess Truth Number #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Milk Milk 3 #&gt; 2 Milk Tea 1 #&gt; 3 Tea Milk 1 #&gt; 4 Tea Tea 3 # Turn into a 2x2 table for fisher.test select(spread(x, Truth, Number), -Guess) #&gt; # A tibble: 2 x 2 #&gt; Milk Tea #&gt; * &lt;int&gt; &lt;int&gt; #&gt; 1 4 0 #&gt; 2 0 4 # Use spread to make it a 2 x 2 table fisher.test(select(spread(x, Truth, Number), -Guess), alternative = &quot;greater&quot;) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: select(spread(x, Truth, Number), -Guess) #&gt; p-value = 0.01 #&gt; alternative hypothesis: true odds ratio is greater than 1 #&gt; 95 percent confidence interval: #&gt; 2 Inf #&gt; sample estimates: #&gt; odds ratio #&gt; Inf fisher.test(select(spread(y, Truth, Number), -Guess)) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: select(spread(y, Truth, Number), -Guess) #&gt; p-value = 0.5 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.212 621.934 #&gt; sample estimates: #&gt; odds ratio #&gt; 6.41 7.2.3 One-Sample Tests n &lt;- 1018 x.bar &lt;- 550 / n se &lt;- sqrt(0.5 * 0.5 / n) # standard deviation of sampling distribution # upper red area in the figure upper &lt;- pnorm(x.bar, mean = 0.5, sd = se, lower.tail = FALSE) # lower red area in the figure; identical to the upper area lower &lt;- pnorm(0.5 - (x.bar - 0.5), mean = 0.5, sd = se) # two side p value upper + lower #&gt; [1] 0.0102 2 * upper #&gt; [1] 0.0102 # one sided p value upper #&gt; [1] 0.00508 z.score &lt;- (x.bar - 0.5) / se z.score #&gt; [1] 2.57 pnorm(z.score, lower.tail = FALSE) # one-sided p-value #&gt; [1] 0.00508 2 * pnorm(z.score, lower.tail = FALSE) # two-sided p-value #&gt; [1] 0.0102 # 99% confidence interval contains 0.5 c(x.bar - qnorm(0.995) * se, x.bar + qnorm(0.995) * se) #&gt; [1] 0.500 0.581 # 95% confidence interval does not contain 0.5 c(x.bar - qnorm(0.975) * se, x.bar + qnorm(0.975) * se) #&gt; [1] 0.510 0.571 # no continuity correction to get the same p-value as above prop.test(550, n = n, p = 0.5, correct = FALSE) #&gt; #&gt; 1-sample proportions test without continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 7, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.510 0.571 #&gt; sample estimates: #&gt; p #&gt; 0.54 # with continuity correction prop.test(550, n = n, p = 0.5) #&gt; #&gt; 1-sample proportions test with continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 6, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.509 0.571 #&gt; sample estimates: #&gt; p #&gt; 0.54 prop.test(550, n = n, p = 0.5, conf.level = 0.99) #&gt; #&gt; 1-sample proportions test with continuity correction #&gt; #&gt; data: 550 out of n, null probability 0.5 #&gt; X-squared = 6, df = 1, p-value = 0.01 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 99 percent confidence interval: #&gt; 0.499 0.581 #&gt; sample estimates: #&gt; p #&gt; 0.54 # two-sided one-sample t-test t.test(STAR$g4reading, mu = 710) #&gt; #&gt; One Sample t-test #&gt; #&gt; data: STAR$g4reading #&gt; t = 10, df = 2000, p-value &lt;2e-16 #&gt; alternative hypothesis: true mean is not equal to 710 #&gt; 95 percent confidence interval: #&gt; 719 723 #&gt; sample estimates: #&gt; mean of x #&gt; 721 7.2.4 Two-sample tests The ATE estimates are stored in a data frame, star_ate. Note that the dplyr function transmute is like mutate, but only returns the variables specified in the function. star_ate %&gt;% transmute(p_value_1sided = pnorm(-abs(ate_est), mean = 0, sd = ate_se), p_value_2sided = 2 * pnorm(-abs(ate_est), mean = 0, sd = ate_se)) #&gt; # A tibble: 1 x 2 #&gt; p_value_1sided p_value_2sided #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0935 0.187 t.test(g4reading ~ classtype, data = filter(STAR, classtype %in% c(&quot;small class&quot;, &quot;regular class&quot;))) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: g4reading by classtype #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean in group small class mean in group regular class #&gt; 723 720 or t.test(filter(STAR, classtype == &quot;small class&quot;)$g4reading, filter(STAR, classtype == &quot;regular class&quot;)$g4reading) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: filter(STAR, classtype == &quot;small class&quot;)$g4reading and filter(STAR, classtype == &quot;regular class&quot;)$g4reading #&gt; t = 1, df = 2000, p-value = 0.2 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.70 8.71 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 723 720 data(&quot;resume&quot;, package = &quot;qss&quot;) x &lt;- resume %&gt;% count(race, call) %&gt;% spread(call, n) %&gt;% ungroup() x #&gt; # A tibble: 2 x 3 #&gt; race `0` `1` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 black 2278 157 #&gt; 2 white 2200 235 prop.test(as.matrix(select(x, -race)), alternative = &quot;greater&quot;) #&gt; #&gt; 2-sample test for equality of proportions with continuity #&gt; correction #&gt; #&gt; data: as.matrix(select(x, -race)) #&gt; X-squared = 20, df = 1, p-value = 2e-05 #&gt; alternative hypothesis: greater #&gt; 95 percent confidence interval: #&gt; 0.0188 1.0000 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.936 0.903 Assign sample sizes and proportions, then calculate point estimates, standard error, z-statistic and one-sided p-value. n0 &lt;- sum(resume$race == &quot;black&quot;) n1 &lt;- sum(resume$race == &quot;white&quot;) p &lt;- mean(resume$call) p0 &lt;- mean(filter(resume, race == &quot;black&quot;)$call) p1 &lt;- mean(filter(resume, race == &quot;white&quot;)$call) est &lt;- p1 - p0 est #&gt; [1] 0.032 se &lt;- sqrt(p * (1 - p) * (1 / n0 + 1 / n1)) se #&gt; [1] 0.0078 zstat &lt;- est / se zstat #&gt; [1] 4.11 pnorm(-abs(zstat)) #&gt; [1] 1.99e-05 The only thing that changed is using filter for selecting the groups. 7.2.5 Power Analysis # set the parameters n &lt;- 250 p.star &lt;- 0.48 # data generating process p &lt;- 0.5 # null value alpha &lt;- 0.05 # critical value cr.value &lt;- qnorm(1 - alpha / 2) # standard errors under the hypothetical data generating process se.star &lt;- sqrt(p.star * (1 - p.star) / n) # standard error under the null se &lt;- sqrt(p * (1 - p) / n) # power pnorm(p - cr.value * se, mean = p.star, sd = se.star) + pnorm(p + cr.value * se, mean = p.star, sd = se.star, lower.tail = FALSE) # parameters n1 &lt;- 500 n0 &lt;- 500 p1.star &lt;- 0.05 p0.star &lt;- 0.1 # overall call back rate as a weighted average p &lt;- (n1 * p1.star + n0 * p0.star) / (n1 + n0) # standard error under the null se &lt;- sqrt(p * (1 - p) * (1 / n1 + 1 / n0)) # standard error under the hypothetical data generating process se.star &lt;- sqrt(p1.star * (1 - p1.star) / n1 + p0.star * (1 - p0.star) / n0) pnorm(-cr.value * se, mean = p1.star - p0.star, sd = se.star) + pnorm(cr.value * se, mean = p1.star - p0.star, sd = se.star, lower.tail = FALSE) power.prop.test(n = 500, p1 = 0.05, p2 = 0.1, sig.level = 0.05) power.prop.test(p1 = 0.05, p2 = 0.1, sig.level = 0.05, power = 0.9) power.t.test(n = 100, delta = 0.25, sd = 1, type = &quot;one.sample&quot;) power.t.test(power = 0.9, delta = 0.25, sd = 1, type = &quot;one.sample&quot;) power.t.test(delta = 0.25, sd = 1, type = &quot;two.sample&quot;, alternative = &quot;one.sided&quot;, power = 0.9) 7.3 Linear Regression Model with Uncertainty 7.3.1 Linear Regression as a Generative Model Load the minimum wage date included with the qss package: data(&quot;minwage&quot;, package = &quot;qss&quot;) minwage &lt;- mutate(minwage, fullPropBefore = fullBefore / (fullBefore + partBefore), fullPropAfter = fullAfter / (fullAfter + partAfter), NJ = as.integer(location == &quot;PA&quot;)) fit_minwage &lt;- lm(fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + chain, data = minwage) fit_minwage #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Coefficients: #&gt; NJ fullPropBefore wageBefore chainburgerking #&gt; -0.0542 0.1688 0.0813 -0.0614 #&gt; chainkfc chainroys chainwendys #&gt; -0.0966 -0.1522 -0.1659 fit_minwage1 &lt;- lm(fullPropAfter ~ NJ + fullPropBefore + wageBefore + chain, data = minwage) fit_minwage1 #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Coefficients: #&gt; (Intercept) NJ fullPropBefore wageBefore #&gt; -0.0614 -0.0542 0.1688 0.0813 #&gt; chainkfc chainroys chainwendys #&gt; -0.0352 -0.0908 -0.1045 gather_predictions(slice(minwage, 1), fit_minwage, fit_minwage1) %&gt;% select(model, pred) #&gt; # A tibble: 2 x 2 #&gt; model pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 fit_minwage 0.271 #&gt; 2 fit_minwage1 0.271 7.3.2 Inference about coefficients Use the tidy function to return the coefficients, including confidence intervals, as a data frame: data(&quot;women&quot;, package = &quot;qss&quot;) fit_women &lt;- lm(water ~ reserved, data = women) summary(fit_women) #&gt; #&gt; Call: #&gt; lm(formula = water ~ reserved, data = women) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -23.99 -14.74 -7.86 2.26 316.01 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 14.74 2.29 6.45 4.2e-10 *** #&gt; reserved 9.25 3.95 2.34 0.02 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 33.4 on 320 degrees of freedom #&gt; Multiple R-squared: 0.0169, Adjusted R-squared: 0.0138 #&gt; F-statistic: 5.49 on 1 and 320 DF, p-value: 0.0197 tidy(fit_women) #&gt; term estimate std.error statistic p.value #&gt; 1 (Intercept) 14.74 2.29 6.45 4.22e-10 #&gt; 2 reserved 9.25 3.95 2.34 1.97e-02 You need to set conf.int = TRUE for tidy to include the confidence interval: summary(fit_minwage) #&gt; #&gt; Call: #&gt; lm(formula = fullPropAfter ~ -1 + NJ + fullPropBefore + wageBefore + #&gt; chain, data = minwage) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.4862 -0.1813 -0.0281 0.1513 0.7509 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; NJ -0.0542 0.0332 -1.63 0.1034 #&gt; fullPropBefore 0.1688 0.0566 2.98 0.0031 ** #&gt; wageBefore 0.0813 0.0389 2.09 0.0374 * #&gt; chainburgerking -0.0614 0.1755 -0.35 0.7266 #&gt; chainkfc -0.0966 0.1793 -0.54 0.5904 #&gt; chainroys -0.1522 0.1832 -0.83 0.4066 #&gt; chainwendys -0.1659 0.1853 -0.90 0.3711 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.244 on 351 degrees of freedom #&gt; Multiple R-squared: 0.635, Adjusted R-squared: 0.628 #&gt; F-statistic: 87.2 on 7 and 351 DF, p-value: &lt;2e-16 tidy(fit_minwage, conf.int = TRUE) #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; 1 NJ -0.0542 0.0332 -1.633 0.10343 -0.11953 0.0111 #&gt; 2 fullPropBefore 0.1688 0.0566 2.981 0.00307 0.05744 0.2801 #&gt; 3 wageBefore 0.0813 0.0389 2.090 0.03737 0.00478 0.1579 #&gt; 4 chainburgerking -0.0614 0.1755 -0.350 0.72657 -0.40648 0.2837 #&gt; 5 chainkfc -0.0966 0.1793 -0.539 0.59044 -0.44919 0.2560 #&gt; 6 chainroys -0.1522 0.1832 -0.831 0.40664 -0.51238 0.2080 #&gt; 7 chainwendys -0.1659 0.1853 -0.895 0.37115 -0.53031 0.1985 7.3.3 Inference about predictions data(&quot;MPs&quot;, package = &quot;qss&quot;) MPs_labour &lt;- filter(MPs, party == &quot;labour&quot;) MPs_tory &lt;- filter(MPs, party == &quot;tory&quot;) labour_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &lt; 0)) labour_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_labour, margin &gt; 0)) tory_fit1 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &lt; 0)) tory_fit2 &lt;- lm(ln.net ~ margin, data = filter(MPs_tory, margin &gt; 0)) Predictions at the threshold. The broom function augment will return prediction fitted values and standard errors for each value, but not the confidence intervals themselves (we’d have to multiply the correct t-distribution with degrees of freedom.) So instead, we’ll directly use the predict function: tory_y0 &lt;- predict(tory_fit1, interval = &quot;confidence&quot;, newdata = tibble(margin = 0)) %&gt;% as_tibble() tory_y0 #&gt; # A tibble: 1 x 3 #&gt; fit lwr upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 12.5 12.1 13.0 tory_y1 &lt;- predict(tory_fit2, interval = &quot;confidence&quot;, newdata = tibble(margin = 0)) %&gt;% as_tibble() tory_y1 #&gt; # A tibble: 1 x 3 #&gt; fit lwr upr #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 13.2 12.8 13.6 Alternatively, using augment (and assuming a normal distribution since the number of observations is so large its not worth worrying about the t-distribution): tory_y0 &lt;- augment(tory_fit1, newdata = tibble(margin = 0)) %&gt;% mutate(lwr = .fitted + qnorm(0.025) * .se.fit, upr = .fitted + qnorm(0.975) * .se.fit) tory_y0 #&gt; margin .fitted .se.fit lwr upr #&gt; 1 0 12.5 0.214 12.1 13 tory_y1 &lt;- augment(tory_fit2, newdata = tibble(margin = 0)) %&gt;% mutate(lwr = .fitted + qnorm(0.025) * .se.fit, upr = .fitted + qnorm(0.975) * .se.fit) tory_y1 #&gt; margin .fitted .se.fit lwr upr #&gt; 1 0 13.2 0.192 12.8 13.6 y1_range &lt;- data_grid(filter(MPs_tory, margin &lt;= 0), margin) tory_y0 &lt;- augment(tory_fit1, newdata = y1_range) y2_range &lt;- data_grid(filter(MPs_tory, margin &gt;= 0), margin) tory_y1 &lt;- augment(tory_fit2, newdata = y2_range) ggplot() + geom_ref_line(v = 0) + geom_point(aes(y = ln.net, x = margin), data = MPs_tory) + # plot losers geom_ribbon(aes(x = margin, ymin = .fitted + qnorm(0.025) * .se.fit, ymax = .fitted + qnorm(0.975) * .se.fit), data = tory_y0, alpha = 0.3) + geom_line(aes(x = margin, y = .fitted), data = tory_y0) + # plot winners geom_ribbon(aes(x = margin, ymin = .fitted + qnorm(0.025) * .se.fit, ymax = .fitted + qnorm(0.975) * .se.fit), data = tory_y1, alpha = 0.3) + geom_line(aes(x = margin, y = .fitted), data = tory_y1) + labs(x = &quot;Margin of vitory&quot;, y = &quot;log net wealth&quot;) tory_y1 &lt;- augment(tory_fit1, newdata = tibble(margin = 0)) tory_y1 #&gt; margin .fitted .se.fit #&gt; 1 0 12.5 0.214 tory_y0 &lt;- augment(tory_fit2, newdata = tibble(margin = 0)) tory_y0 #&gt; margin .fitted .se.fit #&gt; 1 0 13.2 0.192 summary(tory_fit1) #&gt; #&gt; Call: #&gt; lm(formula = ln.net ~ margin, data = filter(MPs_tory, margin &lt; #&gt; 0)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.320 -0.472 -0.035 0.663 3.580 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.538 0.214 58.54 &lt;2e-16 *** #&gt; margin 1.491 1.291 1.15 0.25 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.43 on 119 degrees of freedom #&gt; Multiple R-squared: 0.0111, Adjusted R-squared: 0.00277 #&gt; F-statistic: 1.33 on 1 and 119 DF, p-value: 0.251 summary(tory_fit2) #&gt; #&gt; Call: #&gt; lm(formula = ln.net ~ margin, data = filter(MPs_tory, margin &gt; #&gt; 0)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.858 -0.877 0.001 0.830 3.126 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.188 0.192 68.69 &lt;2e-16 *** #&gt; margin -0.728 1.982 -0.37 0.71 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.29 on 100 degrees of freedom #&gt; Multiple R-squared: 0.00135, Adjusted R-squared: -0.00864 #&gt; F-statistic: 0.135 on 1 and 100 DF, p-value: 0.714 Since we aren’t doing anything more with these values, there isn’t much benefit in keeping them in data frames. # standard error se_diff &lt;- sqrt(tory_y0$.se.fit ^ 2 + tory_y1$.se.fit ^ 2) se_diff #&gt; [1] 0.288 # point estimate diff_est &lt;- tory_y1$.fitted - tory_y0$.fitted diff_est #&gt; [1] -0.65 # confidence interval CI &lt;- c(diff_est - se_diff * qnorm(0.975), diff_est + se_diff * qnorm(0.975)) CI #&gt; [1] -1.2134 -0.0859 # hypothesis test z.score &lt;- diff_est / se_diff # two sided p value p.value &lt;- 2 * pnorm(abs(z.score), lower.tail = FALSE) p.value #&gt; [1] 0.0239 For more on using projects read Project-oriented workflow.↩ See the discussion in R for Data Science on how tibble objects differ from base data.frame objects in how [ is handled.↩ "]
]
