[
["probability.html", "7 Probability 7.1 Prerequisites 7.2 Probability 7.3 Conditional Probability 7.4 Large Sample Theorems", " 7 Probability 7.1 Prerequisites library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;stringr&quot;) We will also, use the function qss_data_url: 7.2 Probability Original code k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse birthday &lt;- function(k) { logdenom &lt;- k * log(365) + lfactorial(365 - k) lognumer &lt;- lfactorial(365) pr &lt;- 1 - exp(lognumer - logdenom) pr } bday &lt;- tibble(k = 1:50, pr = birthday(k)) ggplot(bday, aes(x = k , y = pr)) + geom_hline(yintercept = 0.5, colour = &quot;white&quot;, size = 2) + geom_line() + geom_point() + scale_y_continuous(&quot;Probability that at least two\\n people have the same birthday&quot;, limits = c(0, 1)) + labs(x = &quot;Number of people&quot;) Note: The logarithm is used for numerical stability. Basically, “floating-point” numbers are approximations of numbers. If you perform arithmetic with numbers that are very large, very small, or vary differently in magnitudes, you could have problems. Logarithms help with some of those issues. See “Falling Into the Floating Point Trap” in The R Inforno for a summary of floating point numbers. See these John Fox posts 1 2 for an example of numerical stability gone wrong. Also see: http://andrewgelman.com/2016/06/11/log-sum-of-exponentials/. 7.2.1 Sampling without replacement Original code: k &lt;- 23 # number of people sims &lt;- 1000 # number of simulations event &lt;- 0 # counter for (i in 1:sims) { days &lt;- sample(1:365, k, replace = TRUE) days.unique &lt;- unique(days) # unique birthdays ## if there are duplicates, the number of unique birthdays ## will be less than the number of birthdays, which is `k&#39; if (length(days.unique) &lt; k) { event &lt;- event + 1 } } ## fraction of trials where at least two bdays are the same answer &lt;- event / sims answer tidyverse code: Instead of using a for loop, we could do the simulations using a functional as described in R for Data Science. Define the function sim_bdays to randomly sample k birthdays, and returns TRUE if there are any duplicates. sim_bdays &lt;- function(k) { days &lt;- sample(1:365, k, replace = TRUE) length(unique(days)) &lt; k } Set the parameters for 1,000 simulations, and 23 individuals. We use map_lgl since sim_bdays returns a logical value (TRUE, FALSE): sims &lt;- 1000 k &lt;- 23 map_lgl(seq_len(sims), ~ sim_bdays(k)) %&gt;% mean() #&gt; [1] 0.486 7.2.2 Combinations Original code choose(84, 6) #&gt; [1] 4.06e+08 7.3 Conditional Probability 7.3.1 Conditional, Marginal, and Joint Probabilities Original: FLVoters &lt;- read.csv(&quot;FLVoters.csv&quot;) dim(FLVoters) FLVoters &lt;- na.omit(FLVoters) dim(FLVoters) tidyverse FLVoters &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLVoters.csv&quot;)) dim(FLVoters) #&gt; [1] 10000 6 FLVoters &lt;- FLVoters %&gt;% na.omit() original: margin.race &lt;- prop.table(table(FLVoters$race)) margin.race margin.gender &lt;- prop.table(table(FLVoters$gender)) margin.gender tidyverse: Instead of using prop.base, we calculate the probabilities with a data frame. Marginal probabilities of race, margin_race &lt;- FLVoters %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) margin_race #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 175 0.01920 #&gt; 2 black 1194 0.13102 #&gt; 3 hispanic 1192 0.13080 #&gt; 4 native 29 0.00318 #&gt; 5 other 310 0.03402 #&gt; 6 white 6213 0.68177 Marginal probabilities of gender: margin_gender &lt;- FLVoters %&gt;% count(gender) %&gt;% mutate(prop = n / sum(n)) margin_gender #&gt; # A tibble: 2 × 3 #&gt; gender n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f 4883 0.536 #&gt; 2 m 4230 0.464 Original: prop.table(table(FLVoters$race[FLVoters$gender == &quot;f&quot;])) tidyverse: FLVoters %&gt;% filter(gender == &quot;f&quot;) %&gt;% count(race) %&gt;% mutate(prop = n / sum(n)) #&gt; # A tibble: 6 × 3 #&gt; race n prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian 83 0.01700 #&gt; 2 black 678 0.13885 #&gt; 3 hispanic 666 0.13639 #&gt; 4 native 17 0.00348 #&gt; 5 other 158 0.03236 #&gt; 6 white 3281 0.67192 Original: joint.p &lt;- prop.table(table(race = FLVoters$race, gender = FLVoters$gender)) joint.p #&gt; gender #&gt; race f m #&gt; asian 0.00911 0.01010 #&gt; black 0.07440 0.05662 #&gt; hispanic 0.07308 0.05772 #&gt; native 0.00187 0.00132 #&gt; other 0.01734 0.01668 #&gt; white 0.36004 0.32174 tidyverse: joint_p &lt;- FLVoters %&gt;% count(gender, race) %&gt;% # needed because it is still grouped by gender ungroup() %&gt;% mutate(prop = n / sum(n)) joint_p #&gt; # A tibble: 12 × 4 #&gt; gender race n prop #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 f asian 83 0.00911 #&gt; 2 f black 678 0.07440 #&gt; 3 f hispanic 666 0.07308 #&gt; 4 f native 17 0.00187 #&gt; 5 f other 158 0.01734 #&gt; 6 f white 3281 0.36004 #&gt; # ... with 6 more rows We can convert the data frame to have gender as columns: joint_p %&gt;% ungroup() %&gt;% select(-n) %&gt;% spread(gender, prop) #&gt; # A tibble: 6 × 3 #&gt; race f m #&gt; * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian 0.00911 0.01010 #&gt; 2 black 0.07440 0.05662 #&gt; 3 hispanic 0.07308 0.05772 #&gt; 4 native 0.00187 0.00132 #&gt; 5 other 0.01734 0.01668 #&gt; 6 white 0.36004 0.32174 Original: rowSums(joint.p) tidyverse: Sum over race: joint_p %&gt;% group_by(race) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 6 × 2 #&gt; race prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 asian 0.01920 #&gt; 2 black 0.13102 #&gt; 3 hispanic 0.13080 #&gt; 4 native 0.00318 #&gt; 5 other 0.03402 #&gt; 6 white 0.68177 Original: colSums(joint.p) tidyverse: Sum over gender joint_p %&gt;% group_by(gender) %&gt;% summarise(prop = sum(prop)) #&gt; # A tibble: 2 × 2 #&gt; gender prop #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 f 0.536 #&gt; 2 m 0.464 Original: FLVoters$age.group &lt;- NA # initialize a variable FLVoters$age.group[FLVoters$age &lt;= 20] &lt;- 1 FLVoters$age.group[FLVoters$age &gt; 20 &amp; FLVoters$age &lt;= 40] &lt;- 2 FLVoters$age.group[FLVoters$age &gt; 40 &amp; FLVoters$age &lt;= 60] &lt;- 3 FLVoters$age.group[FLVoters$age &gt; 60] &lt;- 4 tidyverse: Use the cut FLVoters &lt;- FLVoters %&gt;% mutate(age_group = cut(age, c(0, 20, 40, 60, Inf), right = TRUE, labels = c(&quot;&lt;= 20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;&gt; 60&quot;))) Original: joint3 &lt;- prop.table(table(race = FLVoters$race, age.group = FLVoters$age.group, gender = FLVoters$gender)) joint3 tidyverse: joint3 &lt;- FLVoters %&gt;% count(race, age_group, gender) %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) joint3 #&gt; # A tibble: 47 × 5 #&gt; race age_group gender n prop #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 asian &lt;= 20 f 1 0.000110 #&gt; 2 asian &lt;= 20 m 2 0.000219 #&gt; 3 asian 20-40 f 24 0.002634 #&gt; 4 asian 20-40 m 26 0.002853 #&gt; 5 asian 40-60 f 38 0.004170 #&gt; 6 asian 40-60 m 47 0.005157 #&gt; # ... with 41 more rows original: margin.age &lt;- prop.table(table(FLVoters$age.group)) margin.age joint3[&quot;black&quot;, 4, &quot;f&quot;] / margin.age[4] tidyverse: Marginal probabilities by age groups margin_age &lt;- FLVoters %&gt;% count(age_group) %&gt;% mutate(prop = n / sum(n)) margin_age #&gt; # A tibble: 4 × 3 #&gt; age_group n prop #&gt; &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 161 0.0177 #&gt; 2 20-40 2469 0.2709 #&gt; 3 40-60 3285 0.3605 #&gt; 4 &gt; 60 3198 0.3509 Calculate the probabilities that each group is in a given age group, and show $P( | ) left_join(joint3, select(margin_age, age_group, margin_age = prop), by = &quot;age_group&quot;) %&gt;% mutate(prob_age_group = prop / margin_age) %&gt;% filter(race == &quot;black&quot;, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;) %&gt;% select(race, age_group, gender, prob_age_group) #&gt; # A tibble: 1 × 4 #&gt; race age_group gender prob_age_group #&gt; &lt;chr&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 black &gt; 60 f 0.0538 Original: joint2 &lt;- prop.table(table(age.group = FLVoters$age.group, gender = FLVoters$gender)) joint2 joint2[4, &quot;f&quot;] joint3[&quot;black&quot;, 4, &quot;f&quot;] / joint2[4, &quot;f&quot;] tidyverse: Two-way joint probability table for age group and gender joint2 &lt;- FLVoters %&gt;% count(age_group, gender) %&gt;% ungroup() %&gt;% mutate(prob_age_gender = n / sum(n)) joint2 #&gt; # A tibble: 8 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &lt;= 20 f 88 0.00966 #&gt; 2 &lt;= 20 m 73 0.00801 #&gt; 3 20-40 f 1304 0.14309 #&gt; 4 20-40 m 1165 0.12784 #&gt; 5 40-60 f 1730 0.18984 #&gt; 6 40-60 m 1555 0.17064 #&gt; # ... with 2 more rows The joint probability \\(P(\\text{age} &gt; 60 \\land \\text{female})\\), filter(joint2, age_group == &quot;&gt; 60&quot;, gender == &quot;f&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender n prob_age_gender #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f 1761 0.193 The conditional probabilities \\(P(race | gender, age)\\), condprob_race &lt;- left_join(joint3, select(joint2, -n), by = c(&quot;age_group&quot;, &quot;gender&quot;)) %&gt;% mutate(prob_race = prop / prob_age_gender) %&gt;% arrange(age_group, gender) %&gt;% select(age_group, gender, race, prob_race) Each row is the \\(P(race | age_group, gender)\\), so \\(P(\\text{black} | \\text{female} \\land \\text{age} &gt; 60)\\), filter(condprob_race, gender == &quot;f&quot;, age_group == &quot;&gt; 60&quot;, race == &quot;black&quot;) #&gt; # A tibble: 1 × 4 #&gt; age_group gender race prob_race #&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &gt; 60 f black 0.0977 7.3.2 Independence Original code plot(c(margin.race * margin.gender[&quot;f&quot;]), # product of marginal probs. c(joint.p[, &quot;f&quot;]), # joint probabilities xlim = c(0, 0.4), ylim = c(0, 0.4), xlab = &quot;P(race) * P(female)&quot;, ylab = &quot;P(race and female)&quot;) abline(0, 1) # 45 degree line Tidyverse Create a table with the products of margins of race and age. Using the function crossing to create a tibble with all combinations of race and gender and the independent prob. race_gender_indep &lt;- crossing(select(margin_race, race, prob_race = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(prob_indep = prob_race * prob_gender) %&gt;% left_join(select(joint_p, gender, race, prob = prop), by = c(&quot;gender&quot;, &quot;race&quot;)) %&gt;% select(race, gender, everything()) race_gender_indep #&gt; # A tibble: 12 × 6 #&gt; race gender prob_race prob_gender prob_indep prob #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 asian f 0.0192 0.536 0.01029 0.00911 #&gt; 2 asian m 0.0192 0.464 0.00891 0.01010 #&gt; 3 black f 0.1310 0.536 0.07021 0.07440 #&gt; 4 black m 0.1310 0.464 0.06082 0.05662 #&gt; 5 hispanic f 0.1308 0.536 0.07009 0.07308 #&gt; 6 hispanic m 0.1308 0.464 0.06071 0.05772 #&gt; # ... with 6 more rows ggplot(race_gender_indep, aes(x = prob_indep, y = prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(. ~ gender) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(x = expression(P(&quot;race&quot;) * P(&quot;gender&quot;)), y = expression(P(&quot;race and gender&quot;))) Original code ## joint independence plot(c(joint3[, 4, &quot;f&quot;]), # joint probability margin.race * margin.age[4] * margin.gender[&quot;f&quot;], # product of marginals xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;joint independence&quot;, xlab = &quot;P(race and above 60 and female)&quot;, ylab = &quot;P(race) * P(above 60) * P(female)&quot;) abline(0, 1) ## conditional independence given female plot(c(joint3[, 4, &quot;f&quot;]) / margin.gender[&quot;f&quot;], # joint prob. given female ## product of marginals (joint.p[, &quot;f&quot;] / margin.gender[&quot;f&quot;]) * (joint2[4, &quot;f&quot;] / margin.gender[&quot;f&quot;]), xlim = c(0, 0.3), ylim = c(0, 0.3), main = &quot;marginal independence&quot;, xlab = &quot;P(race and above 60 | female)&quot;, ylab = &quot;P(race | female) * P(above 60 | female)&quot;) abline(0, 1) While the original code only calculates joint-independence value for values of age &gt; 60, and female, this calculates the joint probabilities for all combinations of the three variables, and facets by age and gender. joint_indep &lt;- # all combinations of race, age, gender crossing(select(margin_race, race, prob_race = prop), select(margin_age, age_group, prob_age = prop), select(margin_gender, gender, prob_gender = prop)) %&gt;% mutate(indep_prob = prob_race * prob_age * prob_gender) %&gt;% left_join(select(joint3, race, age_group, gender, prob = prop), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% replace_na(list(prob = 0)) ggplot(joint_indep, aes(x = prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age and gender)&quot;, y = &quot;P(race) * P(age) * P(gender)&quot;, title = &quot;Joint Independence&quot;) The original code only calculates the conditional independence given female; this calculates conditional independence for all values of gender: cond_gender &lt;- left_join(select(joint3, race, age_group, gender, joint_prob = prop), select(margin_gender, gender, prob_gender = prop), by = c(&quot;gender&quot;)) %&gt;% mutate(cond_prob = joint_prob / prob_gender) # P(race | gender) prob_race_gender &lt;- left_join(select(joint_p, race, gender, prob_race_gender = prop), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_race = prob_race_gender / prob_gender) # P(age | gender) prob_age_gender &lt;- left_join(select(joint2, age_group, gender, prob_age_gender), select(margin_gender, gender, prob_gender = prop), by = &quot;gender&quot;) %&gt;% mutate(prob_age = prob_age_gender / prob_gender) # indep prob of race and age indep_cond_gender &lt;- full_join(select(prob_race_gender, race, gender, prob_race), select(prob_age_gender, age_group, gender, prob_age), by = &quot;gender&quot;) %&gt;% mutate(indep_prob = prob_race * prob_age) inner_join(select(indep_cond_gender, race, age_group, gender, indep_prob), select(cond_gender, race, age_group, gender, cond_prob), by = c(&quot;gender&quot;, &quot;age_group&quot;, &quot;race&quot;)) %&gt;% ggplot(aes(x = cond_prob, y = indep_prob, colour = race)) + geom_abline(intercept = 0, slope = 1, colour = &quot;white&quot;, size = 2) + geom_point() + facet_grid(age_group ~ gender) + coord_fixed() + labs(x = &quot;P(race and age | gender)&quot;, y = &quot;P(race | gender) * P(age | gender)&quot;, title = &quot;Marginal independence&quot;) Orginal code for the Monty-Hall problem sims &lt;- 1000 doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) result.switch &lt;- result.noswitch &lt;- rep(NA, sims) for (i in 1:sims) { ## randomly choose the initial door first &lt;- sample(1:3, size = 1) result.noswitch[i] &lt;- doors[first] remain &lt;- doors[-first] # remaining two doors ## Monty chooses one door with a goat monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) result.switch[i] &lt;- remain[-monty] } mean(result.noswitch == &quot;car&quot;) This code doesn’t rely on any non-tidyverse code, but the following is another way to write it. First, create a function for a single iteration. This will save the results as data frame. choose_door &lt;- function(.id) { # put doors inside the function to ensure it doesn&#39;t get changed doors &lt;- c(&quot;goat&quot;, &quot;goat&quot;, &quot;car&quot;) first &lt;- sample(1:3, 1) remain &lt;- doors[-first] monty &lt;- sample((1:2)[remain == &quot;goat&quot;], size = 1) ret &lt;- tribble(~strategy, ~result, &quot;no switch&quot;, doors[first], &quot;switch&quot;, remain[-monty]) ret[[&quot;.id&quot;]] &lt;- .id ret } Now use map_df to run choose_door multiple times, and then summarize the results: sims &lt;- 1000 results &lt;- map_df(seq_len(sims), choose_door) results %&gt;% group_by(strategy) %&gt;% summarise(pct_win = mean(result == &quot;car&quot;)) #&gt; # A tibble: 2 × 2 #&gt; strategy pct_win #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 no switch 0.35 #&gt; 2 switch 0.65 Can we make this even more general? What about a function that takes a data frame as its input with the choices? … 7.3.3 Predicting Race Using Surname and Residence Location Start with the Census names files: cnames &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;names.csv&quot;)) glimpse(cnames) #&gt; Observations: 151,671 #&gt; Variables: 7 #&gt; $ surname &lt;chr&gt; &quot;SMITH&quot;, &quot;JOHNSON&quot;, &quot;WILLIAMS&quot;, &quot;BROWN&quot;, &quot;JONES&quot;, ... #&gt; $ count &lt;int&gt; 2376206, 1857160, 1534042, 1380145, 1362755, 11278... #&gt; $ pctwhite &lt;dbl&gt; 73.34, 61.55, 48.52, 60.72, 57.69, 85.80, 64.73, 6... #&gt; $ pctblack &lt;dbl&gt; 22.22, 33.80, 46.72, 34.54, 37.73, 10.41, 30.77, 0... #&gt; $ pctapi &lt;dbl&gt; 0.40, 0.42, 0.37, 0.41, 0.35, 0.42, 0.40, 1.43, 0.... #&gt; $ pcthispanic &lt;dbl&gt; 1.56, 1.50, 1.60, 1.64, 1.44, 1.43, 1.58, 90.82, 9... #&gt; $ pctothers &lt;dbl&gt; 2.48, 2.73, 2.79, 2.69, 2.79, 1.94, 2.52, 1.09, 0.... For each surname, contains variables with the probability that it belongs to an individual of a given race (pctwhite, pctblack, …). We want to find the most-likely race for a given surname, by finding the race with the maximum proportion. Instead of dealing with multiple variables, it is easier to use max on a single variable, so we will rearrange the data to in order to use a grouped summarize, and then merge the new variable back to the original data sets. Note, this code takes a while. most_likely &lt;- cnames %&gt;% select(-count) %&gt;% gather(race_pred, pct, -surname) %&gt;% # remove pct prefix mutate(race_pred = str_replace(race_pred, &quot;^pct&quot;, &quot;&quot;)) %&gt;% # group by surname group_by(surname) %&gt;% filter(pct == max(pct)) %&gt;% # if there are ties, keep only one obs slice(1) %&gt;% # don&#39;t need pct anymore select(-pct) %&gt;% mutate(race_pred = factor(race_pred), # need to convert to factor first, else errors race_pred = fct_recode(race_pred, &quot;asian&quot; = &quot;api&quot;, &quot;other&quot; = &quot;others&quot;)) Now merge that back to the original cnames data frame. cnames &lt;- left_join(cnames, most_likely, by = &quot;surname&quot;) Instead of using match, use inner_join to merge the surnames to FLVoters: FLVoters &lt;- inner_join(FLVoters, cnames, by = &quot;surname&quot;) dim(FLVoters) #&gt; [1] 8022 14 FLVoters also includes a “native” category that the surname dataset does not. FLVoters &lt;- FLVoters %&gt;% mutate(race2 = fct_recode(race, other = &quot;native&quot;)) Check that the levels of race and race_pred are the same: FLVoters %&gt;% count(race2) #&gt; # A tibble: 5 × 2 #&gt; race2 n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 asian 140 #&gt; 2 black 1078 #&gt; 3 hispanic 1023 #&gt; 4 other 277 #&gt; 5 white 5504 FLVoters %&gt;% count(race_pred) #&gt; # A tibble: 5 × 2 #&gt; race_pred n #&gt; &lt;fctr&gt; &lt;int&gt; #&gt; 1 white 6516 #&gt; 2 black 258 #&gt; 3 hispanic 1121 #&gt; 4 asian 120 #&gt; 5 other 7 Now we can calculate True positive rate for all races FLVoters %&gt;% group_by(race2) %&gt;% summarise(tp = mean(race2 == race_pred)) %&gt;% arrange(desc(tp)) #&gt; # A tibble: 5 × 2 #&gt; race2 tp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 white 0.95022 #&gt; 2 hispanic 0.84653 #&gt; 3 asian 0.56429 #&gt; 4 black 0.16048 #&gt; 5 other 0.00361 and the False discovery rate for all races, FLVoters %&gt;% group_by(race_pred) %&gt;% summarise(fp = mean(race2 != race_pred)) %&gt;% arrange(desc(fp)) #&gt; # A tibble: 5 × 2 #&gt; race_pred fp #&gt; &lt;fctr&gt; &lt;dbl&gt; #&gt; 1 other 0.857 #&gt; 2 asian 0.342 #&gt; 3 black 0.329 #&gt; 4 hispanic 0.227 #&gt; 5 white 0.197 Now add residence data FLCensus &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;FLCensusVTD.csv&quot;)) \\(P(race)\\) in Florida: race.prop &lt;- FLCensus %&gt;% select(total.pop, white, black, api, hispanic, others) %&gt;% gather(race, pct, -total.pop) %&gt;% group_by(race) %&gt;% summarise(mean = weighted.mean(pct, weights = total.pop)) %&gt;% arrange(desc(mean)) race.prop #&gt; # A tibble: 5 × 2 #&gt; race mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 white 0.6045 #&gt; 2 hispanic 0.2128 #&gt; 3 black 0.1394 #&gt; 4 api 0.0219 #&gt; 5 others 0.0214 7.3.4 Predicting Election Outcomes with Uncertainty Original code pres08 &lt;- read.csv(&quot;pres08.csv&quot;) ## two-party vote share pres08$p &lt;- pres08$Obama / (pres08$Obama + pres08$McCain) Tidyverse code pres08 &lt;- read_csv(qss_data_url(&quot;probability&quot;, &quot;pres08.csv&quot;)) %&gt;% mutate(p = Obama / (Obama + McCain)) Original code n.states &lt;- nrow(pres08) # number of states n &lt;- 1000 # number of respondents sims &lt;- 10000 # number of simulations ## Obama&#39;s electoral votes Obama.ev &lt;- rep(NA, sims) for (i in 1:sims) { ## samples number of votes for Obama in each state draws &lt;- rbinom(n.states, size = n, prob = pres08$p) ## sums state&#39;s electoral college votes if Obama wins the majority Obama.ev[i] &lt;- sum(pres08$EV[draws &gt; n/2]) } hist(Obama.ev, freq = FALSE, main = &quot;Prediction of Election Outcome&quot;, xlab = &quot;Obama&#39;s Electoral College Votes&quot;) abline(v = 364, col = &quot;red&quot;) # actual result Tidyverse code Write a function to simulate the elections. df is the data frame (pres08) with the state, EV, and p columns. n_draws is the size of the binomial distribution to draw from. .id is the simulation number. sim_election &lt;- function(.id, df, n_draws = 1000) { # For each state randomly sample mutate(df, draws = rbinom(n(), n_draws, p)) %&gt;% filter(draws &gt; (n_draws / 2)) %&gt;% summarise(EV = sum(EV), .id = .id) } Now simulate the election 10,000 times: sims &lt;- 10000 sim_results &lt;- map_df(seq_len(sims), ~ sim_election(.x, pres08, n_draws = 1000)) In the 2008 election, Obama received 364 electoral votes ELECTION_EV &lt;- 364 And plot them, ggplot(sim_results, aes(x = EV, y = ..density..)) + geom_histogram(binwidth = 10, fill = &quot;gray30&quot;) + geom_vline(xintercept = ELECTION_EV, colour = &quot;black&quot;, size = 2) + labs(x = &quot;Electoral Votes&quot;, y = &quot;density&quot;) Original code mean(Obama.ev) ## [1] 352.1646 ## probability of binomial random variable taking greater than n/2 votes sum(pres08$EV * pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE)) ## [1] 352.1388 ## approximate variance using Monte Carlo draws var(Obama.ev) ## [1] 268.7592 ## theoretical variance pres08$pb &lt;- pbinom(n/2, size = n, prob = pres08$p, lower.tail = FALSE) V &lt;- sum(pres08$pb*(1-pres08$pb)*pres08$EV^2) V ## [1] 268.8008 ## approximate standard deviation using Monte Carlo draws sd(Obama.ev) ## [1] 16.39388 ## theoretical standard deviation sqrt(V) ## [1] 16.39515 tidyverse code Simulation mean, variance, and standard deviations: sim_results %&gt;% select(EV) %&gt;% summarise_all(funs(mean, var, sd)) #&gt; # A tibble: 1 × 3 #&gt; mean var sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 270 16.4 Theoretical probabilities # we cannot use n, because mutate will look for n() first. n_draws &lt;- 1000 pres08 %&gt;% mutate(pb = pbinom(n_draws / 2, size = n_draws, prob = p, lower.tail = FALSE)) %&gt;% summarise(mean = sum(pb * EV), V = sum(pb * (1 - pb) * EV ^ 2), sd = sqrt(V)) #&gt; # A tibble: 1 × 3 #&gt; mean V sd #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 352 269 16.4 7.4 Large Sample Theorems 7.4.1 Law of Large Numbers Original: sims &lt;- 1000 ## 3 separate simulations for each x.binom &lt;- rbinom(sims, p = 0.2, size = 10) ## computing sample mean with varying sample size mean.binom &lt;- cumsum(x.binom) / 1:sims Tidyverse: Put the simulation number, x and mean in a tibble: sims &lt;- 1000 p &lt;- 0.2 size = 10 lln_binom &lt;- tibble( n = seq_len(sims), x = rbinom(sims, p = p, size = size), mean = cumsum(x) / n, distrib = str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;)) Original: ## default runif() is uniform(0, 1) x.unif &lt;- runif(sims) mean.unif &lt;- cumsum(x.unif) / 1:sims Tidyverse: lln_unif &lt;- tibble(n = seq_len(sims), x = runif(sims), mean = cumsum(x) / n, distrib = str_c(&quot;Uniform(0, 1)&quot;)) Original: par(cex = 1.5) ## plot for binomial plot(1:sims, mean.binom, type = &quot;l&quot;, ylim = c(1, 3), xlab = &quot;Sample size&quot;, ylab = &quot;Sample mean&quot;, main = &quot;Binomial(10, 0.2)&quot;) abline(h = 2, lty = &quot;dashed&quot;) # expectation ## plot for uniform plot(1:sims, mean.unif, type = &quot;l&quot;, ylim = c(0, 1), xlab = &quot;Sample size&quot;, ylab = &quot;Sample mean&quot;, main = &quot;Uniform(0, 1)&quot;) abline(h = 0.5, lty = &quot;dashed&quot;) # expectation Tidyverse: true_means &lt;- tribble(~distrib, ~mean, &quot;Uniform(0, 1)&quot;, 0.5, str_c(&quot;Binomial(&quot;, size, &quot;, &quot;, p, &quot;)&quot;), size * p) ggplot() + geom_hline(aes(yintercept = mean), data = true_means, colour = &quot;white&quot;, size = 2) + geom_line(aes(x = n, y = mean), data = bind_rows(lln_binom, lln_unif)) + facet_grid(distrib ~ ., scales = &quot;free_y&quot;) + labs(x = &quot;Sample Size&quot;, y = &quot;Sample Mean&quot;) 7.4.2 Central Limit Theorem Original: par(cex = 1.5) ## sims = number of simulations n.samp &lt;- 1000 z.binom &lt;- z.unif &lt;- rep(NA, sims) for (i in 1:sims) { x &lt;- rbinom(n.samp, p = 0.2, size = 10) z.binom[i] &lt;- (mean(x) - 2) / sqrt(1.6 / n.samp) x &lt;- runif(n.samp, min = 0, max = 1) z.unif[i] &lt;- (mean(x) - 0.5) / sqrt(1 / (12 * n.samp)) } ## histograms; nclass specifies the number of bins hist(z.binom, freq = FALSE, nclass = 40, xlim = c(-4, 4), ylim = c(0, 0.6), xlab = &quot;z-score&quot;, main = &quot;Binomial(0.2, 10)&quot;) x &lt;- seq(from = -3, to = 3, by = 0.01) lines(x, dnorm(x)) # overlay the standard Normal PDF hist(z.unif, freq = FALSE, nclass = 40, xlim = c(-4, 4), ylim = c(0, 0.6), xlab = &quot;z-score&quot;, main = &quot;Uniform(0, 1)&quot;) lines(x, dnorm(x)) tidyverse: Instead of using a for loop, write functions. The population mean of the binomial distribution is \\(\\mu = p n\\) and the variance is \\(\\mu = p (1 - p) n\\). sims &lt;- 1000 n_samp &lt;- 1000 # Mean of binomial distribution binom_mean &lt;- function(size, p) { size * p } # Variance of binomial distribution binom_var &lt;- function(size, p) { size * p * (1 - p) } sim_binom_clt &lt;- function(n_samp, size, p) { x &lt;- rbinom(n_samp, prob = p, size = size) z &lt;- (mean(x) - binom_mean(size, p)) / sqrt(binom_var(size, p) / n_samp) tibble(distrib = str_c(&quot;Binomial(&quot;, p, &quot;, &quot;, size, &quot;)&quot;), z = z) } # Mean of uniform distribution unif_mean &lt;- function(min, max) { 0.5 * (min + max) } # Variance of uniform distribution unif_var &lt;- function(min, max) { (1 / 12) * (max - min) ^ 2 } sim_unif_clt &lt;- function(n_samp, min = 0, max = 1) { x &lt;- runif(n_samp, min = min, max = max) z &lt;- (mean(x) - unif_mean(min, max)) / sqrt(unif_var(min, max) / n_samp) tibble(distrib = str_c(&quot;Uniform(&quot;, min, &quot;, &quot;, max, &quot;)&quot;), z = z) } Since we will calculate this for n_samp = 1000 and n_samp, we might as well write a function for it. clt_plot &lt;- function(n_samp) { bind_rows(map_df(seq_len(sims), ~ sim_binom_clt(n_samp, size, p)), map_df(seq_len(sims), ~ sim_unif_clt(n_samp))) %&gt;% ggplot(aes(x = z)) + geom_density() + geom_rug() + stat_function(fun = dnorm, colour = &quot;red&quot;) + facet_grid(distrib ~ .) + ggtitle(str_c(&quot;Sample size = &quot;, n_samp)) } clt_plot(1000) clt_plot(100) "]
]
